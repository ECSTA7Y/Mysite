mutate(isna = is.na(val)) %>%
group_by(key) %>%
mutate(total = n()) %>%
group_by(key, total, isna) %>%
summarise(num.isna = n()) %>%
mutate(pct = num.isna / total * 100)
ceps <- read_csv("C:/Users/xsong/Desktop/table/cepsNA.csv")
missing <- ceps %>%
gather(key = "key", value = "val") %>%
mutate(isna = is.na(val)) %>%
group_by(key) %>%
mutate(total = n()) %>%
group_by(key, total, isna) %>%
summarise(num.isna = n()) %>%
mutate(pct = num.isna / total * 100)
levels <- (missing %>% filter(isna == T) %>% arrange(pct))$key
ggplot(missing) +
geom_point(aes(reorder(key,pct),
pct,fill=pct),
stat = 'identity',size=0.5,color='grey') +
scale_x_discrete(limits = levels) +
scale_fill_manual(name = "",values = c('white','tomato'), labels = c("非缺失值", "缺失值")) +
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
ggplot(missing) +
geom_point(aes(reorder(key,pct),
pct,fill=pct),
stat = 'identity',size=0.5,color='grey')
ggplot(missing) +
geom_point(aes(reorder(key,pct),
pct,fill=pct),
stat = 'identity',size=0.5)
ggplot(missing) +
geom_point(aes(reorder(key,pct),
pct,fill=pct),
stat = 'identity',size=0.5) +
coord_flip()
ceps <- read_csv("C:/Users/xsong/Desktop/table/cepsNA.csv")
missing.values <- ceps %>%
gather(key = "key", value = "val") %>%
mutate(isna = is.na(val)) %>%
group_by(key) %>%
mutate(total = n()) %>%
group_by(key, total, isna) %>%
summarise(num.isna = n()) %>%
mutate(pct = num.isna / total * 100)
levels <- (missing.values %>% filter(isna == T) %>% arrange(pct))$key
ggplot(missing.values) +
geom_bar(aes(reorder(key,pct),
pct,fill=isna),
stat = 'identity',size=0.5,color='grey') +
scale_x_discrete(limits = levels) +
scale_fill_manual(name = "",values = c('white','tomato'), labels = c("非缺失值", "缺失值")) +
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
ggplot(missing.values) +
geom_point(aes(reorder(key,pct),pct),color='grey')
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
ggplot(missing.values) +
geom_point(aes(reorder(key,pct),pct),color='grey') +
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
ggplot(missing.values) +
geom_bar(aes(reorder(key,pct),
pct,fill=isna),
stat = 'identity',size=0.5,color='grey') +
scale_x_discrete(limits = levels) +
scale_fill_manual(name = "",values = c('white','tomato'), labels = c("非缺失值", "缺失值")) +
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
ggplot(missing.values) +
geom_point(aes(reorder(key,pct),pct),color='grey') +
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
ggplot(missing.values) +
geom_point(aes(reorder(key,pct),pct),color='grey') +
scale_x_discrete(limits = levels) +
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
levels
missing.values
missing.values %>%
filter(isna == T) %>%
ggplot() +
geom_point(aes(reorder(key,pct),pct),color='grey') +
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
ggplot(missing.values) +
geom_bar(aes(reorder(key,pct),
pct,fill=isna),
stat = 'identity',size=0.5,color='grey') +
scale_x_discrete(limits = levels) +
scale_fill_manual(name = "",values = c('white','tomato'), labels = c("非缺失值", "缺失值")) +
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
missing.values %>%
filter(isna == T) %>%
ggplot() +
geom_point(aes(reorder(key,pct),pct),color='grey') +
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
missing.values %>%
filter(isna == T) %>%
ggplot() +
geom_point(aes(reorder(key,pct),pct),color=pct) +
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
missing.values %>%
filter(isna == T) %>%
ggplot() +
geom_point(aes(reorder(key,pct),pct),fill=pct) +
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
missing.values %>%
filter(isna == T) %>%
ggplot() +
geom_point(aes(reorder(key,pct),pct)) +
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
missing.values %>%
filter(isna == T) %>%
ggplot() +
geom_point(aes(reorder(key,pct),pct,fill=pct)) +
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
missing.values %>%
filter(isna == T) %>%
ggplot() +
geom_point(aes(reorder(key,pct),pct,col=pct)) +
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
?scale_colour_viridis_d
ceps <- read_csv("C:/Users/xsong/Desktop/table/cepsNA.csv")
missing.values <- ceps %>%
gather(key = "key", value = "val") %>%
mutate(isna = is.na(val)) %>%
group_by(key) %>%
mutate(total = n()) %>%
group_by(key, total, isna) %>%
summarise(num.isna = n()) %>%
mutate(pct = num.isna / total * 100) %>%
filter(isna == T) %>%
ggplot() +
geom_point(aes(reorder(key,pct),
pct,col=pct)) +
scale_colour_viridis_c()+
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
ceps <- read_csv("C:/Users/xsong/Desktop/table/cepsNA.csv")
missing.values <- ceps %>%
gather(key = "key", value = "val") %>%
mutate(isna = is.na(val)) %>%
group_by(key) %>%
mutate(total = n()) %>%
group_by(key, total, isna) %>%
summarise(num.isna = n()) %>%
mutate(pct = num.isna / total * 100) %>%
filter(isna == T) %>%
ggplot() +
geom_point(aes(reorder(key,pct),
pct,col=pct)) +
scale_colour_viridis_c()+
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
ceps <- read_csv("C:/Users/xsong/Desktop/table/cepsNA.csv")
missing.values <- ceps %>%
gather(key = "key", value = "val") %>%
mutate(isna = is.na(val)) %>%
group_by(key) %>%
mutate(total = n()) %>%
group_by(key, total, isna) %>%
summarise(num.isna = n()) %>%
mutate(pct = num.isna / total * 100)
missing.values %>%
filter(isna == T) %>%
ggplot() +
geom_point(aes(reorder(key,pct),
pct,col=pct)) +
scale_colour_viridis_c()+
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
ceps <- read_csv("C:/Users/xsong/Desktop/table/cepsNA.csv")
missing.values <- ceps %>%
gather(key = "key", value = "val") %>%
mutate(isna = is.na(val)) %>%
group_by(key) %>%
mutate(total = n()) %>%
group_by(key, total, isna) %>%
summarise(num.isna = n()) %>%
mutate(pct = num.isna / total * 100)
missing.values %>%
filter(isna == T) %>%
ggplot() +
geom_point(aes(reorder(key,pct),
pct,col=pct)) +
scale_colour_viridis_c(guide =F)+
coord_flip() +
labs(x ='变量', y = "%")+
theme_bw()
rmarkdown::convert_ipynb("C:/Users/xsong/titanic-machine-learning.ipynb", output = "C:/Users/xsong/titanic-machine-learning.Rmd")
rmarkdown:::convert_ipynb("C:/Users/xsong/titanic-machine-learning.ipynb", output = "C:/Users/xsong/titanic-machine-learning.Rmd")
library(reticulate)
?reticulate
use_condaenv()
use_python()
use_python("C:/ProgramData/Anaconda3/python.exe", required = FALSE)
import numpy as np
use_virtualenv("myenv")
Sys.which("python")
library(reticulate)
use_python("C:/ProgramData/Anaconda3", required = FALSE)
library(reticulate)
use_condaenv("C:/ProgramData/Anaconda3")
use_condaenv("myenv")
py_config()
library(reticulate)
os <- import("os")
os$listdir(".")
library(reticulate)
use_python("~/miniconda3/bin/python")
repl_python()
import numpy as np
import seaborn as sns
from scipy.stats import f
from scipy.stats import boxcox
r = f.rvs(30, 30, size=500)
np.random.seed(1234)
transformed,fitted_lambda = boxcox(r)
print('lambda=',fitted_lambda)
sns.distplot(r, rug=True,color='green')
sns.distplot(transformed, rug=True)
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from numpy import loadtxt
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
salary_table =pd.read_csv("E:\\R_codes\\others\\nba\\NBA_season1718_salary.csv",encoding = 'utf-8')
salary_table=salary_table[['Player','season17_18']]
salary_table=salary_table.rename(columns={'season17_18':'salary17_18'}) #变量重命名
salary_table['salary17_18']=salary_table['salary17_18']/1000000 #数据单位转化为'百万'
salary_table
quit
library(reticulate)
#use_python("C:/ProgramData/Anaconda3/python")
use_virtualenv('r-reticulate')
Sys.which("python")
repl_python()
import numpy as np
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost
from numpy import loadtxt
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
sns.set_style("whitegrid")
train= pd.read_csv('E:/MaLearning/titanic/train.csv',encoding = 'utf-8')
test= pd.read_csv('E:/MaLearning/titanic/test.csv',encoding = 'utf-8')
train
(train.isnull().sum() / len(train)) * 100
def convert_dummy(df, feature,rank=0):
pos = pd.get_dummies(df[feature], prefix=feature)
mode = df[feature].value_counts().index[rank]
biggest = feature + '_' + str(mode)
pos.drop([biggest],axis=1,inplace=True)
df.drop([feature],axis=1,inplace=True)
df=df.join(pos)
return df
virtualenv_create("r-reticulate")
virtualenv_install("r-reticulate", "scipy")
virtualenv_install("r-reticulate", "pandas")
exit
virtualenv_create("r-reticulate")
virtualenv_install("r-reticulate", "scipy")
virtualenv_install("r-reticulate", "pandas")
conda_create("r-reticulate")
conda_install("r-reticulate", "scipy")
?conda_install
conda_install("r-reticulate", "seaborn")
library(reticulate)
use_condaenv("r-reticulate")
#conda_create("r-reticulate")
#conda_install("r-reticulate", "scipy")
py_available()
use_condaenv("r-reticulate")
py_discover_config()
library(reticulate)
use_condaenv("r-reticulate")
py_module_available('seaborn')
library(reticulate)
use_condaenv("r-reticulate")
py_module_available('matplotlib')
library(reticulate)
use_condaenv("r-reticulate")
py_module_available('pandas')
py_initialize(config$python, config$libpython, config$pythonhome, config$virtualenv_activate, config$version >= "3.0", interactive(), numpy_load_error)
?py_initialize
reticulate::py_config()
library(reticulate)
use_condaenv("r-reticulate")
py_module_available('pandas')
#conda_create("r-reticulate")
#conda_install("r-reticulate", "scipy")
sessionInfo()
library(reticulate)
repl_python
repl_python()
py_available(initialize = FALSE)
py_available(initialize = FALSE)
exit
py_available()
py_module_available('pandas')
library(reticulate)
#conda_create("r-reticulate")
#conda_install("r-reticulate", "scipy")
#conda_install("r-reticulate", "pandas")
#conda_install("r-reticulate", "seaborn")
use_condaenv("r-reticulate")
py_module_available('pandas')
iris
library(tm)
install.packages('tm')
library(tm)
?tm
??tm
1819/16180
w <- c(3,6,7,0,1,11,7)
rank(2)
w <- c(3,6,7,0,1,11,7)
rank(w)
w <- c(5,4,3,2)
rank(w)
?rank
library(tm)
acq
data("acq")
View(acq)
acq
data("crude")
tdm <- TermDocumentMatrix(crude,
control = list(removePunctuation = TRUE,
stopwords = TRUE))
tdm
?between
?dplyr::between
between(1:12, 7, 9)
dplyr::between(1:12, 7, 9)
dplyr::between
?scale_colour_gradientn
library(ggplot2)
?scale_colour_gradientn
??tm::replaceWords
?tm::replaceWords
?tm:replaceWords
library(tm)
??tm::replaceWords
?tm::replaceWords
?replaceWords
?tm
??tm
data("crude")
inspect(crude[[14]])
inspect(removePunctuation(crude[[14]]))
library("wordnet")
install.packages('wordnet')
library("wordnet")
install.packages('rJava')
library("wordnet")
data("crude")
## Document access triggers the stemming function
## (i.e., all other documents are not stemmed yet)
tm_map(crude, stemDocument, lazy = TRUE)[[1]]
## Use wrapper to apply character processing function
tm_map(crude, content_transformer(tolower))
crude
reuters <- tm_map(reuters, stripWhitespace)
data(reuters)
inspect(ovid[1:2])
data("crude")
inspect(crude[1:3])
inspect(crude[[1]])
tdm <- TermDocumentMatrix(crude)[1:10, 1:10]
inspect(tdm)
library("wordnet")
library("openNLP")
install.packages('openNLP')
library("openNLP")
crudeTDM <- TermDocMatrix(crude, control = list(stopwords = TRUE))
library(tm)
?TermDocMatrix
crudeTDM <- TermDocmentMatrix(crude, control = list(stopwords = TRUE))
crudeTDM <- TermDocumentMatrix(crude, control = list(stopwords = TRUE))
crudeTDM
(crudeTDMHighFreq <- findFreqTerms(crudeTDM, 10, Inf))
View(crude)
crude[["127"]]
Data(crudeTDM)[1:10, crudeTDMHighFreq]
findAssocs(crudeTDM, "oil", 0.85)
data("crude")
tdm <- TermDocumentMatrix(crude)
findAssocs(tdm, c("oil", "opec", "xyz"), c(0.7, 0.75, 0.1))
plot(data("crude")
tdm <- TermDocumentMatrix(crude)
findAssocs(tdm, c("oil", "opec", "xyz"), c(0.7, 0.75, 0.1)))
plot(findAssocs(tdm, c("oil", "opec", "xyz"), c(0.7, 0.75, 0.1)))
?rJava
??rJava
stemDoc(acq[[10]])
dtm <- DocumentTermMatrix(reuters)
inspect(dtm)
data('dtm')
reut21578 <- system.file("texts", "crude", package = "tm")
reuters <- VCorpus(DirSource(reut21578, mode = "binary"),readerControl = list(reader = readReut21578XMLasPlain))
reuters
inspect(reuters)
View(reuters)
reuters
reuters[["127"]]
inpect(reuters[["127"]])
inspect(reuters[["127"]])
?smote
`%>%` <- magrittr::`%>%`
library(caret)
library(readr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(pROC)
library(stargazer)
library(ROSE)
?ovun.sample
library(tm)
oz <- Corpus(DirSource("OzBooks/"))
?tm
data("crude")
meta(crude[[1]])
DublinCore(crude[[1]])
meta(crude[[1]], tag = "topics")
meta(crude[[1]], tag = "comment") <- "A short comment."
meta(crude[[1]], tag = "topics") <- NULL
DublinCore(crude[[1]], tag = "creator") <- "Ano Nymous"
DublinCore(crude[[1]], tag = "format") <- "XML"
DublinCore(crude[[1]])
meta(crude[[1]])
meta(crude)
meta(crude, type = "corpus")
meta(crude, "labels") <- 21:40
meta(crude)
rss <- Corpus(GmaneSource("http://rss.gmane.org/
gmane.comp.lang.r.general"))
??GmaneSource
reut21578 <- system.file("texts", "crude", package = "tm")
reut21578
?regexec
?e1701
??e1701
index(e1701)
library(devtools)
check(e1701)
data(iris)
attach(iris)
## classification mode
# default with factor response:
model <- svm(Species ~ ., data = iris)
# alternatively the traditional interface:
x <- subset(iris, select = -Species)
y <- Species
model <- svm(x, y)
print(model)
summary(model)
browseURL(https://blog.csdn.net/weixin_41875052/article/details/79902515)
ls("package:dplyr")
lsf.str("package:dplyr")
search()
help(package = dplyr)
help(package = dplyr)
help(dplyr)
help(package = dplyr)
help(package = cart)
help(package = rpart)
blogdown:::new_post_addin()
setwd('F:\\Mysite\\Mysite')
blogdown:::serve_site()
blogdown:::stop_server()
