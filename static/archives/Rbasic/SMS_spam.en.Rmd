---
title: SMS spam text classification
author:  
date: 2019/12/1
output:
  bookdown::html_document2:
    toc: true
    theme: readable
---

[Text-Classification](https://cfss.uchicago.edu/notes/supervised-text-classification/)

```{r, include=F}
knitr::opts_chunk$set(comment=NA,error=T,message = F,warning = F,fig.align='center',out.width ='90%')
```

# Intro

Spam messages are really troublesome, thus people invent  algorithms to block them. Naive Bayes is a basic model for text classification. I will use that to build a text message filter system. Finaly I will integrate it into [a Shiny App](https://github.com/ECSTA7Y/txtnb_en). 

Now I will use R `tidytext` to do some basic visualization and analysis, use `tm` package to build Document-Term-Matrix (DTM), thus algorithms will understand information in text data.

After doing my analysis, I 'd like to share some experience.
First, after using `read_csv` to import data, you should not use  `tibble` to convert, because `read_csv` could directly get `tibble`. Otherwise you will receive ERROR.

There are lot of ways to generate DTM in R. Before deciding use `tm::DocumentTermMatrix`, I tried `quanteda::dfm_weight`. However, it always returns ERROR. Also, I tried to use `tidytext` to remove stop word, numbers, and then use `tidytext::cast_dtm` to convert `DocumentTermMatrix` object. It will cause problems because my class tag variable is inconsistent with my DTM. I could not solve, thus my method remains. I recommend not to mix `tidytext` and `tm` frame work, or your problem will be complex.

I also use Python did [a similar analysis](https://www.kaggle.com/rikdifos/sms-text-classification). 

# How to do ?

```{r}
library(magrittr)
library(quanteda)
library(tidytext)
library(ggplot2)
library(dplyr)
library(tm)
library(readr)
library(stringr)
```

```{r}
sms <- read_csv("E:/MaLearning/SPAM text message 20170820 - Data.csv")
```

There are two columns in original data, the first is tag record(ham or spam), another is text content. The proportion of ham messages is about`r  round(prop.table(table(sms$Category))[1],2)`, The proportion of spam messages is about`r  round(prop.table(table(sms$Category))[2],2)`.

```{r}
sms %>% count(Category)
#sms %$% prop.table(table(Category))[1]
```


Our data looks like:

```{r}
smswd <- sms %>%
  rename(message = Message,tag=Category) %>% 
  mutate(ID = row_number())
head(smswd)
Y <- as.factor(smswd$tag)
```


```{r}
smswd$message <- lapply(smswd$message , iconv, "UTF-8", "ASCII", sub="") # remove other language

smsvis <- smswd %>%
  unnest_tokens(word,message) %>%
  filter(!str_detect(word, "^[0-9]*$")) %>%
  anti_join(stop_words) %>% 
  mutate(word = SnowballC::wordStem(word,language = "english"))
head(smsvis)

library(reshape2)
library(wordcloud)
smsvis %>%
  count(word, tag, sort = T) %>%
  acast(word ~ tag, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("blue", "red"),max.words = 50)

smsvis %>%
  count(tag, word) %>%
  group_by(tag) %>%
  top_n(15) %>%
  ggplot() +
  geom_col(aes(reorder(word, n), 
               n, fill = tag),
           show.legend = F) +
  facet_wrap(~tag, scales = "free_y") +
  coord_flip()

smsvis %>%
  count(tag, word) %>%
  bind_tf_idf(word,tag,n) %>% 
  group_by(tag) %>%
  top_n(10) %>%
ggplot() + 
  geom_col(aes(reorder(word,tf_idf), 
             tf_idf, fill = tag),
           show.legend = F) +
  facet_wrap(~tag, scales = "free_y") +
  coord_flip()



```


```{r}

ms_corpus <- VCorpus(VectorSource(smswd$message))

sms_dtm <- DocumentTermMatrix(ms_corpus, control =
                                 list(tolower = T,
                                      removeNumbers = T,
                                      stopwords = T,
                                      removePunctuation = T,
                                      stemming = T))

dim(sms_dtm) #5572

sms_dtm1 <- removeSparseTerms(sms_dtm, sparse = .98)
smsmat <- as.matrix(sms_dtm1)
#head(smsmat)
dim(smsmat)
```

Original DTM has `r dim(sms_dtm)[2]` features, which is High-dimensional and very sparse. I use `removeSparseTerms` to keep `r dim(smsmat)[2]` features.

# Algorithms

Navies Bayes is used as basic method for text classification. Since every word in text data is independent. That is to say, one word 's location is not depend on another word, which satisfy Navies Bayes hypothesis of independence. Besides, I used Support Vector Machine method.


```{r}
library(caret)
library(e1071)
svmc <- svm(smsmat, Y)
print(svmc)
summary(svmc)
pred <- predict(svmc,smsmat)
#confusionMatrix(pred,Y,positive ='spam',mode="prec_recall")
conMatrix <- confusionMatrix(pred,Y,
                             positive ='spam',
                             mode="prec_recall") 
conMatrix[["table"]]


nb = naiveBayes(smsmat, Y) # klaR could also do NaiveBayes.
#train = smsmat[1:2,]
#setwd('F:/Mysite/Mysite/static/archives/Rbasic')
#saveRDS(train, "train.rds")
#save(nb,"naiveBayes.RData")
## save model
#load("naiveBayes.RData")
#print(nb)
#summary(nb)
pred1 = predict(nb,smsmat)

conMatrix1 = confusionMatrix(pred1,Y,
                              positive ='spam',
                              mode="prec_recall") 
#confusionMatrix
conMatrix1[["table"]]

prop.table(conMatrix1[["table"]],1)
```

SVM's accuracy is `r round(conMatrix[["overall"]][["Accuracy"]],2)`, balanced  accuracy is `r round(conMatrix[["byClass"]][["Balanced Accuracy"]],2)`。

Naive Bayes 's accuracy is `r round(conMatrix1[["overall"]][["Accuracy"]],2)`, balanced  accuracy is `r round(conMatrix1[["byClass"]][["Balanced Accuracy"]],2)` .

Now I write a function to plot confusion matrix using `ggplot2`:

```{r,fig.asp=0.8}
plot_table <- function(x,xlab='Predicted label',
                       ylab='True label',
                       normalize = F){
  if(normalize){
    x <- round(prop.table(x,1), 2)
    mar <- as.data.frame(x)
  }
  else{
    mar <- as.data.frame(x)
  }
  ggplot2::ggplot(mar,aes(mar[,2],mar[,1])) +
    geom_tile(aes(fill=Freq),color='black') +
    scale_fill_gradientn(colours = c('gray98',
                                     'steelblue1',
                                     'midnightblue'))+
    geom_label(aes(label = Freq)) +
     labs(fill='',x=xlab,y=ylab) +
     ylim(rev(levels(mar[,2])))+
     scale_y_discrete(expand=c(0,0))+
     scale_x_discrete(expand=c(0,0))
}

plot_table(conMatrix1[["table"]],'Reference','Prediction')+
  theme_bw()

plot_table(conMatrix1[["table"]])+
  theme_bw()

plot_table(conMatrix1[["table"]],'Reference','Prediction',normalize=T)+
  theme_bw()
```

抛开准确率问题不谈，这里有另一个问题，为了避免不必要的损失，大部分人更希望过滤系统尽可能不要把有用的信息删掉。也就是说：

> Better to let go of a thousand than kill one by mistake.

所以，对于这个任务来说，算法的精确率越高越好，召回率(或者说查全率)越低越好。根据混淆矩阵，SVM的召回率为`r round(conMatrix[["byClass"]][["Recall"]],2)`，朴素贝叶斯的召回率为`r round(conMatrix1[["byClass"]][["Recall"]],2)`。SVM的精确率为`r round(conMatrix[["byClass"]][["Precision"]],2)`，朴素贝叶斯的精确率为`r round(conMatrix1[["byClass"]][["Precision"]],2)`。从混淆矩阵中我们也能看出，朴素贝叶斯算法将更多普通短信归类于垃圾短信了。因此SVM显然是更佳的模型。

# Application

下面我们用自己输入的短信文本放入朴素贝叶斯模型进行评价。同样，我们编写一个函数，输入短信字符串，输出判别结果。


```{r}
new <- 'please go home at 4 o clock bro' 

new2 <- 'We are trying to contact you.Please call our customer service representative on FREEPHONE.Claim code S89. Valid 12hrs only'

test_result <- function(model,string){
  ms_corpus <- VCorpus(VectorSource(string))
test_dtm <- DocumentTermMatrix(ms_corpus, control =
                                 list(tolower = T,
                                      removeNumbers = T,
                                      stopwords = T,
                                      removePunctuation = T,
                                      stemming = T))
test_dtmx <- as.matrix(test_dtm)
result <- predict(model,test_dtmx)
return(result)
}



test_result(nb,new)
test_result(nb,new2)
```

经过检验，支持向量机不能支持新数据的预测。如果新的测试集的变量超出了训练集的变量，就无法运行。而朴素贝叶斯可以(我没弄清楚为什么)，所以我们可以使用训练好的模型任意输入新的短信文本进行判断。

另外，为了更好地分享短信过滤程序，我将训练好的朴素贝叶斯模型保存下来并编写成Shiny App。使得用户输入任意短信文本就能在用户友好的图形界面的得到判定结果。用户可以点击[这里](https://github.com/ECSTA7Y/txtnb)查看Shiny App的源代码。你也可以在本地的R中输入以下代码下载并运行这个程序：

```r
library(shiny)
runGitHub( "txtnb", "ECSTA7Y")
```
```{r, include=F}
knitr::opts_chunk$set(comment=NA,error=T,message = F,warning = F,fig.align='center',out.width ='90%')
```
https://i.loli.net/2019/12/25/lQ3mIj5iDNFzUMx.gif








```{r,eval=F,include=F}
smswddm

inspect(smswddm)

smswddm<- removeSparseTerms(smswddm, sparse=0.985)

inspect(smswddm)
smswddm
smswddm <- as.matrix(smswddm)
head(smswddm)

library(e1071)
model <- svm(smswddm, Y)
print(model)
summary(model)
library(tm)
library(Matrix)
smswddm <- smswd %>% 
   count(ID,word) %>% 
   cast_sparse(ID,word,n)

docvars(sms.corpus, "Category") <- data$Category

spam.plot <- corpus_subset(sms.corpus, docvar1 == "spam")  
spam.plot <- dfm(spam.plot, tolower = TRUE, remove_punct = TRUE, remove_twitter = TRUE, remove_numbers = TRUE, remove=stopwords("SMART"))
spam.col <- brewer.pal(10, "BrBG")  

spam.cloud <- textplot_wordcloud(spam.plot, min.freq = 16, color = spam.col)  
title("Spam Wordcloud", col.main = "grey14")  

ham.plot <- corpus_subset(sms.corpus, docvar1 == "ham")  
ham.plot <- dfm(ham.plot, tolower = TRUE, removePunct = TRUE, removeTwitter = TRUE, removeNumbers = TRUE, remove=c("gt", "lt", stopwords("SMART")))  
ham.col <- brewer.pal(10, "BrBG")  
textplot_wordcloud(ham.plot, min.freq = 50, colors = ham.col, fixed.asp = TRUE)  
title("Ham Wordcloud", col.main = "grey14")


```
```{r,eval=F,include=F}
sms.dfm <- dfm(sms.corpus, tolower = TRUE)  
sms.dfm <- dfm_trim(sms.dfm, min_count = 5, min_docfreq = 3)  
sms.dfm <- dfm_weight(sms.dfm, type = "tfidf")  

sms.raw.train <- raw.data[1:4738,]  
sms.raw.test <- raw.data[4739:nrow(raw.data),]


sms.dfm.train <- sms.dfm[1:4738,]  
sms.dfm.test <- sms.dfm[4739:nrow(raw.data),]  
sms.classifier <- textmodel_nb(sms.dfm.train, sms.raw.train$Category)  


sms.predictions <- predict(sms.classifier, newdata = sms.dfm.test)  
table(sms.predictions$nb.predicted, sms.raw.test$Category)  
```

