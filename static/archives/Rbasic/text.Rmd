---
title: 'Text Mining'
author: 'Malcolm'
date: '2019-10-30'
output:
  bookdown::html_document2:
    toc: true
    theme: readable
---

```{r,include=F}
knitr::opts_chunk$set(comment=NA,error=T,message = F,warning = F,fig.align='center',out.width ='90%')
```

```{r}
library(readr)
library(ggplot2)
library(dplyr)
library(purrr)
`%>%` <- magrittr::`%>%`
```


```{r}
setwd('D:/RapLyrics-Scraper-master/lyrics_US')
library(tidytext)
library(dplyr)
data(stop_words)
temp = list.files(path = 'D:/RapLyrics-Scraper-master/lyrics_US',
  pattern='*.txt')
name = strsplit(temp,"_lyrics.txt") %>% as.character()
for (i in 1:length(temp)) {
  assign(name[i],read_csv(temp[i],col_names = F) %>% 
  as.character() %>% 
  tibble(text = .) %>% 
  unnest_tokens(word,text) %>% 
  anti_join(stop_words) %>% 
  mutate(rapper=name[i]))
}
rm('stop_words')
dfs = sapply(.GlobalEnv, is.data.frame) 
rap <- do.call(rbind, mget(names(dfs)[dfs]))
kponlydt <- ls()[ls()!= 'rap']
rm(list = kponlydt)

fusion <- read_csv("D:/RapLyrics-Scraper-master/lyrics_US/fusion/fusion.txt",col_names = F) %>% 
  as.character() %>% 
  tibble(text = .)

fusion <- as.character(fusion)
colnames(fusion)[1] <- 'txt'

##fusion <- tibble(text = fusion)
#tidy_books %>%
#  count(word, sort = TRUE) 
```

```{r}

```



A token is a meaningful unit of text, most often a word, that we are interested in using for further analysis, and tokenization is the process of splitting text into tokens.


```{r }
library(tidytext)
library(dplyr)
fusion <- tibble(text = fusion)
fusion
data(stop_words)

text_df <- fusion %>% 
  unnest_tokens(word,text) %>% 
  anti_join(stop_words)

head(text_df,10)

wdf <- text_df %>%
  count(word, sort = T) 
wdf
```

```{r}
wdf %>%
  filter(n > 800) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) + 
  geom_point(size=1.5, color='red')+
  xlab(NULL) +
  coord_flip()
```

```{r}
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```


```{r}
library(tidyr)

wdf1 <- wdf %>%
  inner_join(get_sentiments("bing")) 

wdf2 <- wdf %>%
  inner_join(get_sentiments("afinn")) 

wdf3 <- wdf %>%
  inner_join(get_sentiments("nrc")) 



wdf1 %>%
  filter(n > 300) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n,col=sentiment,
             shape=sentiment)) + 
  geom_point(size=1.5)+
  xlab(NULL) +
  coord_flip()+
  scale_colour_viridis_d()

library(RColorBrewer)
wdf3 %>%
  filter(n > 300) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n,color=sentiment)) + 
  geom_point(size=1.5)+
  xlab(NULL) +
  coord_flip()+
  scale_color_brewer(palette="Paired")

wdf2 %>%
  filter(n > 300) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() + 
  geom_point(aes(word, n,col=value,size=value))+
  scale_colour_gradient(low='navyblue',high='red')+
  coord_flip() 
```

```{r}
library(wordcloud)

text_df %>%
  mutate(word = reorder(word, n)) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

```{r}
rap_bigrams <- fusion %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
rap_bigrams

rap_bigrams <- rap_bigrams %>%
  count(bigram, sort = T)

rap_bigrams

rap_bigrams <- rap_bigrams %>%
  separate(bigram, c("word1", "word2"),
           sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  filter(word1!='yeah',word1!='uh',
         word1!='ha',word1!='la') #%>% 
  #unite(bigram, word1, word2, sep = " ")

#rap_bigrams %>%
#  count(bigram, sort = T)
rap_bigrams
```


```{r}
library(igraph)
library(ggraph)

# filter for only relatively common combinations
bigram_graph <- rap_bigrams %>%
  filter(n > 20) %>%
  graph_from_data_frame()

bigram_graph

set.seed(2016)

a <- grid::arrow(type = "open", length = unit(.05, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(show.legend = F,arrow = a,edge_colour='blue',edge_width=0.5) +
  geom_node_point(color = "red", size = 1) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1,col='purple') +
  theme_void()

#set.seed(2017)
#ggraph(bigram_graph, layout = "fr") +
#  geom_edge_link() +
#  geom_node_point() +
#  geom_node_text(aes(label =name), vjust = 1, hjust = 1)
```

```{r}
wdf <- wdf %>%
  mutate(document=rep(1:4,each=6007,len=30039)) %>% 
  bind_tf_idf(word,document, n)

wdf
```


Zipf’s law states that the frequency that a word appears is inversely proportional to its rank

[Machine Learning for Natural Language Processing](https://www.lexalytics.com/lexablog/machine-learning-vs-natural-language-processing-part-1)

[A Gentle Introduction to the Bag-of-Words Model](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)

[The Beginner’s Guide to Text Vectorization](https://monkeylearn.com/blog/beginners-guide-text-vectorization/)

[Text Classification](https://monkeylearn.com/text-classification/)

Kaggle Notebooks:

[SMS_Spam_ML](https://www.kaggle.com/benjaminlott/basic-machine-learning-of-spam-text-analysis/)

[Basic NLP with NLTK](https://www.kaggle.com/alvations/basic-nlp-with-nltk)

[Awesome Tutorials and Basic Text Classification with Movie Reviews](https://www.kaggle.com/arunkumarramanan/awesome-ml-and-text-classification-movie-reviews)

[Spam Detection and Deep NLP](https://www.kaggle.com/nihalbey/spam-detection-and-deep-nlp)

<!--
[Python自动分箱，计算woe,iv](https://blog.csdn.net/KIDxu/article/details/88647080)    
[数据挖掘模型中的IV和WOE详解](https://blog.csdn.net/kingzone_2008/article/details/80449287)
--->

```{r}
wdf <- rap %>%
  group_by(rapper) %>% 
  count(word, sort = T) %>% 
  mutate(word = reorder(word, n),
         word=as.character(word)) %>% 
  filter(word!='na',word!='00a0') 
wdf

bing_word_counts <- rap %>%
  inner_join(get_sentiments("bing")) %>%
  count(word,sentiment, sort = T) %>%
  ungroup()

bing_word_counts

wdf %>%
  filter(n > 200) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) + 
  geom_point(size=1.5, color='red')+
  xlab(NULL) +
  coord_flip()+
  facet_wrap(~rapper, scales = "free_y")

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = F) +
  facet_wrap(~sentiment, scales = "free_y") +
  coord_flip()

library(wordcloud)

rap %>%
  count(word) %>% 
  filter(word!='na',word!='00a0') %>% 
  with(wordcloud(word, n, max.words = 100))

library(reshape2)

rap %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = T) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("blue", "red"),max.words = 100)

```

```{r}
totalword <- wdf %>% 
  group_by(rapper) %>% 
  summarize(total = sum(n))

book_words <- left_join(wdf, totalword)
totalword
book_words

ggplot(book_words, aes(n/total)) +
  geom_histogram(show.legend = F) 

```

```{r,fig.asp=1.2}
ggplot(book_words, aes(n/total, fill = rapper)) +
  geom_histogram(show.legend = F) +
  facet_wrap(~rapper, ncol = 6, scales = "free_y")
```


```{r,fig.asp=4}
book_words <- book_words %>%
  bind_tf_idf(word,rapper, n)

book_words

book_words %>%
  mutate(word = factor(word,
                       levels = rev(unique(word)))) %>% 
  group_by(rapper) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(reorder(word,tf_idf), 
             tf_idf, fill = rapper)) +
  geom_col(show.legend = F) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~rapper, ncol = 3, scales = "free") +
  coord_flip()
```






