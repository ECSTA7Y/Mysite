---
title: 'Outliers and Other Fragmented Works'
author: ' '
date: '2020-03-16'
slug: march_works
---



<p>Practice builds experience. A few weeks ago, I finished several very fragmented works, which needs to be summerized.
First is outliers detection. Until joining in <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview">House Prediction Competiton</a> did I realized how important outliers handling is! At first I recode outliers (I define <em>outliers</em> by histogram) to a high quantile. I gradually found this is wrong. It will generate accumulation of special values. Then, it does no good to prediction. I posted <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/discussion/132754#765810">this question</a> and read others tutorial notebooks. I found using binary distibution plot (scatter plot) is helpful. Those points who are opposite to main distribution should be dropped. It really improve my LB ranking.</p>
<p>However, there’s no free lunch, dropping samples will cause sample size reduction. The first method will not lose samples. This is a trade-off. Another way for tackling outliers is <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html">IsolationForest</a>, but I haven’t tried it.</p>
<hr />
<p>A <a href="http://www.win-vector.com/blog/2013/05/bayesian-and-frequentist-approaches-ask-the-right-question/">post</a> written by Nina Zumel focuses on Bayesian approach is really easy to understand. It introduce a simple framework of Bayesian inference. Although the data is simple, it reminds me that</p>
<blockquote>
<p>Common statistical tests are linear models</p>
</blockquote>
<p>written by <a href="https://lindeloev.github.io/tests-as-linear/">Jonas Kristoffer Lindeløv</a>. The simple framework could be generilzed to regression analysis.</p>
<hr />
<p>Some papers written by big professors not teaching you technic, but improve your <em>data IQ</em>. The first one is <em>Statistical Modeling: The Two Cultures</em> by Leo Brieman. Others could be found at <a href="https://cosx.org/2020/03/what-it-creates-is-natural-ds-ai-development/">this article</a>’s reference list. Those papers are great help to my thesis.</p>
<hr />
<p>After finishing a text classification task (a semi-intern), I found it’s important to make columns 1:1 corresponding between train set and test set, or the result will be totally messed up. I write this:</p>
<pre class="python"><code>def consist_train_test(test):
    &#39;&#39;&#39;
    make test set columns corresponding to train set
    &#39;&#39;&#39;
    new_df = pd.DataFrame()
    for i in train_col: # train_col = train.columns
        if i in test.columns:
            new_df[i] = test[i]
        else:
            new_df[i] = 0
    new_df.fillna(0, inplace = True)
    order = train_col
    new_df[order] # order data
    return new_df</code></pre>
<p>This actually a practical trick. Some algorithms will throw an error if you don’t do this. Similarly, I solved this problem at my <a href="https://xiaosong.shinyapps.io/spam_text/">shiny app</a>.</p>
