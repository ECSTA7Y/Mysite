---
title: "初中生学习成绩的影响因素——基于分类与回归树的分析"
author: ' '
date: '2019-05-08'
slug: edutree
tags: R
categories: R
---

## 数据  

本文采用[中国教育追踪调查](https://ceps.ruc.edu.cn/) (China Education Panel Survey, CEPS)2013-2014学年基线数据。
采用PPS抽样方法，以人口平均受教育水平和人口比例为分层变量并通过两阶段分层抽样从从全国随机抽取了28个县级单位作为调查点。具体而言，CEPS在每个入样县（区）所辖地理范围内分别抽取4所初中学校。并在每所入样学校中分别抽取4个班级，包括2个七年级班和2个九年级班。

## 模型与算法

CART算法全称为分类与回归树（Classification and Regression Trees）。它可以处理分类与回归算法，以及生存分析因变量。作为一种非参数的机器学习方法，CART决策树无需对数据的分布做任何假定。CART算法划分数据的依据是变量的取值顺序，因此它对异常值不敏感。最后，通过交叉验证（Cross Validation）的方法求得预测误差。
回归树模型可表示为：
\[f(x) = \sum\limits_{m = 1}^M {{c_m}I(x \in {R_m})} \]
 
其中，$x$是一系列输入特征（自变量），${R_1},{R_2},...,{R_m}$是输入空间被划分的M个区域。 是区域${R_m}$对应的最优值。$I$代表的是指示函数（indicator function），当输入变量$x$属于区域 ${R_m}$时，输出为1，否则输出为0。
CART算法选择基尼系数进行属性划分。CART算法可以运用于分类和回归问题中。

## 因变量

本研究的因变量是被调查学生上一次期中考试的成绩。包括数学成绩、语文成绩、英语成绩。我们将同时对总成绩进行分析。

为了讨论学习成绩的门槛效应，我们将数学成绩是否位于所有学生的前25\%(是=1，否=0)单独划分为一个分类变量建立分类树。下图展示的是标准成绩的直方图。

## 自变量

下面展示了自变量以及自变量在模型中的编码：

家庭变量：每星期零用钱(money)、上兴趣班费用总计(clfee)、监督孩子的作业(qianzi)、花在孩子身上的时间(lifetm)、孩子交流方言(dial)、父母交流方言(chidia)、家长教育期望(eduyexp)、对孩子未来的信心(futcfd)、孩子户口类型(huko)、家长教育程度(eduy)、家长政治面貌(dangy)、住房是否生产经营用(houspro)。  

个人变量：性别(sex)、是否独生子女(onechi)、父亲教育水平(faedu)、母亲教育水平(maedu)、爸爸经常酗酒(drunk)、父母经常吵架(qurel)、父母之间关系很好(relation)、有独立书桌(desk)、家里有电脑和网络(net)、家庭交流方言(dialect)、父母督促学习天数(chkhmwk chkcouse)、父母教育期望(eduexp)。

学校变量：学校性质(schtype)、教室数量(schcsrm)、学校电脑数(comno)、图书数量(bknum)、生均财政拨款(buget)、持有教师资格证人数(eduqua)、打架斗殴(fight)、破坏公物(brkpb)、吸烟(smok)、饮酒(drink)、高级教师年收入(teainc)。

```{r add ,message = F,prompt=T, comment = NA,warning = F,message = F,echo = FALSE,fig.align='center'}
setwd("E:/edu/data2")
library(haven)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(lattice)
require(stats)
library(ggplot2)
library(pROC)
library(randomForest)

ceps <- read_dta("E:/edu/data2/ceps.dta")
ceps <- lapply(ceps, unclass)
ceps <- data.frame(ceps)
ceps$schids <- as.factor(ceps$schids)
attach(ceps)
histogram(sdtotal,equal.widths = TRUE, nint = 70, xlab = "标准化总成绩",type = "density",ylab = '密度')
```


为了直观地描述不同学校与学习成绩的差异，下面展示了以学校为分组变量，原始数学成绩为因变量的箱线图。

```{r a ,message = F,prompt=T, comment = NA,warning = F,message = F,echo = FALSE,fig.align='center'}
setwd("E:/edu/data2")
ceps <- read_dta("E:/edu/data2/ceps.dta")
ceps1 <- subset(ceps,schids<=30)
ceps1$schids <- as.factor(ceps1$schids)
ggplot(ceps1, aes(schids,tr_mat)) + 
  geom_boxplot() +
  xlab('学校代码') + ylab('数学原始成绩')

ceps <- lapply(ceps, unclass)
ceps <- data.frame(ceps)

```

下面展示的是数学原始成绩是否位于前25\%的分类决策树。首先，我们在备选的约50个自变量中纳入学校哑变量。仅考虑家庭、个体、班级因素对学生学习成绩的影响。通过划分训练集和测试集的方式计算模型的预测错误率。检验模型的泛化能力。

```{r c ,message = F,prompt=T, comment = NA,warning = F,message = F, include = T,fig.align='center'}
set.seed(1212323) #设置随机数种子
sampled <- sample(1:nrow(ceps),nrow(ceps)*0.3,replace=FALSE) #抽取30%样本作为验证集
test <- ceps[sampled,] #验证集
train <- ceps[-sampled,] #训练集

cont <- rpart.control(minsplit=5,maxcompete=100,xval=10,maxdepth=30,cp=0.01) #设置参数

cltree3 <- rpart(matgreat ~  schids + sex + onechi +  maedu +  faedu + drunk + 
qurel + relation + desk + net + dialect + chkhmwk + chkcouse + 
classtm +  clpre + revitm + subject + know + drsmok + commhr + 
schtype + schcsrm + comno +  bknum + buget + eduqua + fight + 
brkpb + smok +  drink +  teainc + money + clfee + qianzi + lifetm +  
dial +  chidia +  futcfd +  huko + eduy +  dangy +  houspro ,data = 
subset(train,grade9 == 1),weights = sweight,method = "class",parms = 
list(split="gini"),control=cont,na.action = na.omit,model=T) #决策树
knitr::kable(printcp(cltree3),caption="复杂度参数表",digits =4)
rpart.plot(cltree3)

test <- na.omit(test) #删除缺失值
y.pr <- predict(cltree3,test,type="prob") #预测测试集概率

pr <- y.pr[,2] 
test <- cbind(test,pr)
modelroc <- roc(test$matgreat,test$pr)
plot(modelroc,xlab ='伪正类率',ylab='真正类率') #ROC 曲线

test$yhat <- ifelse(test$pr >0.5,'1','0') #大于0.5设置为"是"
attach(test)
tab <- table(yhat,matgreat) 
treeerror <- (sum(tab)-sum(diag(tab)))/sum(tab)
tab
treeerror #测试集错误率
```

```{r d ,message = F,prompt=T, comment = NA,warning = F,message = F, include = T,fig.align='center'}
cont <- rpart.control(minsplit=5,maxcompete=100,xval=10,maxdepth=30,cp=0.014) #设置参数

cltree3 <- rpart(matgreat ~  schids + sex + onechi +  maedu +  faedu + drunk + 
qurel + relation + desk + net + dialect + chkhmwk + chkcouse + 
classtm +  clpre + revitm + subject + know + drsmok + commhr + 
schtype + schcsrm + comno + bknum + buget + eduqua + fight + 
brkpb + smok +  drink +  teainc + money + clfee + qianzi + lifetm +  
dial +  chidia +  futcfd + huko + eduy + dangy +  houspro ,data = 
subset(train,grade9 == 1),weights = sweight,method = "class",parms = 
list(split="gini"),control=cont,na.action = na.omit) #决策树

knitr::kable(printcp(cltree3),caption="复杂度参数表",digits =4)
cltree3$variable.importance #各个变量的重要性程度

rpart.plot(cltree3)

test <- na.omit(test) #删除缺失值
y.pr <- predict(cltree3,test,type="prob") #预测测试集概率

pr <- y.pr[,2] 
test <- cbind(test,pr)
modelroc <- roc(test$matgreat,test$pr)
auc(modelroc) #计算AUC值
plot(modelroc,xlab ='伪正类率',ylab='真正类率') #ROC 曲线

test$yhat <- ifelse(test$pr >0.5,'1','0') #大于0.5设置为"是"
attach(test)
tab <- table(yhat,matgreat) 
treeerror <- (sum(tab)-sum(diag(tab)))/sum(tab)
tab #混淆矩阵，行为测试集预测值，列为测试集观测值
treeerror #测试集错误率
```


可以发现，对因变量分类有显著贡献的变量有学校哑变量、对孩子未来的信心、上兴趣班费用总计、与学生交流时间。可见学校层面的特征对数学成绩的影响非常强。

在初次拟合分类树之后，我们对分类树进行剪枝。通过第一棵分类树的结果，我们选取预测误差(即表中的xerror通过交叉验证获得)最小的CP值带入到下一个模型中，最终的验证集错误率在25\%左右。

### 随机森林
```{r randomForest,message = F,prompt=T, comment = NA,warning = F,message = F, include = T,fig.align='center'}

train$matgreat <- as.factor(train$matgreat)
train <- na.omit(train)
train <- filter(train, is.na(matgreat)==F)

ratree <- randomForest(matgreat ~ schids + sex + onechi +  maedu +  faedu + drunk + qurel + relation + desk + net + dialect + chkhmwk + chkcouse + classtm +  clpre + revitm + subject + know + drsmok + commhr + schtype + schcsrm + comno + bknum + buget + eduqua + fight + brkpb + smok + drink + teainc + money + clfee + qianzi + lifetm +  dial +  chidia +  futcfd + huko + eduy + dangy +  houspro  ,data = subset(train,grade9 == 1),importance=T,proximity=T,mtry=16)

#ratree
#summary(ratree)
MDSplot(ratree,matgreat,palette=NULL)

importance(ratree, type=1, scale=T) #计算模型变量的重要性

```


