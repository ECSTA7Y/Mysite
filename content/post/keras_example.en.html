---
title: Guide to Keras Deep Learning 
slug: keras_example
author: ' '
date: '2020-05-16'
---



<p>Xiao Song</p>
<p><a href="https://xsong.ltd/" class="uri">https://xsong.ltd/</a></p>
<p>As a beginner, itâ€™s necessary to write some simple guides using simple data.</p>
<pre class="python"><code>import numpy as np 
import pandas as pd

X_train = pd.read_csv(&#39;E:/some_code/py_basic/house_price/data/train1.csv&#39;) # prepare data
X_test = pd.read_csv(&#39;E:/some_code/py_basic/house_price/data/test1.csv&#39;)

Y_train = np.array(X_train[&#39;SalePrice&#39;])
Y_train = np.log1p(Y_train)

X_train.drop([&#39;SalePrice&#39;],axis = 1, inplace = True)</code></pre>
<pre class="python"><code>Y_train
#&gt; array([12.24769912, 12.10901644, 12.31717117, ..., 12.25486757,
#&gt;        12.49313327, 11.86446927])</code></pre>
<pre class="python"><code>X_train.shape
#&gt; (1436, 109)</code></pre>
<pre class="python"><code>X_test.shape
#&gt; (1459, 109)</code></pre>
<div id="feature-standardization" class="section level1">
<h1>Feature Standardization</h1>
<p>Standardize features by removing the mean and scaling to unit variance:
<span class="math display">\[z = {{x - u} \over s}\]</span></p>
<p><span class="math inline">\(z\)</span> is standardize feature of <span class="math inline">\(x\)</span>, u is the mean of <span class="math inline">\(x\)</span>, and <span class="math inline">\(s\)</span> is the standard deviation of <span class="math inline">\(x\)</span>.</p>
<pre class="python"><code>#from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import StandardScaler

def standardize(df):
    &#39;&#39;&#39;standardize features&#39;&#39;&#39;
    transformer = StandardScaler().fit(df) 
    newX = transformer.transform(df)
    X = pd.DataFrame(newX,columns = df.columns)
    return X

X_train = standardize(X_train) # X train
X_test = standardize(X_test) </code></pre>
<pre class="python"><code>X_train
#&gt;       MSSubClass  MSZoningRM  ...  SaleTypeWD  SaleConditionOth
#&gt; 0       0.074674   -0.517138  ...    0.389312         -0.465778
#&gt; 1      -0.874282   -0.517138  ...    0.389312         -0.465778
#&gt; 2       0.074674   -0.517138  ...    0.389312         -0.465778
#&gt; 3       0.311913   -0.517138  ...    0.389312          2.146946
#&gt; 4       0.074674   -0.517138  ...    0.389312         -0.465778
#&gt; ...          ...         ...  ...         ...               ...
#&gt; 1431   -0.874282    1.933720  ...    0.389312         -0.465778
#&gt; 1432    0.074674   -0.517138  ...    0.389312         -0.465778
#&gt; 1433   -0.874282   -0.517138  ...    0.389312         -0.465778
#&gt; 1434    0.311913   -0.517138  ...    0.389312         -0.465778
#&gt; 1435   -0.874282   -0.517138  ...    0.389312         -0.465778
#&gt; 
#&gt; [1436 rows x 109 columns]</code></pre>
<pre class="python"><code>X_train.describe()
#&gt;          MSSubClass    MSZoningRM  ...    SaleTypeWD  SaleConditionOth
#&gt; count  1.436000e+03  1.436000e+03  ...  1.436000e+03      1.436000e+03
#&gt; mean  -5.721205e-17  3.101821e-16  ... -3.223203e-16     -1.622039e-16
#&gt; std    1.000348e+00  1.000348e+00  ...  1.000348e+00      1.000348e+00
#&gt; min   -8.742817e-01 -5.171379e-01  ... -2.568635e+00     -4.657780e-01
#&gt; 25%   -8.742817e-01 -5.171379e-01  ...  3.893119e-01     -4.657780e-01
#&gt; 50%   -1.625649e-01 -5.171379e-01  ...  3.893119e-01     -4.657780e-01
#&gt; 75%    3.119131e-01 -5.171379e-01  ...  3.893119e-01     -4.657780e-01
#&gt; max    3.158781e+00  1.933720e+00  ...  3.893119e-01      2.146946e+00
#&gt; 
#&gt; [8 rows x 109 columns]</code></pre>
<pre class="python"><code>X_train.shape[1]
#&gt; 109</code></pre>
</div>
<div id="neural-network-building" class="section level1">
<h1>Neural Network Building</h1>
<pre class="python"><code>from keras import models
#&gt; Using TensorFlow backend.
#&gt; C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
#&gt;   _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])
#&gt; C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
#&gt;   _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])
#&gt; C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
#&gt;   _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])
#&gt; C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
#&gt;   _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])
#&gt; C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
#&gt;   _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])
#&gt; C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
#&gt;   np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])
#&gt; C:\ProgramData\Anaconda3\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
#&gt;   _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])
#&gt; C:\ProgramData\Anaconda3\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
#&gt;   _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])
#&gt; C:\ProgramData\Anaconda3\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
#&gt;   _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])
#&gt; C:\ProgramData\Anaconda3\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
#&gt;   _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])
#&gt; C:\ProgramData\Anaconda3\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
#&gt;   _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])
#&gt; C:\ProgramData\Anaconda3\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
#&gt;   np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])
from keras import layers

model = models.Sequential()
model.add(layers.Dense(64, activation=&#39;relu&#39;,input_shape=(X_train.shape[1],)))
model.add(layers.Dense(64, activation=&#39;relu&#39;))
model.add(layers.Dense(1))

model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;mse&#39;, metrics=[&#39;mae&#39;])
model.summary()
#&gt; Model: &quot;sequential_1&quot;
#&gt; _________________________________________________________________
#&gt; Layer (type)                 Output Shape              Param #   
#&gt; =================================================================
#&gt; dense_1 (Dense)              (None, 64)                7040      
#&gt; _________________________________________________________________
#&gt; dense_2 (Dense)              (None, 64)                4160      
#&gt; _________________________________________________________________
#&gt; dense_3 (Dense)              (None, 1)                 65        
#&gt; =================================================================
#&gt; Total params: 11,265
#&gt; Trainable params: 11,265
#&gt; Non-trainable params: 0
#&gt; _________________________________________________________________</code></pre>
<pre class="python"><code>#?models.Sequential.fit</code></pre>
<pre class="python"><code>model.fit(X_train,Y_train,
    epochs = 20, # Number of epochs to train the model
    batch_size = 512) # Number of samples per gradient update.
#&gt; Epoch 1/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 172.2260 - mae: 13.1112
#&gt; 1436/1436 [==============================] - 0s 196us/step - loss: 160.0101 - mae: 12.6332
#&gt; Epoch 2/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 140.1727 - mae: 11.8303
#&gt; 1436/1436 [==============================] - 0s 13us/step - loss: 134.0486 - mae: 11.5655
#&gt; Epoch 3/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 120.1656 - mae: 10.9463
#&gt; 1436/1436 [==============================] - 0s 7us/step - loss: 115.9018 - mae: 10.7475
#&gt; Epoch 4/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 103.7653 - mae: 10.1600
#&gt; 1436/1436 [==============================] - 0s 23us/step - loss: 99.3971 - mae: 9.9375
#&gt; Epoch 5/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 87.8397 - mae: 9.3257
#&gt; 1436/1436 [==============================] - 0s 26us/step - loss: 83.6289 - mae: 9.0899
#&gt; Epoch 6/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 72.4388 - mae: 8.4369
#&gt; 1436/1436 [==============================] - 0s 21us/step - loss: 68.7455 - mae: 8.1996
#&gt; Epoch 7/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 60.7598 - mae: 7.6859
#&gt; 1436/1436 [==============================] - 0s 15us/step - loss: 55.0177 - mae: 7.2695
#&gt; Epoch 8/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 45.5617 - mae: 6.5405
#&gt; 1436/1436 [==============================] - 0s 8us/step - loss: 42.7941 - mae: 6.3155
#&gt; Epoch 9/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 35.6815 - mae: 5.6939
#&gt; 1436/1436 [==============================] - 0s 9us/step - loss: 32.4171 - mae: 5.3669
#&gt; Epoch 10/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 26.0480 - mae: 4.7380
#&gt; 1436/1436 [==============================] - 0s 25us/step - loss: 24.0971 - mae: 4.4846
#&gt; Epoch 11/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 20.0992 - mae: 4.0066
#&gt; 1436/1436 [==============================] - 0s 7us/step - loss: 17.7943 - mae: 3.7275
#&gt; Epoch 12/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 14.1610 - mae: 3.2529
#&gt; 1436/1436 [==============================] - 0s 24us/step - loss: 13.2439 - mae: 3.1282
#&gt; Epoch 13/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 11.2266 - mae: 2.8607
#&gt; 1436/1436 [==============================] - 0s 6us/step - loss: 10.1040 - mae: 2.6834
#&gt; Epoch 14/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 8.1120 - mae: 2.3516
#&gt; 1436/1436 [==============================] - 0s 6us/step - loss: 7.9515 - mae: 2.3555
#&gt; Epoch 15/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 6.7580 - mae: 2.1564
#&gt; 1436/1436 [==============================] - 0s 20us/step - loss: 6.4041 - mae: 2.0975
#&gt; Epoch 16/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 5.2496 - mae: 1.8611
#&gt; 1436/1436 [==============================] - 0s 9us/step - loss: 5.2633 - mae: 1.8882
#&gt; Epoch 17/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 4.2215 - mae: 1.6635
#&gt; 1436/1436 [==============================] - 0s 6us/step - loss: 4.3897 - mae: 1.7116
#&gt; Epoch 18/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 3.9114 - mae: 1.5937
#&gt; 1436/1436 [==============================] - 0s 8us/step - loss: 3.7163 - mae: 1.5625
#&gt; Epoch 19/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 3.1460 - mae: 1.4324
#&gt; 1436/1436 [==============================] - 0s 7us/step - loss: 3.1714 - mae: 1.4317
#&gt; Epoch 20/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 2.8883 - mae: 1.3825
#&gt; 1436/1436 [==============================] - 0s 22us/step - loss: 2.7371 - mae: 1.3229
#&gt; &lt;keras.callbacks.callbacks.History object at 0x0000000055BF7EF0&gt;
#&gt; 
#&gt; WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.</code></pre>
</div>
<div id="validation-data" class="section level1">
<h1>Validation data</h1>
<p>first I create validation set:</p>
<pre class="python"><code>x_val = X_train[:1000]
partial_x_train = X_train[1000:]
y_val = Y_train[:1000]
partial_y_train = Y_train[1000:]</code></pre>
<pre class="python"><code>history = model.fit(partial_x_train,partial_y_train,
    epochs=20,
    batch_size=512,
    validation_data=(x_val, y_val))
#&gt; Train on 436 samples, validate on 1000 samples
#&gt; Epoch 1/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 119us/step - loss: 2.4873 - mae: 1.2346 - val_loss: 2.3698 - val_mae: 1.2334
#&gt; Epoch 2/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 16us/step - loss: 2.3066 - mae: 1.1856 - val_loss: 2.2964 - val_mae: 1.2112
#&gt; Epoch 3/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 16us/step - loss: 2.1474 - mae: 1.1416 - val_loss: 2.2296 - val_mae: 1.1909
#&gt; Epoch 4/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 21us/step - loss: 2.0048 - mae: 1.1010 - val_loss: 2.1678 - val_mae: 1.1724
#&gt; Epoch 5/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 27us/step - loss: 1.8761 - mae: 1.0632 - val_loss: 2.1111 - val_mae: 1.1550
#&gt; Epoch 6/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 18us/step - loss: 1.7587 - mae: 1.0285 - val_loss: 2.0586 - val_mae: 1.1384
#&gt; Epoch 7/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 23us/step - loss: 1.6519 - mae: 0.9959 - val_loss: 2.0104 - val_mae: 1.1227
#&gt; Epoch 8/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 16us/step - loss: 1.5542 - mae: 0.9655 - val_loss: 1.9652 - val_mae: 1.1075
#&gt; Epoch 9/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 18us/step - loss: 1.4644 - mae: 0.9367 - val_loss: 1.9231 - val_mae: 1.0935
#&gt; Epoch 10/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 14us/step - loss: 1.3817 - mae: 0.9093 - val_loss: 1.8832 - val_mae: 1.0801
#&gt; Epoch 11/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 32us/step - loss: 1.3048 - mae: 0.8827 - val_loss: 1.8456 - val_mae: 1.0673
#&gt; Epoch 12/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 25us/step - loss: 1.2329 - mae: 0.8573 - val_loss: 1.8094 - val_mae: 1.0553
#&gt; Epoch 13/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 16us/step - loss: 1.1658 - mae: 0.8335 - val_loss: 1.7757 - val_mae: 1.0442
#&gt; Epoch 14/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 18us/step - loss: 1.1032 - mae: 0.8114 - val_loss: 1.7436 - val_mae: 1.0334
#&gt; Epoch 15/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 18us/step - loss: 1.0449 - mae: 0.7909 - val_loss: 1.7131 - val_mae: 1.0231
#&gt; Epoch 16/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 18us/step - loss: 0.9903 - mae: 0.7714 - val_loss: 1.6838 - val_mae: 1.0129
#&gt; Epoch 17/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 21us/step - loss: 0.9390 - mae: 0.7526 - val_loss: 1.6562 - val_mae: 1.0032
#&gt; Epoch 18/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 18us/step - loss: 0.8907 - mae: 0.7345 - val_loss: 1.6297 - val_mae: 0.9938
#&gt; Epoch 19/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 16us/step - loss: 0.8452 - mae: 0.7171 - val_loss: 1.6041 - val_mae: 0.9846
#&gt; Epoch 20/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 39us/step - loss: 0.8021 - mae: 0.7002 - val_loss: 1.5794 - val_mae: 0.9760</code></pre>
<p>Extract cross validation information:</p>
<pre class="python"><code>history_dict = history.history
loss_values = history_dict[&#39;loss&#39;]
val_loss_values = history_dict[&#39;val_loss&#39;]
epochs = range(1, len(loss_values) + 1)
cv_info = pd.DataFrame({&#39;epochs&#39;:epochs,&#39;loss_values&#39;:loss_values,&#39;val_loss_values&#39;:val_loss_values})  </code></pre>
<pre class="python"><code>cv_info
#&gt;     epochs  loss_values  val_loss_values
#&gt; 0        1     2.487254         2.369775
#&gt; 1        2     2.306626         2.296410
#&gt; 2        3     2.147378         2.229572
#&gt; 3        4     2.004820         2.167784
#&gt; 4        5     1.876132         2.111091
#&gt; 5        6     1.758654         2.058567
#&gt; 6        7     1.651914         2.010367
#&gt; 7        8     1.554202         1.965166
#&gt; 8        9     1.464412         1.923060
#&gt; 9       10     1.381660         1.883166
#&gt; 10      11     1.304796         1.845612
#&gt; 11      12     1.232922         1.809419
#&gt; 12      13     1.165803         1.775716
#&gt; 13      14     1.103157         1.743641
#&gt; 14      15     1.044881         1.713088
#&gt; 15      16     0.990320         1.683790
#&gt; 16      17     0.938994         1.656181
#&gt; 17      18     0.890735         1.629693
#&gt; 18      19     0.845238         1.604073
#&gt; 19      20     0.802120         1.579433</code></pre>
<pre class="python"><code>cv_info = pd.melt(cv_info, id_vars=[&#39;epochs&#39;], value_vars=[&#39;loss_values&#39;, &#39;val_loss_values&#39;]) </code></pre>
<pre class="python"><code>from plotnine import *

(
ggplot(cv_info,aes(&#39;epochs&#39;,&#39;value&#39;,color = &#39;variable&#39;)) +
geom_line() +
geom_point()
)
#&gt; &lt;ggplot: (-9223372036742256254)&gt;</code></pre>
<p><img src="/post/keras_example.en_files/figure-html/unnamed-chunk-17-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="prediction" class="section level1">
<h1>Prediction</h1>
<pre class="python"><code>Y_pred = model.predict(X_test)
Y_pred
#&gt; array([[12.268027],
#&gt;        [10.130738],
#&gt;        [11.916036],
#&gt;        ...,
#&gt;        [11.767007],
#&gt;        [ 9.90804 ],
#&gt;        [10.503699]], dtype=float32)</code></pre>
<pre class="python"><code>Y_pred.shape
#&gt; (1459, 1)</code></pre>
<pre class="python"><code>Y_pred = np.concatenate(Y_pred).ravel() # to flatten 2 dimition array
Y_pred = np.expm1(Y_pred)
Y_pred
#&gt; array([212781.83 ,  25101.89 , 149646.17 , ..., 128926.68 ,  20090.258,
#&gt;         36449.094], dtype=float32)</code></pre>
<pre class="python"><code>test = pd.read_csv(&#39;./data/test.csv&#39;)
#&gt; Error in py_call_impl(callable, dots$args, dots$keywords): FileNotFoundError: [Errno 2] File ./data/test.csv does not exist: &#39;./data/test.csv&#39;
#&gt; 
#&gt; Detailed traceback: 
#&gt;   File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
#&gt;   File &quot;C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\parsers.py&quot;, line 676, in parser_f
#&gt;     return _read(filepath_or_buffer, kwds)
#&gt;   File &quot;C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\parsers.py&quot;, line 448, in _read
#&gt;     parser = TextFileReader(fp_or_buf, **kwds)
#&gt;   File &quot;C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\parsers.py&quot;, line 880, in __init__
#&gt;     self._make_engine(self.engine)
#&gt;   File &quot;C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\parsers.py&quot;, line 1114, in _make_engine
#&gt;     self._engine = CParserWrapper(self.f, **self.options)
#&gt;   File &quot;C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\parsers.py&quot;, line 1891, in __init__
#&gt;     self._reader = parsers.TextReader(src, **kwds)
#&gt;   File &quot;pandas\_libs\parsers.pyx&quot;, line 374, in pandas._libs.parsers.TextReader.__cinit__
#&gt;   File &quot;pandas\_libs\parsers.pyx&quot;, line 673, in pandas._libs.parsers.TextReader._setup_parser_source
submission = pd.DataFrame({&#39;id&#39;: test[&#39;Id&#39;], &#39;SalePrice&#39;: Y_pred})
#&gt; Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;test&#39; is not defined
#&gt; 
#&gt; Detailed traceback: 
#&gt;   File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
submission.head(10)
#&gt; Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;submission&#39; is not defined
#&gt; 
#&gt; Detailed traceback: 
#&gt;   File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;</code></pre>
<pre class="python"><code>#submission.to_csv(&#39;./output/keras.csv&#39;,index = False) # save result</code></pre>
</div>
