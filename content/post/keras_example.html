---
title: Keras入门示范
slug: keras_example
author: ' '
date: '2020-05-16'
---



<p>Xiao Song</p>
<p><a href="https://xsong.ltd/" class="uri">https://xsong.ltd/</a></p>
<p>As a beginner, it’s necessary to write some simple guides using simple data.</p>
<pre class="python"><code>import warnings
warnings.filterwarnings(&#39;ignore&#39;)
import numpy as np 
import pandas as pd

X_train = pd.read_csv(&#39;E:/some_code/py_basic/house_price/data/train1.csv&#39;) # prepare data
X_test = pd.read_csv(&#39;E:/some_code/py_basic/house_price/data/test1.csv&#39;)

Y_train = np.array(X_train[&#39;SalePrice&#39;])
Y_train = np.log1p(Y_train)

X_train.drop([&#39;SalePrice&#39;],axis = 1, inplace = True)</code></pre>
<pre class="python"><code>Y_train
#&gt; array([12.24769912, 12.10901644, 12.31717117, ..., 12.25486757,
#&gt;        12.49313327, 11.86446927])</code></pre>
<pre class="python"><code>X_train.shape
#&gt; (1436, 109)</code></pre>
<pre class="python"><code>X_test.shape
#&gt; (1459, 109)</code></pre>
<div id="feature-standardization" class="section level1">
<h1>Feature Standardization</h1>
<p>Standardize features by removing the mean and scaling to unit variance:
<span class="math display">\[z = {{x - u} \over s}\]</span></p>
<p><span class="math inline">\(z\)</span> is standardize feature of <span class="math inline">\(x\)</span>, u is the mean of <span class="math inline">\(x\)</span>, and <span class="math inline">\(s\)</span> is the standard deviation of <span class="math inline">\(x\)</span>.</p>
<pre class="python"><code>#from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import StandardScaler

def standardize(df):
    &#39;&#39;&#39;standardize features&#39;&#39;&#39;
    transformer = StandardScaler().fit(df) 
    newX = transformer.transform(df)
    X = pd.DataFrame(newX,columns = df.columns)
    return X

X_train = standardize(X_train) # X train
X_test = standardize(X_test) </code></pre>
<pre class="python"><code>X_train
#&gt;       MSSubClass  MSZoningRM  ...  SaleTypeWD  SaleConditionOth
#&gt; 0       0.074674   -0.517138  ...    0.389312         -0.465778
#&gt; 1      -0.874282   -0.517138  ...    0.389312         -0.465778
#&gt; 2       0.074674   -0.517138  ...    0.389312         -0.465778
#&gt; 3       0.311913   -0.517138  ...    0.389312          2.146946
#&gt; 4       0.074674   -0.517138  ...    0.389312         -0.465778
#&gt; ...          ...         ...  ...         ...               ...
#&gt; 1431   -0.874282    1.933720  ...    0.389312         -0.465778
#&gt; 1432    0.074674   -0.517138  ...    0.389312         -0.465778
#&gt; 1433   -0.874282   -0.517138  ...    0.389312         -0.465778
#&gt; 1434    0.311913   -0.517138  ...    0.389312         -0.465778
#&gt; 1435   -0.874282   -0.517138  ...    0.389312         -0.465778
#&gt; 
#&gt; [1436 rows x 109 columns]</code></pre>
<pre class="python"><code>X_train.describe()
#&gt;          MSSubClass    MSZoningRM  ...    SaleTypeWD  SaleConditionOth
#&gt; count  1.436000e+03  1.436000e+03  ...  1.436000e+03      1.436000e+03
#&gt; mean  -5.721205e-17  3.101821e-16  ... -3.223203e-16     -1.622039e-16
#&gt; std    1.000348e+00  1.000348e+00  ...  1.000348e+00      1.000348e+00
#&gt; min   -8.742817e-01 -5.171379e-01  ... -2.568635e+00     -4.657780e-01
#&gt; 25%   -8.742817e-01 -5.171379e-01  ...  3.893119e-01     -4.657780e-01
#&gt; 50%   -1.625649e-01 -5.171379e-01  ...  3.893119e-01     -4.657780e-01
#&gt; 75%    3.119131e-01 -5.171379e-01  ...  3.893119e-01     -4.657780e-01
#&gt; max    3.158781e+00  1.933720e+00  ...  3.893119e-01      2.146946e+00
#&gt; 
#&gt; [8 rows x 109 columns]</code></pre>
<pre class="python"><code>X_train.shape[1]
#&gt; 109</code></pre>
</div>
<div id="neural-network-building" class="section level1">
<h1>Neural Network Building</h1>
<pre class="python"><code>from keras import models
#&gt; Using TensorFlow backend.
from keras import layers

model = models.Sequential()
model.add(layers.Dense(64, activation=&#39;relu&#39;,input_shape=(X_train.shape[1],)))
model.add(layers.Dense(64, activation=&#39;relu&#39;))
model.add(layers.Dense(1))

model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;mse&#39;, metrics=[&#39;mae&#39;])
model.summary()
#&gt; Model: &quot;sequential_1&quot;
#&gt; _________________________________________________________________
#&gt; Layer (type)                 Output Shape              Param #   
#&gt; =================================================================
#&gt; dense_1 (Dense)              (None, 64)                7040      
#&gt; _________________________________________________________________
#&gt; dense_2 (Dense)              (None, 64)                4160      
#&gt; _________________________________________________________________
#&gt; dense_3 (Dense)              (None, 1)                 65        
#&gt; =================================================================
#&gt; Total params: 11,265
#&gt; Trainable params: 11,265
#&gt; Non-trainable params: 0
#&gt; _________________________________________________________________</code></pre>
<pre class="python"><code>#?models.Sequential.fit</code></pre>
<pre class="python"><code>model.fit(X_train,Y_train,
    epochs = 20, # Number of epochs to train the model
    batch_size = 512) # Number of samples per gradient update.
#&gt; Epoch 1/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 155.3925 - mae: 12.4410
#&gt; 1436/1436 [==============================] - 0s 130us/step - loss: 141.8419 - mae: 11.8801
#&gt; Epoch 2/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 120.9269 - mae: 10.9759
#&gt; 1436/1436 [==============================] - 0s 8us/step - loss: 114.5524 - mae: 10.6760
#&gt; Epoch 3/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 101.0041 - mae: 10.0214
#&gt; 1436/1436 [==============================] - 0s 12us/step - loss: 95.1813 - mae: 9.7202
#&gt; Epoch 4/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 82.7641 - mae: 9.0533
#&gt; 1436/1436 [==============================] - 0s 21us/step - loss: 78.1950 - mae: 8.7888
#&gt; Epoch 5/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 66.5168 - mae: 8.0900
#&gt; 1436/1436 [==============================] - 0s 42us/step - loss: 62.8788 - mae: 7.8424
#&gt; Epoch 6/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 53.1013 - mae: 7.1717
#&gt; 1436/1436 [==============================] - 0s 19us/step - loss: 49.3402 - mae: 6.8834
#&gt; Epoch 7/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 40.5059 - mae: 6.1747
#&gt; 1024/1436 [====================&gt;.........] - ETA: 0s - loss: 39.2861 - mae: 6.0604
#&gt; 1436/1436 [==============================] - 0s 96us/step - loss: 37.7007 - mae: 5.9199
#&gt; Epoch 8/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 30.1289 - mae: 5.2018
#&gt; 1436/1436 [==============================] - 0s 33us/step - loss: 28.0882 - mae: 4.9802
#&gt; Epoch 9/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 22.6676 - mae: 4.3443
#&gt; 1436/1436 [==============================] - 0s 6us/step - loss: 20.5223 - mae: 4.1081
#&gt; Epoch 10/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 16.2448 - mae: 3.5311
#&gt; 1436/1436 [==============================] - 0s 5us/step - loss: 14.8127 - mae: 3.3574
#&gt; Epoch 11/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 11.3709 - mae: 2.8576
#&gt; 1436/1436 [==============================] - 0s 6us/step - loss: 10.7030 - mae: 2.7638
#&gt; Epoch 12/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 8.4014 - mae: 2.4070
#&gt; 1436/1436 [==============================] - 0s 6us/step - loss: 7.8423 - mae: 2.3118
#&gt; Epoch 13/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 6.2350 - mae: 2.0244
#&gt; 1436/1436 [==============================] - 0s 5us/step - loss: 5.9073 - mae: 1.9799
#&gt; Epoch 14/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 4.6460 - mae: 1.7458
#&gt; 1436/1436 [==============================] - 0s 6us/step - loss: 4.5811 - mae: 1.7272
#&gt; Epoch 15/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 3.7946 - mae: 1.5703
#&gt; 1436/1436 [==============================] - 0s 6us/step - loss: 3.6796 - mae: 1.5343
#&gt; Epoch 16/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 3.3029 - mae: 1.4636
#&gt; 1436/1436 [==============================] - 0s 5us/step - loss: 3.0489 - mae: 1.3934
#&gt; Epoch 17/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 2.5761 - mae: 1.2628
#&gt; 1436/1436 [==============================] - 0s 5us/step - loss: 2.5894 - mae: 1.2791
#&gt; Epoch 18/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 2.3425 - mae: 1.2194
#&gt; 1436/1436 [==============================] - 0s 5us/step - loss: 2.2409 - mae: 1.1893
#&gt; Epoch 19/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 2.1216 - mae: 1.1526
#&gt; 1436/1436 [==============================] - 0s 5us/step - loss: 1.9715 - mae: 1.1085
#&gt; Epoch 20/20
#&gt; 
#&gt;  512/1436 [=========&gt;....................] - ETA: 0s - loss: 1.7365 - mae: 1.0519
#&gt; 1436/1436 [==============================] - 0s 5us/step - loss: 1.7552 - mae: 1.0434
#&gt; &lt;keras.callbacks.callbacks.History object at 0x0000000055D48860&gt;
#&gt; 
#&gt; WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.</code></pre>
</div>
<div id="validation-data" class="section level1">
<h1>Validation data</h1>
<p>first I create validation set:</p>
<pre class="python"><code>x_val = X_train[:1000]
partial_x_train = X_train[1000:]
y_val = Y_train[:1000]
partial_y_train = Y_train[1000:]</code></pre>
<pre class="python"><code>history = model.fit(partial_x_train,partial_y_train,
    epochs=20,
    batch_size=512,
    validation_data=(x_val, y_val))
#&gt; Train on 436 samples, validate on 1000 samples
#&gt; Epoch 1/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 85us/step - loss: 1.4483 - mae: 0.9401 - val_loss: 1.6381 - val_mae: 1.0069
#&gt; Epoch 2/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 21us/step - loss: 1.3434 - mae: 0.9046 - val_loss: 1.6135 - val_mae: 0.9987
#&gt; Epoch 3/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 18us/step - loss: 1.2534 - mae: 0.8741 - val_loss: 1.5917 - val_mae: 0.9912
#&gt; Epoch 4/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 16us/step - loss: 1.1738 - mae: 0.8458 - val_loss: 1.5716 - val_mae: 0.9843
#&gt; Epoch 5/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 18us/step - loss: 1.1024 - mae: 0.8195 - val_loss: 1.5531 - val_mae: 0.9781
#&gt; Epoch 6/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 32us/step - loss: 1.0375 - mae: 0.7949 - val_loss: 1.5359 - val_mae: 0.9723
#&gt; Epoch 7/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 16us/step - loss: 0.9780 - mae: 0.7718 - val_loss: 1.5196 - val_mae: 0.9671
#&gt; Epoch 8/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 16us/step - loss: 0.9231 - mae: 0.7504 - val_loss: 1.5042 - val_mae: 0.9621
#&gt; Epoch 9/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 16us/step - loss: 0.8719 - mae: 0.7303 - val_loss: 1.4897 - val_mae: 0.9574
#&gt; Epoch 10/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 16us/step - loss: 0.8241 - mae: 0.7109 - val_loss: 1.4759 - val_mae: 0.9530
#&gt; Epoch 11/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 14us/step - loss: 0.7790 - mae: 0.6922 - val_loss: 1.4626 - val_mae: 0.9488
#&gt; Epoch 12/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 16us/step - loss: 0.7364 - mae: 0.6739 - val_loss: 1.4500 - val_mae: 0.9448
#&gt; Epoch 13/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 11us/step - loss: 0.6963 - mae: 0.6559 - val_loss: 1.4376 - val_mae: 0.9404
#&gt; Epoch 14/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 14us/step - loss: 0.6588 - mae: 0.6383 - val_loss: 1.4260 - val_mae: 0.9362
#&gt; Epoch 15/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 11us/step - loss: 0.6232 - mae: 0.6210 - val_loss: 1.4144 - val_mae: 0.9319
#&gt; Epoch 16/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 11us/step - loss: 0.5895 - mae: 0.6044 - val_loss: 1.4033 - val_mae: 0.9278
#&gt; Epoch 17/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 14us/step - loss: 0.5577 - mae: 0.5881 - val_loss: 1.3926 - val_mae: 0.9241
#&gt; Epoch 18/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 14us/step - loss: 0.5278 - mae: 0.5726 - val_loss: 1.3820 - val_mae: 0.9203
#&gt; Epoch 19/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 14us/step - loss: 0.4996 - mae: 0.5574 - val_loss: 1.3720 - val_mae: 0.9167
#&gt; Epoch 20/20
#&gt; 
#&gt; 436/436 [==============================] - 0s 11us/step - loss: 0.4729 - mae: 0.5428 - val_loss: 1.3618 - val_mae: 0.9129</code></pre>
<p>Extract cross validation information:</p>
<pre class="python"><code>history_dict = history.history
loss_values = history_dict[&#39;loss&#39;]
val_loss_values = history_dict[&#39;val_loss&#39;]
epochs = range(1, len(loss_values) + 1)
cv_info = pd.DataFrame({&#39;epochs&#39;:epochs,&#39;loss_values&#39;:loss_values,&#39;val_loss_values&#39;:val_loss_values})  </code></pre>
<pre class="python"><code>cv_info
#&gt;     epochs  loss_values  val_loss_values
#&gt; 0        1     1.448276         1.638064
#&gt; 1        2     1.343392         1.613477
#&gt; 2        3     1.253372         1.591659
#&gt; 3        4     1.173802         1.571612
#&gt; 4        5     1.102398         1.553143
#&gt; 5        6     1.037460         1.535872
#&gt; 6        7     0.977973         1.519616
#&gt; 7        8     0.923061         1.504239
#&gt; 8        9     0.871926         1.489711
#&gt; 9       10     0.824143         1.475908
#&gt; 10      11     0.779022         1.462641
#&gt; 11      12     0.736392         1.449981
#&gt; 12      13     0.696348         1.437634
#&gt; 13      14     0.658771         1.425955
#&gt; 14      15     0.623234         1.414390
#&gt; 15      16     0.589503         1.403301
#&gt; 16      17     0.557725         1.392590
#&gt; 17      18     0.527814         1.382006
#&gt; 18      19     0.499623         1.372013
#&gt; 19      20     0.472888         1.361787</code></pre>
<pre class="python"><code>cv_info = pd.melt(cv_info, id_vars=[&#39;epochs&#39;], value_vars=[&#39;loss_values&#39;, &#39;val_loss_values&#39;]) </code></pre>
<pre class="python"><code>from plotnine import *

(
ggplot(cv_info,aes(&#39;epochs&#39;,&#39;value&#39;,color = &#39;variable&#39;)) +
geom_line() +
geom_point()
)
#&gt; &lt;ggplot: (112676044)&gt;</code></pre>
<p><img src="/post/keras_example_files/figure-html/unnamed-chunk-17-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="prediction" class="section level1">
<h1>Prediction</h1>
<pre class="python"><code>Y_pred = model.predict(X_test)
Y_pred
#&gt; array([[11.550178],
#&gt;        [13.072351],
#&gt;        [11.186147],
#&gt;        ...,
#&gt;        [13.100971],
#&gt;        [ 9.631788],
#&gt;        [11.013977]], dtype=float32)</code></pre>
<pre class="python"><code>Y_pred.shape
#&gt; (1459, 1)</code></pre>
<pre class="python"><code>Y_pred = np.concatenate(Y_pred).ravel() # to flatten 2 dimition array
Y_pred = np.expm1(Y_pred)
Y_pred
#&gt; array([103794.47 , 475608.03 ,  72123.33 , ..., 489416.53 ,  15240.668,
#&gt;         60715.883], dtype=float32)</code></pre>
<pre class="python"><code>test = pd.read_csv(&#39;E:/some_code/py_basic/house_price/data/test.csv&#39;)
submission = pd.DataFrame({&#39;id&#39;: test[&#39;Id&#39;], &#39;SalePrice&#39;: Y_pred})
submission.head(10)
#&gt;      id      SalePrice
#&gt; 0  1461  103794.468750
#&gt; 1  1462  475608.031250
#&gt; 2  1463   72123.328125
#&gt; 3  1464   56685.023438
#&gt; 4  1465   46361.867188
#&gt; 5  1466  109739.531250
#&gt; 6  1467   83739.242188
#&gt; 7  1468  177004.093750
#&gt; 8  1469  165609.843750
#&gt; 9  1470  522844.250000</code></pre>
<pre class="python"><code>#submission.to_csv(&#39;./output/keras.csv&#39;,index = False) # save result</code></pre>
</div>
