---
title: "初中生学习成绩的影响因素: 基于分类与回归树的分析"
author: ' '
date: '2019-05-08'
slug: edutree
---



<div class="section level2">
<h2>数据</h2>
<p>本文采用<a href="https://ceps.ruc.edu.cn/">中国教育追踪调查</a> (China Education Panel Survey, CEPS)2013-2014学年基线数据。 采用PPS抽样方法，以人口平均受教育水平和人口比例为分层变量并通过两阶段分层抽样从从全国随机抽取了28个县级单位作为调查点。具体而言，CEPS在每个入样县（区）所辖地理范围内分别抽取4所初中学校。并在每所入样学校中分别抽取4个班级，包括2个七年级班和2个九年级班。</p>
</div>
<div class="section level2">
<h2>模型与算法</h2>
<p>CART算法全称为分类与回归树（Classification and Regression Trees）。它可以处理分类与回归算法，以及生存分析因变量。作为一种非参数的机器学习方法，CART决策树无需对数据的分布做任何假定。CART算法划分数据的依据是变量的取值顺序，因此它对异常值不敏感。最后，通过交叉验证（Cross Validation）的方法求得预测误差。 回归树模型可表示为： <span class="math display">\[f(x) = \sum\limits_{m = 1}^M {{c_m}I(x \in {R_m})} \]</span></p>
<p>其中，<span class="math inline">\(x\)</span>是一系列输入特征（自变量），<span class="math inline">\({R_1},{R_2},...,{R_m}\)</span>是输入空间被划分的M个区域。 是区域<span class="math inline">\({R_m}\)</span>对应的最优值。<span class="math inline">\(I\)</span>代表的是指示函数（indicator function），当输入变量<span class="math inline">\(x\)</span>属于区域 <span class="math inline">\({R_m}\)</span>时，输出为1，否则输出为0。 CART算法选择基尼系数进行属性划分。CART算法可以运用于分类和回归问题中。</p>
<p>为了防止过拟合问题(Over-fitted),需要对过于复杂的树模型进行剪枝。这也是训练模型的重要过程。通过改善模型的复杂度参数(Complexity Parameter, CP)。在CART算法中，复杂度参数定义如下：</p>
<p><span class="math display">\[{{\rm{C}}_\alpha }(T) = C(T) + \alpha \left| T \right|\]</span></p>
<p>其中，<span class="math inline">\(T\)</span>为任意子树。<span class="math inline">\(C(T)\)</span>为对训练数据的预测误差（如基尼指数）， <span class="math inline">\(\left| T \right|\)</span>为子树的叶结点个数，<span class="math inline">\(\alpha \ge 0\)</span>为参数，<span class="math inline">\({{\rm{C}}_\alpha }(T)\)</span>是参数为<span class="math inline">\(\alpha\)</span>时的子树<span class="math inline">\(T\)</span>的整体损失。参数<span class="math inline">\(\alpha\)</span>测量了模型的复杂度。</p>
</div>
<div class="section level2">
<h2>变量</h2>
<p>本研究依据以往教育学和社会学文献，选取了50个自变量进行预测。为了降低测量误差，本研究在变量选取的过程中遵循了以下两个原则：1.尽可能挑选“客观的”变量。2.尽可能挑选直观上对学习成绩有重要影响的变量。</p>
<div class="section level3">
<h3>因变量</h3>
<p>本研究的因变量是被调查学生上一次期中考试的成绩。包括数学成绩、语文成绩、英语成绩。我们将同时对总成绩进行分析。</p>
<p>为了讨论学习成绩的门槛效应，我们将数学成绩是否位于所有学生的前25%(是=1，否=0)单独划分为一个分类变量建立分类树。下图展示了标准化总成绩的密度直方图估计。箱线图展示了我们抽取的30所学校的数学原始成绩差异。我们可以发现，标准化总成绩基本上呈现了正态分布。不同学校间的数学成绩差异十分显著。</p>
</div>
<div class="section level3">
<h3>自变量</h3>
<p>下面展示了自变量以及自变量在模型中的编码：</p>
<p>家庭变量：每星期零用钱(money)、上兴趣班费用总计(clfee)、监督孩子的作业(qianzi)、花在孩子身上的时间(lifetm)、孩子交流方言(dial)、父母交流方言(chidia)、家长教育期望(eduyexp)、对孩子未来的信心(futcfd)、孩子户口类型(huko)、家长教育程度(eduy)、家长政治面貌(dangy)、住房是否生产经营用(houspro)。</p>
<p>个人变量：性别(sex)、是否独生子女(onechi)、父亲教育水平(faedu)、母亲教育水平(maedu)、爸爸经常酗酒(drunk)、父母经常吵架(qurel)、父母之间关系很好(relation)、有独立书桌(desk)、家里有电脑和网络(net)、家庭交流方言(dialect)、父母督促学习天数(chkhmwk chkcouse)、父母教育期望(eduexp)。</p>
<p>学校变量：学校性质(schtype)、教室数量(schcsrm)、学校电脑数(comno)、图书数量(bknum)、生均财政拨款(buget)、持有教师资格证人数(eduqua)、打架斗殴(fight)、破坏公物(brkpb)、吸烟(smok)、饮酒(drink)、高级教师年收入(teainc)。</p>
<p>班级变量：教师总课时(classtm)、 备课时间(clpre)、批改作业时间(revitm)、班主任教授本班科目(subject)、认识多少家长(know)、是否有抽烟喝酒的学生(drsmok)、与学生交流时间(commhr)。</p>
<p><img src="/post/edutree_files/figure-html/add%20-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>为了直观地描述不同学校与学习成绩的差异，下面展示了以学校为分组变量，原始数学成绩为因变量的箱线图。</p>
<pre class="r"><code>setwd(&quot;E:/edu/data2&quot;)
ceps &lt;- read_dta(&quot;E:/edu/data2/ceps.dta&quot;)
#抽出30所学校，作成绩箱线图
ceps1 &lt;- ceps %&gt;% 
group_by(schids) %&gt;% 
summarize(mathmean = mean(tr_mat, na.rm = T))
set.seed(2019)
sample &lt;- sample(1:nrow(ceps1),30,replace = F)#随机抽取30个学校
ceps1 &lt;- ceps1[sample,]
ceps1 &lt;- ceps1$schids #提取抽中学校编号的向量
delte &lt;- subset(ceps,schids == ceps1[1])
#遍历抽中的学校id，纵向合并数据框
for (i in ceps1) delte &lt;- rbind(delte, subset(ceps,schids == i)) 
#箱线图
delte$schids &lt;- as.factor(delte$schids)  
ggplot(delte, aes(schids,tr_mat)) + 
  geom_boxplot(size=0.1) +
  xlab(&#39;学校代码&#39;) + ylab(&#39;数学原始成绩&#39;)</code></pre>
<p><img src="/post/edutree_files/figure-html/a-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#ceps1 &lt;- subset(ceps,schids&lt;=30)

ceps &lt;- lapply(ceps, unclass)
ceps &lt;- data.frame(ceps)</code></pre>
</div>
</div>
<div class="section level2">
<h2>分析结果</h2>
<div class="section level3">
<h3>回归树</h3>
<p>首先我们将展示总成绩的回归结果。由于七年级和九年级学生考试总分的不同。我们将九年级子样本单独进行分析，并采用10折交叉验证计算模型的验证误差。</p>
<pre class="r"><code>cont &lt;- rpart.control(minsplit=5,maxcompete=100,xval=10,maxdepth=30,cp=0.01)
tree &lt;- rpart(sdtotal ~ schids + sex + onechi +  maedu +  faedu + drunk + 
qurel + relation +  desk + net + dialect + chkhmwk + chkcouse + 
classtm +  clpre + revitm + subject + know + drsmok + commhr + 
schtype + schcsrm +  comno +  bknum + buget +  eduqua + fight + 
brkpb + smok +  drink +  teainc + money + clfee + qianzi + lifetm +  
dial +  chidia +  futcfd +  huko + eduy +  dangy +  houspro ,data = 
subset(ceps,grade9 == 1),weights = sweight,method = &quot;anova&quot;,parms = 
list(split=&quot;gini&quot;),control=cont,na.action = na.omit)</code></pre>
<pre class="r"><code>tree$cptable #复杂度参数表</code></pre>
<pre><code>          CP nsplit rel error    xerror         xstd
1 0.07468185      0 1.0000000 1.0008194 0.0005634699
2 0.03717668      1 0.9253181 0.9281575 0.0005277763
3 0.01820999      2 0.8881415 0.9108954 0.0005315458
4 0.01328867      3 0.8699315 0.9052528 0.0005502155
5 0.01308371      4 0.8566428 0.8989148 0.0005484642
6 0.01281814      5 0.8435591 0.8989148 0.0005484642
7 0.01000000      6 0.8307410 0.8843597 0.0005490267</code></pre>
<pre class="r"><code>rpart.plot(tree)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:sfff"></span>
<img src="/post/edutree_files/figure-html/sfff-1.png" alt="总成绩的回归树" width="100%" />
<p class="caption">
Figure 1: 总成绩的回归树
</p>
</div>
<p>下面展示的是数学原始成绩是否位于前25%的分类决策树。首先，我们在备选的约50个自变量中纳入学校哑变量。仅考虑家庭、个体、班级因素对学生学习成绩的影响。通过划分训练集和测试集的方式计算模型的预测错误率。检验模型的泛化能力。</p>
<p><a href="https://www.jianshu.com/p/08c28c5c55a7">R语言表格</a></p>
<pre class="r"><code>set.seed(1212323) #设置随机数种子
sampled &lt;- sample(1:nrow(ceps),nrow(ceps)*0.3,replace=FALSE) #抽取30%样本作为验证集
test &lt;- ceps[sampled,] #测试集
train &lt;- ceps[-sampled,] #训练集

cont &lt;- rpart.control(minsplit=5,maxcompete=100,xval=10,maxdepth=30,cp=0.01) #设置参数

cltree3 &lt;- rpart(matgreat ~  schids + sex + onechi +  maedu +  faedu + drunk + 
qurel + relation + desk + net + dialect + chkhmwk + chkcouse + 
classtm +  clpre + revitm + subject + know + drsmok + commhr + 
schtype + schcsrm + comno +  bknum + buget + eduqua + fight + 
brkpb + smok +  drink +  teainc + money + clfee + qianzi + lifetm +  
dial +  chidia +  futcfd +  huko + eduy +  dangy +  houspro ,data = 
subset(train,grade9 == 1),weights = sweight,method = &quot;class&quot;,parms = 
list(split=&quot;gini&quot;),control=cont,na.action = na.omit,model=T) #决策树
#knitr::kable(printcp(cltree3),caption=&quot;复杂度参数表&quot;,digits =4)
rpart.plot(cltree3)</code></pre>
<p><img src="/post/edutree_files/figure-html/c%20-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>test &lt;- na.omit(test) #删除缺失值
y.pr &lt;- predict(cltree3,test,type=&quot;prob&quot;) #预测测试集概率

pr &lt;- y.pr[,2] 
test &lt;- cbind(test,pr)
modelroc &lt;- roc(test$matgreat,test$pr)
plot(modelroc,xlab =&#39;伪正类率&#39;,ylab=&#39;真正类率&#39;) #ROC 曲线</code></pre>
<p><img src="/post/edutree_files/figure-html/c%20-2.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>test$yhat &lt;- ifelse(test$pr &gt;0.5,&#39;1&#39;,&#39;0&#39;) #大于0.5设置为&quot;是&quot;
attach(test)
tab &lt;- table(yhat,matgreat) 
names(dimnames(tab))&lt;-c(&quot;预测值&quot;,&quot;真实值&quot;)
treeerror &lt;- (sum(tab)-sum(diag(tab)))/sum(tab)
tab</code></pre>
<pre><code>      真实值
预测值   0   1
     0 875 183
     1 138  85</code></pre>
<pre class="r"><code>treeerror #测试集错误率</code></pre>
<pre><code>[1] 0.2505855</code></pre>
<pre class="r"><code>cont &lt;- rpart.control(minsplit=5,maxcompete=100,xval=10,maxdepth=30,cp=0.014) #设置参数

cltree3 &lt;- rpart(matgreat ~  schids + sex + onechi +  maedu +  faedu + drunk + 
qurel + relation + desk + net + dialect + chkhmwk + chkcouse + 
classtm +  clpre + revitm + subject + know + drsmok + commhr + 
schtype + schcsrm + comno + bknum + buget + eduqua + fight + 
brkpb + smok +  drink +  teainc + money + clfee + qianzi + lifetm +  
dial +  chidia +  futcfd + huko + eduy + dangy +  houspro ,data = 
subset(train,grade9 == 1),weights = sweight,method = &quot;class&quot;,parms = 
list(split=&quot;gini&quot;),control=cont,na.action = na.omit) #决策树

#knitr::kable(printcp(cltree3),caption=&quot;复杂度参数表&quot;,digits =4)
cltree3$variable.importance #各个变量的重要性程度</code></pre>
<pre><code>     smok     bknum   classtm    eduqua     buget   subject    revitm 
92336.104 87082.851 72098.477 42822.536 37990.537 28371.716 26624.118 
   schids      know     fight     brkpb     comno    commhr     clpre 
25602.670 25452.345 23572.820 22293.765 21516.771 18454.934 16719.141 
   chidia   schcsrm    teainc   chkhmwk 
 9999.162  9588.660  2283.551  2032.974 </code></pre>
<pre class="r"><code>rpart.plot(cltree3)</code></pre>
<p><img src="/post/edutree_files/figure-html/d%20-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>test &lt;- na.omit(test) #删除缺失值
y.pr &lt;- predict(cltree3,test,type=&quot;prob&quot;) #预测测试集概率

pr &lt;- y.pr[,2] 
test &lt;- cbind(test,pr)
modelroc &lt;- roc(test$matgreat,test$pr)
auc(modelroc) #计算AUC值</code></pre>
<pre><code>Area under the curve: 0.6951</code></pre>
<pre class="r"><code>plot(modelroc,xlab =&#39;伪正类率&#39;,ylab=&#39;真正类率&#39;) #ROC 曲线</code></pre>
<p><img src="/post/edutree_files/figure-html/d%20-2.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>test$yhat &lt;- ifelse(test$pr &gt;0.5,&#39;1&#39;,&#39;0&#39;) #大于0.5设置为&quot;是&quot;
attach(test)
tab &lt;- table(yhat,matgreat) 
treeerror &lt;- (sum(tab)-sum(diag(tab)))/sum(tab)
names(dimnames(tab))&lt;-c(&quot;预测值&quot;,&quot;真实值&quot;) #矩阵小标题
tab #混淆矩阵，行为测试集预测值，列为测试集观测值</code></pre>
<pre><code>      真实值
预测值   0   1
     0 875 183
     1 138  85</code></pre>
<pre class="r"><code>treeerror #测试集错误率</code></pre>
<pre><code>[1] 0.2505855</code></pre>
<p>可以发现，对因变量分类有显著贡献的变量有学校哑变量、对孩子未来的信心、上兴趣班费用总计、与学生交流时间。可见学校层面的特征对数学成绩的影响非常强。</p>
<p>在初次拟合分类树之后，我们对分类树进行剪枝。通过第一棵分类树的结果，我们选取预测误差(即表中的xerror通过交叉验证获得)最小的CP值带入到下一个模型中，最终的验证集错误率在25%左右。</p>
</div>
<div class="section level3">
<h3><a href="http://topepo.github.io/caret/train-models-by-tag.html#implicit-feature-selection">交叉验证</a></h3>
<p>使用<code>caret</code>包进行10次10折交叉验证</p>
<pre class="r"><code>library(caret)
#install.packages(&#39;e1071&#39;)
library(e1071)

ceps$matgreat &lt;- as.factor(ceps$matgreat)

repeatedcv &lt;-trainControl(method =&quot;repeatedcv&quot;, number =10,repeats =10, savePredictions=TRUE)

 rpartFit &lt;- train(matgreat ~ schids + sex + onechi +  maedu +  faedu + drunk + qurel + relation + desk + net + dialect + chkhmwk + chkcouse + classtm +  clpre + revitm + subject + know + drsmok + commhr + schtype + schcsrm + comno + bknum + buget + eduqua + fight + brkpb + smok + drink + teainc + money + clfee + qianzi + lifetm +  dial +  chidia +  futcfd + huko + eduy + dangy +  houspro  ,
                    data = ceps,
                    method = &quot;rpart&quot;,
                    trControl = repeatedcv,
                    na.action = na.omit)
rpartFit</code></pre>
<pre><code>CART 

19487 samples
   42 predictor
    2 classes: &#39;0&#39;, &#39;1&#39; 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 4101, 4101, 4101, 4101, 4101, 4102, ... 
Resampling results across tuning parameters:

  cp          Accuracy   Kappa     
  0.03015564  0.8039239  0.28843270
  0.03631647  0.7957860  0.23829366
  0.07295720  0.7786492  0.08288264

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was cp = 0.03015564.</code></pre>
</div>
<div class="section level3">
<h3>随机森林</h3>
<pre class="r"><code>train$matgreat &lt;- as.factor(train$matgreat)
train &lt;- na.omit(train)
train &lt;- filter(train, is.na(matgreat)==F)

ratree &lt;- randomForest(matgreat ~ schids + sex + onechi +  maedu +  faedu + drunk + qurel + relation + desk + net + dialect + chkhmwk + chkcouse + classtm +  clpre + revitm + subject + know + drsmok + commhr + schtype + schcsrm + comno + bknum + buget + eduqua + fight + brkpb + smok + drink + teainc + money + clfee + qianzi + lifetm +  dial +  chidia +  futcfd + huko + eduy + dangy +  houspro  ,data = subset(train,grade9 == 1),importance=T,proximity=T,mtry=16)

#ratree
#summary(ratree)
#MDSplot(ratree,matgreat,palette=NULL)

importance(ratree, type=1, scale=T) #计算模型变量的重要性</code></pre>
<pre><code>         MeanDecreaseAccuracy
schids             16.8903964
sex                -0.5527399
onechi              5.9131737
maedu              -3.3066436
faedu               2.7760047
drunk              -0.4609220
qurel               2.0729488
relation            0.2582815
desk                4.7214681
net                 3.2923147
dialect             6.6658668
chkhmwk            -1.3403267
chkcouse            0.2740335
classtm            19.5927223
clpre              13.9225375
revitm              9.6381463
subject            14.4385027
know                8.0538974
drsmok             13.7351259
commhr             12.7966501
schtype             4.4636744
schcsrm            16.2866687
comno              16.2330809
bknum              20.1023533
buget              18.8540922
eduqua             17.9241374
fight               6.8018800
brkpb               8.0343708
smok               12.4643054
drink               2.1355375
teainc             29.4668796
money               6.1160261
clfee               8.8551999
qianzi              6.0560669
lifetm              0.2319042
dial                9.3079861
chidia              6.7512246
futcfd             16.3089049
huko                2.8794747
eduy               -0.5798589
dangy               2.2516123
houspro             4.6543525</code></pre>
</div>
</div>
