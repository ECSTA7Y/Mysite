---
title: "交叉验证"
author: ' '
date: '2019-05-25'
slug: cross
tags: ["R","应用统计","机器学习"]
output:
  html_document:
    toc: true
    theme: united
---



<div id="section" class="section level1">
<h1>基本概念</h1>
<p>交叉验证是机器学习中的常用模型选择方法。其中最常用的方法是K折交叉验证(k-fold cross validation)。K折交叉验证重复使用数据，把给定的数据切分为K个互斥子集，每次使用1个子集作为测试集，使用余下k-1个子集的并集作为训练集。其中，训练集用于模型的训练，测试集用于模型的评估和选择。在样本量不够充足的情况下，交叉验证法通过重复使用数据能够减少样本划分不同导致的差别，并且选择测试误差最小的模型，增强模型的泛化能力。</p>
<p>交叉验证主要有以下种类：</p>
<ul>
<li><p>简单交叉验证 (hold-out cross validation)</p></li>
<li><p>k折交叉验证 (K-fold cross-validation)</p></li>
<li><p>留一交叉验证(leave-one-out cross validation)</p></li>
</ul>
<p>谢益辉的统计动画R包<a href="https://yihui.name/animation/example/cv-ani/">animation</a>直观地展示了交叉验证方法。</p>
<pre class="r"><code>library(caret)</code></pre>
</div>
<div id="r" class="section level1">
<h1>R示例</h1>
<div id="k-fold-cross-validation" class="section level2">
<h2>10折交叉验证 (K-fold cross-validation)</h2>
<pre class="r"><code>rfControl &lt;-trainControl( #10折交叉验证
method =&quot;cv&quot;, 
number =10 
)

library(mlbench) #加载数据集
data(BostonHousing)
head(BostonHousing)</code></pre>
<pre><code>     crim zn indus chas   nox    rm  age    dis rad tax ptratio      b
1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90
2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90
3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83
4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63
5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90
6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12
  lstat medv
1  4.98 24.0
2  9.14 21.6
3  4.03 34.7
4  2.94 33.4
5  5.33 36.2
6  5.21 28.7</code></pre>
<pre class="r"><code>nrow(BostonHousing) #样本量</code></pre>
<pre><code>[1] 506</code></pre>
<pre class="r"><code>rpartFit &lt;- train(medv ~ .,
                  data = BostonHousing,
                  method = &quot;rpart&quot;,
                  trControl = rfControl)
rpartFit</code></pre>
<pre><code>CART 

506 samples
 13 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 455, 454, 456, 455, 455, 456, ... 
Resampling results across tuning parameters:

  cp          RMSE      Rsquared   MAE     
  0.07165784  5.773994  0.6132864  4.117588
  0.17117244  6.614233  0.4743554  4.910051
  0.45274420  8.226960  0.3374905  5.943182

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.07165784.</code></pre>
<p>将数据集D划分为K个子集同样存在多种划分方式。为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p 次k折交叉验证结果的均值， 例如常见的有“10次10 折交叉验证”。</p>
<pre class="r"><code>repeatedcv &lt;-trainControl(method =&quot;repeatedcv&quot;, number =10,repeats =10, savePredictions
=TRUE)
rpartFit &lt;- train(medv ~ .,
                  data = BostonHousing,
                  method = &quot;rpart&quot;,
                  trControl = repeatedcv)
rpartFit</code></pre>
<pre><code>CART 

506 samples
 13 predictor

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 454, 456, 454, 455, 456, 455, ... 
Resampling results across tuning parameters:

  cp          RMSE      Rsquared   MAE     
  0.07165784  5.795540  0.6026849  4.170954
  0.17117244  6.656407  0.4715952  4.911988
  0.45274420  8.430011  0.3086941  6.104030

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.07165784.</code></pre>
<pre class="r"><code>importance &lt;- varImp(rpartFit, scale = F)
# 得到各个变量的重要性
plot(importance, xlab = &quot;变量重要性&quot;)</code></pre>
<p><img src="/post/cross_files/figure-html/cv-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="leave-one-out-cross-validation" class="section level2">
<h2>留一交叉验证(leave-one-out cross validation)</h2>
<pre class="r"><code>rfControl &lt;-trainControl(
method =&quot;LOOCV&quot;
) #留一交叉验证
rpartFit &lt;- train(medv ~ .,
                  data = BostonHousing,
                  method = &quot;rpart&quot;,
                  trControl = rfControl)
rpartFit</code></pre>
<pre><code>CART 

506 samples
 13 predictor

No pre-processing
Resampling: Leave-One-Out Cross-Validation 
Summary of sample sizes: 505, 505, 505, 505, 505, 505, ... 
Resampling results across tuning parameters:

  cp          RMSE      Rsquared      MAE     
  0.07165784  6.060240  0.5665051649  4.289854
  0.17117244  7.489499  0.3524415744  6.088882
  0.45274420  9.700709  0.0008358062  7.539715

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.07165784.</code></pre>
<pre class="r"><code>#summary(rpartFit)
varImp(rpartFit, scale = F)</code></pre>
<pre><code>rpart variable importance

        Overall
lstat    0.8646
nox      0.5008
ptratio  0.4643
rm       0.4527
indus    0.2595
crim     0.2484
age      0.2089
chas1    0.0000
dis      0.0000
zn       0.0000
tax      0.0000
rad      0.0000
b        0.0000</code></pre>
<pre class="r"><code># 得到各个变量的重要性</code></pre>
<ul>
<li>我们发现个别变量对于预测完全没有帮助，故可以将这些变量在之后的分析中删除。</li>
</ul>
</div>
</div>
<div id="caretk" class="section level1">
<h1>使用<code>caret</code>包训练K近邻模型</h1>
<ul>
<li>数据：<a href="https://www.kaggle.com/abcsds/pokemon">口袋妖怪</a>数据集</li>
<li>任务：通过各种属性预测神奇宝贝是否为传说级别</li>
</ul>
<blockquote>
<ul>
<li>ID,每只神奇宝贝的ID<br />
</li>
<li>Name,每只神奇宝贝的名字<br />
</li>
<li>Type1:每只神奇宝贝的类型，比如说水系，比如说火系<br />
</li>
<li>Type2:由于有特殊的神奇宝贝拥有复数以上的属性。<br />
</li>
<li>Total:指每只神奇宝贝的强度，一般越高越强<br />
</li>
<li>HP：指每只神奇宝贝的生命值<br />
</li>
<li>Attack:指每只神奇宝贝的攻击力<br />
</li>
<li>Defense:指每只神奇宝贝的防御力<br />
</li>
<li>SP Attack:指每只神奇宝贝面对相克属性神奇宝贝时的攻击力，通常会比普通攻击力高<br />
</li>
<li>SP Defense:指每只神奇宝贝面对相克属性神奇宝贝时的防御力，通常会比普通防御力高。<br />
</li>
<li>Speed:指每只神奇宝贝的速度<br />
</li>
<li>Generation：指每一只神奇宝贝属于哪一部神奇宝贝的，目前分为五部。</li>
<li>Legendary:指每一只神奇宝贝是不是属于传说级别的神奇宝贝，比如麒麟，超梦，梦幻之类的。</li>
</ul>
</blockquote>
<p><a href="https://dataaspirant.com/2017/01/09/knn-implementation-r-using-caret-package/">代码参考</a></p>
<pre class="r"><code>library(readr)
library(caret)
library(kknn)

pokemon &lt;- read_csv(&quot;E:/R_codes/others/Pokemon.csv&quot;)
colnames(pokemon)[1] &lt;- &#39;id&#39; #修改变量名
colnames(pokemon)[3] &lt;- &#39;Type1&#39;
colnames(pokemon)[4] &lt;- &#39;Type2&#39;

pokemon$Legendary &lt;- as.factor(pokemon$Legendary)
pokemon &lt;- na.omit(pokemon)
trctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10)

pokemon &lt;- na.omit(pokemon)

set.seed(3333)
knn_fit &lt;- train(Legendary ~ Type1 + Type2 + Total+ HP + Defense+ Attack + Speed + Generation, data = pokemon, method = &quot;knn&quot;,
 trControl=trctrl,
 preProcess = c(&quot;center&quot;, &quot;scale&quot;), #centering and scaling the data
 tuneLength = 10)
knn_fit</code></pre>
<pre><code>k-Nearest Neighbors 

414 samples
  8 predictor
  2 classes: &#39;FALSE&#39;, &#39;TRUE&#39; 

Pre-processing: centered (44), scaled (44) 
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 373, 373, 372, 372, 372, 373, ... 
Resampling results across tuning parameters:

  k   Accuracy   Kappa      
   5  0.9188560  0.300507757
   7  0.9084495  0.178126329
   9  0.9057433  0.146422463
  11  0.8987631  0.005620524
  13  0.9036063  0.010462884
  15  0.9033682  0.000000000
  17  0.9033682  0.000000000
  19  0.9033682  0.000000000
  21  0.9036121  0.003756345
  23  0.9038502  0.007518721

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was k = 5.</code></pre>
</div>
