---
title: "交叉验证：基于caret包"
author: ' '
date: '2019-05-25'
slug: cross
tags: ["R","应用统计","机器学习"]
output:
  html_document2:
    toc: true
    theme: united
---



<div id="section" class="section level1">
<h1>基本概念</h1>
<p>交叉验证是机器学习中的常用模型选择方法。其中最常用的方法是K折交叉验证(k-fold cross validation)。K折交叉验证重复使用数据，把给定的数据切分为K个互斥子集，每次使用1个子集作为测试集，使用余下k-1个子集的并集作为训练集。其中，训练集用于模型的训练，测试集用于模型的评估和选择。在样本量不够充足的情况下，交叉验证法通过重复使用数据能够减少样本划分不同导致的差别，并且选择测试误差最小的模型，增强模型的泛化能力。</p>
<p>划分测试集和训练集同样有助于更加客观地评价模型的泛化能力。同时，交叉验证也是模型调参的过程。</p>
<p>交叉验证主要有以下种类：</p>
<ul>
<li><p>简单交叉验证 (hold-out cross validation)</p></li>
<li><p>k折交叉验证 (K-fold cross-validation)</p></li>
<li><p>留一交叉验证(leave-one-out cross validation)</p></li>
</ul>
<p>谢益辉的统计动画R包<a href="https://yihui.name/animation/example/cv-ani/">animation</a>直观地展示了交叉验证方法。</p>
<pre class="r"><code>`%&gt;%` &lt;- magrittr::`%&gt;%`
library(caret)
library(readr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(pROC)
library(ROSE)</code></pre>
</div>
<div id="r" class="section level1">
<h1>R示例</h1>
<div id="k-fold-cross-validation" class="section level2">
<h2>10折交叉验证 (K-fold cross-validation)</h2>
<pre class="r"><code>rfControl &lt;-trainControl( #10折交叉验证
method =&quot;cv&quot;, 
number =10 
)

library(mlbench) #加载数据集
data(BostonHousing)
head(BostonHousing)</code></pre>
<pre><code>     crim zn indus chas   nox    rm  age    dis rad tax ptratio      b
1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90
2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90
3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83
4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63
5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90
6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12
  lstat medv
1  4.98 24.0
2  9.14 21.6
3  4.03 34.7
4  2.94 33.4
5  5.33 36.2
6  5.21 28.7</code></pre>
<pre class="r"><code>nrow(BostonHousing) #样本量</code></pre>
<pre><code>[1] 506</code></pre>
<pre class="r"><code>rpartFit &lt;- train(medv ~ .,
                  data = BostonHousing,
                  method = &quot;rpart&quot;,
                  trControl = rfControl)
rpartFit</code></pre>
<pre><code>CART 

506 samples
 13 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 456, 455, 455, 455, 455, 455, ... 
Resampling results across tuning parameters:

  cp          RMSE      Rsquared   MAE     
  0.07165784  5.728939  0.6124071  4.108182
  0.17117244  6.592267  0.4785617  4.839885
  0.45274420  8.198177  0.3359489  5.976964

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.07165784.</code></pre>
<p>将数据集D划分为K个子集同样存在多种划分方式。为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p 次k折交叉验证结果的均值， 例如常见的有“10次10 折交叉验证”。</p>
<pre class="r"><code>repeatedcv &lt;-trainControl(method =&quot;repeatedcv&quot;, number =10,repeats =10, savePredictions
=T)
rpartFit &lt;- train(medv ~ .,
                  data = BostonHousing,
                  method = &quot;rpart&quot;,
                  trControl = repeatedcv)
rpartFit</code></pre>
<pre><code>CART 

506 samples
 13 predictor

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 454, 458, 455, 455, 456, 455, ... 
Resampling results across tuning parameters:

  cp          RMSE      Rsquared   MAE     
  0.07165784  5.664894  0.6217557  4.119880
  0.17117244  6.509313  0.4942258  4.869885
  0.45274420  8.457328  0.3150451  6.128833

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.07165784.</code></pre>
<pre class="r"><code>importance &lt;- varImp(rpartFit, scale = F)
imp &lt;- as.data.frame(importance[[&quot;importance&quot;]])

imp &lt;- imp %&gt;%
  mutate(name=rownames(imp)) %&gt;%
  filter(Overall&gt;0) %&gt;%
  rename(&#39;impt&#39;=&#39;Overall&#39;) #%&gt;%
  #mutate(varname = fct_reorder(varname,impt))

ggplot(imp,aes(reorder(name,impt),impt)) +
  geom_segment( aes(x=reorder(name,impt), xend=reorder(name,impt), y=0, yend=impt),size=1) +
  geom_point(size=3, color=&quot;red&quot;)+
  coord_flip()+
  labs(x=&quot;重要性&quot;,y=&quot;变量名&quot;) + 
  theme_bw()</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="/post/cross_files/figure-html/unnamed-chunk-3-1.png" alt="变量重要性" width="70%" />
<p class="caption">
Figure 1: 变量重要性
</p>
</div>
</div>
<div id="leave-one-out-cross-validation" class="section level2">
<h2>留一交叉验证(leave-one-out cross validation)</h2>
<pre class="r"><code>rfControl &lt;-trainControl(
method =&quot;LOOCV&quot;
) #留一交叉验证
rpartFit &lt;- train(medv ~ .,
                  data = BostonHousing,
                  method = &quot;rpart&quot;,
                  trControl = rfControl)
rpartFit</code></pre>
<pre><code>CART 

506 samples
 13 predictor

No pre-processing
Resampling: Leave-One-Out Cross-Validation 
Summary of sample sizes: 505, 505, 505, 505, 505, 505, ... 
Resampling results across tuning parameters:

  cp          RMSE      Rsquared      MAE     
  0.07165784  6.060240  0.5665051649  4.289854
  0.17117244  7.489499  0.3524415744  6.088882
  0.45274420  9.700709  0.0008358062  7.539715

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.07165784.</code></pre>
<pre class="r"><code>varImp(rpartFit, scale = F)</code></pre>
<pre><code>rpart variable importance

        Overall
lstat    0.8646
nox      0.5008
ptratio  0.4643
rm       0.4527
indus    0.2595
crim     0.2484
age      0.2089
zn       0.0000
chas1    0.0000
tax      0.0000
dis      0.0000
b        0.0000
rad      0.0000</code></pre>
<ul>
<li>我们发现个别变量对于预测完全没有帮助，故可以将这些变量在之后的分析中删除。</li>
</ul>
</div>
</div>
<div id="caretk" class="section level1">
<h1>使用<code>caret</code>包训练K近邻模型</h1>
<ul>
<li>数据：<a href="https://www.kaggle.com/abcsds/pokemon">口袋妖怪</a>数据集</li>
<li>任务：通过各种属性预测神奇宝贝是否为传说级别</li>
</ul>
<blockquote>
<ul>
<li>ID,每只神奇宝贝的ID<br />
</li>
<li>Name,每只神奇宝贝的名字<br />
</li>
<li>Type1:每只神奇宝贝的类型，比如说水系，比如说火系<br />
</li>
<li>Type2:由于有特殊的神奇宝贝拥有复数以上的属性。<br />
</li>
<li>Total:指每只神奇宝贝的强度，一般越高越强<br />
</li>
<li>HP：指每只神奇宝贝的生命值<br />
</li>
<li>Attack:指每只神奇宝贝的攻击力<br />
</li>
<li>Defense:指每只神奇宝贝的防御力<br />
</li>
<li>SP Attack:指每只神奇宝贝面对相克属性神奇宝贝时的攻击力，通常会比普通攻击力高<br />
</li>
<li>SP Defense:指每只神奇宝贝面对相克属性神奇宝贝时的防御力，通常会比普通防御力高。<br />
</li>
<li>Speed:指每只神奇宝贝的速度<br />
</li>
<li>Generation：指每一只神奇宝贝属于哪一部神奇宝贝的，目前分为五部。</li>
<li>Legendary:指每一只神奇宝贝是不是属于传说级别的神奇宝贝，比如麒麟，超梦，梦幻之类的。</li>
</ul>
</blockquote>
<p><a href="https://dataaspirant.com/2017/01/09/knn-implementation-r-using-caret-package/">代码参考</a></p>
<div id="kk-nearest-neighbor" class="section level2">
<h2>K近邻算法(k-nearest neighbor)</h2>
<ul>
<li>K近邻算法对于测试集的给定样本，在训练集中找到与之最邻近的K个样本，把这个给定样本判定为它们的K临近点的多数类。</li>
<li>在样本比较少的情况下，通常使用交叉验证确定K值。</li>
<li>K越大，模型越简单；K越小，模型越复杂，越容易发生过拟合。</li>
</ul>
<pre class="r"><code>pokemon &lt;- read_csv(&quot;E:/R_codes/others/Pokemon.csv&quot;)
colnames(pokemon)[1] &lt;- &#39;id&#39; #修改变量名
colnames(pokemon)[3] &lt;- &#39;Type1&#39;
colnames(pokemon)[4] &lt;- &#39;Type2&#39;

pokemon$Legendary &lt;- as.factor(pokemon$Legendary)
pokemon &lt;- na.omit(pokemon)

pokemon &lt;- pokemon %&gt;%
  drop_na(Type1,Type2,Total,HP,Defense,Attack,Speed,Generation) %&gt;%
  select(Legendary,Type1,Type2,Total,HP,Defense,Attack,Speed,Generation)

#过采样
balanced &lt;- ovun.sample(Legendary ~ Type1 + Type2 + Total+ HP + Defense+ Attack + Speed + Generation, data = pokemon, method = &quot;both&quot;, p=0.5,N=748,seed = 1)$data

#sampled &lt;- sample(1:nrow(pokemon),nrow(pokemon)*0.7,replace=F)
#trainpok &lt;- pokemon[sampled,]
#testpok &lt;- pokemon[-sampled,]

trctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number =10,repeats =10, savePredictions
=T)

set.seed(213)
knn_fit &lt;- train(Legendary ~ Type1 + Type2 + Total+ HP + Defense+ Attack + Speed + Generation, data = balanced, method = &quot;knn&quot;,
 trControl=trctrl,
 tuneLength = 10)
knn_fit</code></pre>
<pre><code>k-Nearest Neighbors 

748 samples
  8 predictor
  2 classes: &#39;FALSE&#39;, &#39;TRUE&#39; 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 674, 673, 674, 672, 674, 674, ... 
Resampling results across tuning parameters:

  k   Accuracy   Kappa    
   5  0.9484350  0.8971586
   7  0.9425515  0.8854659
   9  0.9432165  0.8867946
  11  0.9477308  0.8957603
  13  0.9509650  0.9021897
  15  0.9398669  0.8801152
  17  0.9369208  0.8742642
  19  0.9375875  0.8755897
  21  0.9389192  0.8782317
  23  0.9415845  0.8835236

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was k = 13.</code></pre>
<pre class="r"><code>kacc &lt;- as.data.frame(knn_fit[[&quot;results&quot;]]) 
kacc &lt;- kacc[,1:2]
ggplot(kacc,aes(k,Accuracy)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks=seq(5,26,1))</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-6"></span>
<img src="/post/cross_files/figure-html/unnamed-chunk-6-1.png" alt="K值和准确率的关系" width="65%" />
<p class="caption">
Figure 2: K值和准确率的关系
</p>
</div>
<pre class="r"><code>balanced &lt;- balanced %&gt;%
  mutate(test_pred=predict(knn_fit,newdata=balanced))
conMatrix &lt;- confusionMatrix(balanced$test_pred,balanced$Legendary)
matrix &lt;- conMatrix[[&quot;table&quot;]]
conMatrix</code></pre>
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction FALSE TRUE
     FALSE   351    0
     TRUE     33  364
                                          
               Accuracy : 0.9559          
                 95% CI : (0.9386, 0.9694)
    No Information Rate : 0.5134          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
                                          
                  Kappa : 0.9119          
                                          
 Mcnemar&#39;s Test P-Value : 2.54e-08        
                                          
            Sensitivity : 0.9141          
            Specificity : 1.0000          
         Pos Pred Value : 1.0000          
         Neg Pred Value : 0.9169          
             Prevalence : 0.5134          
         Detection Rate : 0.4693          
   Detection Prevalence : 0.4693          
      Balanced Accuracy : 0.9570          
                                          
       &#39;Positive&#39; Class : FALSE           
                                          </code></pre>
<pre class="r"><code>balanced$test_pred &lt;- as.ordered(balanced$test_pred)
balanced$Legendary &lt;- as.ordered(balanced$Legendary)
roc(balanced$test_pred,balanced$Legendary,plot=T, print.auc=T, auc.polygon=T, max.auc.polygon=T,auc.polygon.col=&quot;skyblue&quot;, print.thres=T,xlab =&#39;伪正类率&#39;,ylab=&#39;真正类率&#39;)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-7"></span>
<img src="/post/cross_files/figure-html/unnamed-chunk-7-1.png" alt="ROC曲线" width="65%" />
<p class="caption">
Figure 3: ROC曲线
</p>
</div>
<pre><code>
Call:
roc.default(response = balanced$test_pred, predictor = balanced$Legendary,     plot = T, print.auc = T, auc.polygon = T, max.auc.polygon = T,     auc.polygon.col = &quot;skyblue&quot;, print.thres = T, xlab = &quot;伪正类率&quot;,     ylab = &quot;真正类率&quot;)

Data: balanced$Legendary in 351 controls (balanced$test_pred FALSE) &lt; 397 cases (balanced$test_pred TRUE).
Area under the curve: 0.9584</code></pre>
<pre class="r"><code>#ggroc()</code></pre>
</div>
</div>
