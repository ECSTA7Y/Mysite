---
title: '交叉验证：基于caret包'
author: ' '
date: '2019-05-25'
slug: cross 
output:
  bookdown::html_document2:
    toc: true
    theme: readable
---



<p><img src="https://img.shields.io/badge/License-CC%20BY--NC--ND%204.0-brightgreen.svg" /></p>
<hr />
<p><a href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation">Cross-validation: evaluating estimator performance</a></p>
<hr />
<div id="基本概念" class="section level1">
<h1>基本概念</h1>
<p>交叉验证是机器学习中的常用模型选择方法。其中最常用的方法是K折交叉验证(k-fold cross validation)。K折交叉验证重复使用数据，把给定的数据切分为K个互斥子集，每次使用1个子集作为测试集，使用余下k-1个子集的并集作为训练集。其中，训练集用于模型的训练，测试集用于模型的评估和选择。在样本量不够充足的情况下，交叉验证法通过重复使用数据能够减少样本划分不同导致的差别，并且选择测试误差最小的模型，增强模型的泛化能力。</p>
<p>划分测试集和训练集同样有助于更加客观地评价模型的泛化能力。交叉验证也是模型调节超参数的评价过程。</p>
<p>交叉验证主要有以下种类：</p>
<ul>
<li><p>简单交叉验证 (hold-out cross validation)</p></li>
<li><p>k折交叉验证 (K-fold cross-validation)</p></li>
<li><p>留一交叉验证(leave-one-out cross validation)</p></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="https://scikit-learn.org/stable/_images/grid_search_cross_validation.png" alt="K折交叉验证原理：所有数据被划分为训练集和测试集。其中，训练集被划分为K份，循环使用其中的K-1份充当训练集，使用剩下的1分充当验证集。" width="90%" />
<p class="caption">
Figure 1: K折交叉验证原理：所有数据被划分为训练集和测试集。其中，训练集被划分为K份，循环使用其中的K-1份充当训练集，使用剩下的1分充当验证集。
</p>
</div>
<ul>
<li>波士顿房价预测</li>
</ul>
<p>输入<code>?mlbench::BostonHousing</code>查看数据集解释。下面主要使用<a href="http://topepo.github.io/caret/index.html">caret</a>包(classification and regression training)进行交叉验证。</p>
<pre class="r"><code>`%&gt;%` &lt;- magrittr::`%&gt;%`
library(caret)
library(readr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(pROC)
library(stargazer)
library(ROSE)</code></pre>
</div>
<div id="r示例" class="section level1">
<h1>R示例</h1>
<div id="折交叉验证-k-fold-cross-validation" class="section level2">
<h2>10折交叉验证 (K-fold cross-validation)</h2>
<pre class="r"><code>rfControl &lt;-trainControl( #10折交叉验证
method =&quot;cv&quot;, 
number =10 
)

library(mlbench) #加载数据集
data(BostonHousing)
head(BostonHousing)
#&gt;      crim zn indus chas   nox    rm  age    dis rad tax ptratio      b lstat
#&gt; 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98
#&gt; 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14
#&gt; 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03
#&gt; 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94
#&gt; 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33
#&gt; 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21
#&gt;   medv
#&gt; 1 24.0
#&gt; 2 21.6
#&gt; 3 34.7
#&gt; 4 33.4
#&gt; 5 36.2
#&gt; 6 28.7
nrow(BostonHousing) #样本量
#&gt; [1] 506

rpartFit &lt;- train(medv ~ .,
                  data = BostonHousing,
                  method = &quot;rpart&quot;,
                  trControl = rfControl)
rpartFit
#&gt; CART 
#&gt; 
#&gt; 506 samples
#&gt;  13 predictor
#&gt; 
#&gt; No pre-processing
#&gt; Resampling: Cross-Validated (10 fold) 
#&gt; Summary of sample sizes: 456, 456, 455, 455, 454, 456, ... 
#&gt; Resampling results across tuning parameters:
#&gt; 
#&gt;   cp          RMSE      Rsquared   MAE     
#&gt;   0.07165784  5.499046  0.6371284  3.942789
#&gt;   0.17117244  6.874506  0.4322510  5.177036
#&gt;   0.45274420  7.977208  0.3649644  5.807824
#&gt; 
#&gt; RMSE was used to select the optimal model using the smallest value.
#&gt; The final value used for the model was cp = 0.07165784.</code></pre>
<p>将数据集D划分为K个子集同样存在多种划分方式。为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p 次k折交叉验证结果的均值， 例如常见的有“10次10 折交叉验证”。每一次重复都会有不同的训练-测试集划分。</p>
<pre class="r"><code>repeatedcv &lt;-trainControl(method =&quot;repeatedcv&quot;, number =10,repeats =10, savePredictions
=T)
rpartFit &lt;- train(medv ~ .,
                  data = BostonHousing,
                  method = &quot;rpart&quot;,
                  trControl = repeatedcv)
rpartFit
#&gt; CART 
#&gt; 
#&gt; 506 samples
#&gt;  13 predictor
#&gt; 
#&gt; No pre-processing
#&gt; Resampling: Cross-Validated (10 fold, repeated 10 times) 
#&gt; Summary of sample sizes: 455, 456, 455, 456, 455, 455, ... 
#&gt; Resampling results across tuning parameters:
#&gt; 
#&gt;   cp          RMSE      Rsquared   MAE     
#&gt;   0.07165784  5.737457  0.6143723  4.159446
#&gt;   0.17117244  6.574678  0.4868092  4.913991
#&gt;   0.45274420  8.424123  0.3121447  6.095523
#&gt; 
#&gt; RMSE was used to select the optimal model using the smallest value.
#&gt; The final value used for the model was cp = 0.07165784.

importance &lt;- varImp(rpartFit, scale = F)
imp &lt;- as.data.frame(importance[[&quot;importance&quot;]])

imp &lt;- imp %&gt;%
  mutate(name=rownames(imp)) %&gt;%
  filter(Overall&gt;0) %&gt;%
  rename(&#39;impt&#39;=&#39;Overall&#39;) #%&gt;%
  #mutate(varname = fct_reorder(varname,impt))

ggplot(imp,aes(reorder(name,impt),impt)) +
  geom_segment( aes(x=reorder(name,impt), xend=reorder(name,impt), y=0, yend=impt),size=1) +
  geom_point(size=3, color=&quot;red&quot;)+
  coord_flip()+
  labs(x=&quot;重要性&quot;,y=&quot;变量名&quot;) + 
  theme_bw()</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-5"></span>
<img src="/post/cross_files/figure-html/unnamed-chunk-5-1.png" alt="变量重要性" width="70%" />
<p class="caption">
Figure 2: 变量重要性
</p>
</div>
</div>
<div id="留一交叉验证leave-one-out-cross-validation" class="section level2">
<h2>留一交叉验证(leave-one-out cross validation)</h2>
<pre class="r"><code>rfControl &lt;-trainControl(
method =&quot;LOOCV&quot;
) #留一交叉验证
rpartFit &lt;- train(medv ~ .,
                  data = BostonHousing,
                  method = &quot;rpart&quot;,
                  trControl = rfControl)
rpartFit
#&gt; CART 
#&gt; 
#&gt; 506 samples
#&gt;  13 predictor
#&gt; 
#&gt; No pre-processing
#&gt; Resampling: Leave-One-Out Cross-Validation 
#&gt; Summary of sample sizes: 505, 505, 505, 505, 505, 505, ... 
#&gt; Resampling results across tuning parameters:
#&gt; 
#&gt;   cp          RMSE      Rsquared      MAE     
#&gt;   0.07165784  6.060240  0.5665051649  4.289854
#&gt;   0.17117244  7.489499  0.3524415744  6.088882
#&gt;   0.45274420  9.700709  0.0008358062  7.539715
#&gt; 
#&gt; RMSE was used to select the optimal model using the smallest value.
#&gt; The final value used for the model was cp = 0.07165784.

varImp(rpartFit, scale = F)
#&gt; rpart variable importance
#&gt; 
#&gt;         Overall
#&gt; lstat    0.8646
#&gt; nox      0.5008
#&gt; ptratio  0.4643
#&gt; rm       0.4527
#&gt; indus    0.2595
#&gt; crim     0.2484
#&gt; age      0.2089
#&gt; dis      0.0000
#&gt; tax      0.0000
#&gt; chas1    0.0000
#&gt; zn       0.0000
#&gt; rad      0.0000
#&gt; b        0.0000</code></pre>
<ul>
<li>我们发现个别变量对于预测完全没有帮助，故可以将这些变量在之后的分析中删除。</li>
</ul>
</div>
</div>
<div id="使用caret包训练k近邻模型" class="section level1">
<h1>使用<code>caret</code>包训练K近邻模型</h1>
<ul>
<li>数据：<a href="https://www.kaggle.com/abcsds/pokemon">口袋妖怪</a>数据集</li>
<li>任务：通过各种属性预测神奇宝贝是否为传说级别</li>
</ul>
<blockquote>
<ul>
<li>ID,每只神奇宝贝的ID<br />
</li>
<li>Name,每只神奇宝贝的名字<br />
</li>
<li>Type1:每只神奇宝贝的类型，比如说水系，比如说火系<br />
</li>
<li>Type2:由于有特殊的神奇宝贝拥有复数以上的属性。<br />
</li>
<li>Total:指每只神奇宝贝的强度，一般越高越强<br />
</li>
<li>HP：指每只神奇宝贝的生命值<br />
</li>
<li>Attack:指每只神奇宝贝的攻击力<br />
</li>
<li>Defense:指每只神奇宝贝的防御力<br />
</li>
<li>SP Attack:指每只神奇宝贝面对相克属性神奇宝贝时的攻击力，通常会比普通攻击力高<br />
</li>
<li>SP Defense:指每只神奇宝贝面对相克属性神奇宝贝时的防御力，通常会比普通防御力高。<br />
</li>
<li>Speed:指每只神奇宝贝的速度<br />
</li>
<li>Generation：指每一只神奇宝贝属于哪一部神奇宝贝的，目前分为五部。</li>
<li>Legendary:指每一只神奇宝贝是不是属于传说级别的神奇宝贝，比如麒麟，超梦，梦幻之类的。</li>
</ul>
</blockquote>
<p><a href="https://dataaspirant.com/2017/01/09/knn-implementation-r-using-caret-package/">代码参考</a></p>
<pre class="r"><code>pokemon &lt;- read_csv(&quot;E:/MaLearning/Pokemon.csv&quot;)
colnames(pokemon)[1] &lt;- &#39;id&#39; #修改变量名
colnames(pokemon)[3] &lt;- &#39;Type1&#39;
colnames(pokemon)[4] &lt;- &#39;Type2&#39;

pokemon &lt;- pokemon %&gt;%
  drop_na(Type1,Type2,Total,HP,Defense,Attack,Speed,Generation) %&gt;%
  select(Legendary,Type1,Type2,Total,HP,Defense,Attack,Speed,Generation) %&gt;%
  mutate(Legendary=as.factor(Legendary),
    Type1=recode(Type1,&#39;Fairy&#39;=&#39;oth&#39;,&#39;Fighting&#39;=&#39;oth&#39;,&#39;Flying&#39;=&#39;oth&#39;)
)</code></pre>
<pre class="r"><code>ggplot(pokemon,aes(Total,Attack,shape=Legendary,color=Legendary)) +
  geom_point(size=3)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-9"></span>
<img src="/post/cross_files/figure-html/unnamed-chunk-9-1.png" alt="过采样前" width="90%" />
<p class="caption">
Figure 3: 过采样前
</p>
</div>
<pre class="r"><code>#过采样
balance &lt;- ovun.sample(Legendary ~ Type1 + Type2 + Total+ HP + Defense+ Attack + Speed + Generation, data = pokemon, method = &quot;both&quot;, p=0.5,N=748,seed = 1)$data

sampled &lt;- sample(1:nrow(balance),nrow(balance)*0.7,replace=F)
trainpok &lt;- balance[sampled,]
testpok &lt;- balance[-sampled,]

ggplot(balance,aes(Total,Attack,shape=Legendary,color=Legendary)) +
  geom_jitter(size=1.5,height = 10, width = 40)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-10"></span>
<img src="/post/cross_files/figure-html/unnamed-chunk-10-1.png" alt="过采样后" width="90%" />
<p class="caption">
Figure 4: 过采样后
</p>
</div>
<div id="k近邻算法k-nearest-neighbor" class="section level2">
<h2>K近邻算法(k-nearest neighbor)</h2>
<ul>
<li>K近邻算法对于测试集的给定样本，在训练集中找到与之最邻近的K个样本，把这个给定样本判定为它们的K临近点的多数类。</li>
<li>在样本比较少的情况下，通常使用交叉验证确定K值。</li>
<li>K越大，模型越简单；K越小，模型越复杂，越容易发生过拟合。</li>
</ul>
<pre class="r"><code>trctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number =10,repeats =10, savePredictions
=T)

set.seed(213)
knn_fit &lt;- train(Legendary ~ Type1 + Type2 + Total+ HP + Defense+ Attack + Speed + Generation, data = trainpok, method = &quot;knn&quot;,
 trControl=trctrl,
 tuneLength = 10)
knn_fit
#&gt; k-Nearest Neighbors 
#&gt; 
#&gt; 523 samples
#&gt;   8 predictor
#&gt;   2 classes: &#39;FALSE&#39;, &#39;TRUE&#39; 
#&gt; 
#&gt; No pre-processing
#&gt; Resampling: Cross-Validated (10 fold, repeated 10 times) 
#&gt; Summary of sample sizes: 471, 470, 470, 471, 471, 470, ... 
#&gt; Resampling results across tuning parameters:
#&gt; 
#&gt;   k   Accuracy   Kappa    
#&gt;    5  0.9523902  0.9049491
#&gt;    7  0.9518130  0.9038010
#&gt;    9  0.9514357  0.9030465
#&gt;   11  0.9512506  0.9026794
#&gt;   13  0.9518166  0.9038074
#&gt;   15  0.9512470  0.9026798
#&gt;   17  0.9487760  0.8977514
#&gt;   19  0.9506809  0.9015500
#&gt;   21  0.9506809  0.9015439
#&gt;   23  0.9493457  0.8988832
#&gt; 
#&gt; Accuracy was used to select the optimal model using the largest value.
#&gt; The final value used for the model was k = 5.

kacc &lt;- as.data.frame(knn_fit[[&quot;results&quot;]]) 
kacc &lt;- kacc[,1:2]
ggplot(kacc,aes(k,Accuracy)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks=seq(5,26,1))</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-11"></span>
<img src="/post/cross_files/figure-html/unnamed-chunk-11-1.png" alt="k近邻算法的K值和准确率的关系" width="90%" />
<p class="caption">
Figure 5: k近邻算法的K值和准确率的关系
</p>
</div>
<pre class="r"><code>
testpok &lt;- testpok %&gt;%
  mutate(pred=predict(knn_fit,testpok))

prob1 &lt;- predict(knn_fit,testpok,type=&#39;prob&#39;)
pr1 &lt;- prob1[,2] 

trainpok &lt;- trainpok %&gt;%
  mutate(pred=predict(knn_fit,trainpok))

prob2 &lt;- predict(knn_fit,trainpok,type=&#39;prob&#39;)
pr2 &lt;- prob2[,2] 

conMatrix1 &lt;- confusionMatrix(testpok$pred,testpok$Legendary) 
#matrix &lt;- conMatrix1[[&quot;table&quot;]]
conMatrix1
#&gt; Confusion Matrix and Statistics
#&gt; 
#&gt;           Reference
#&gt; Prediction FALSE TRUE
#&gt;      FALSE   105    0
#&gt;      TRUE     13  107
#&gt;                                           
#&gt;                Accuracy : 0.9422          
#&gt;                  95% CI : (0.9032, 0.9689)
#&gt;     No Information Rate : 0.5244          
#&gt;     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
#&gt;                                           
#&gt;                   Kappa : 0.8848          
#&gt;                                           
#&gt;  Mcnemar&#39;s Test P-Value : 0.0008741       
#&gt;                                           
#&gt;             Sensitivity : 0.8898          
#&gt;             Specificity : 1.0000          
#&gt;          Pos Pred Value : 1.0000          
#&gt;          Neg Pred Value : 0.8917          
#&gt;              Prevalence : 0.5244          
#&gt;          Detection Rate : 0.4667          
#&gt;    Detection Prevalence : 0.4667          
#&gt;       Balanced Accuracy : 0.9449          
#&gt;                                           
#&gt;        &#39;Positive&#39; Class : FALSE           
#&gt; 

conMatrix2 &lt;- confusionMatrix(trainpok$pred,trainpok$Legendary)
#matrix &lt;- conMatrix[[&quot;table&quot;]]
conMatrix2
#&gt; Confusion Matrix and Statistics
#&gt; 
#&gt;           Reference
#&gt; Prediction FALSE TRUE
#&gt;      FALSE   244    0
#&gt;      TRUE     22  257
#&gt;                                          
#&gt;                Accuracy : 0.9579         
#&gt;                  95% CI : (0.937, 0.9735)
#&gt;     No Information Rate : 0.5086         
#&gt;     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
#&gt;                                          
#&gt;                   Kappa : 0.916          
#&gt;                                          
#&gt;  Mcnemar&#39;s Test P-Value : 7.562e-06      
#&gt;                                          
#&gt;             Sensitivity : 0.9173         
#&gt;             Specificity : 1.0000         
#&gt;          Pos Pred Value : 1.0000         
#&gt;          Neg Pred Value : 0.9211         
#&gt;              Prevalence : 0.5086         
#&gt;          Detection Rate : 0.4665         
#&gt;    Detection Prevalence : 0.4665         
#&gt;       Balanced Accuracy : 0.9586         
#&gt;                                          
#&gt;        &#39;Positive&#39; Class : FALSE          
#&gt; 

testpok$Legendary &lt;- as.ordered(testpok$Legendary)
trainpok$Legendary &lt;- as.ordered(trainpok$Legendary)

roca &lt;- roc(testpok$Legendary,pr1)
rocb &lt;- roc(trainpok$Legendary,pr2)</code></pre>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve">学习曲线</a></p>
<pre class="r"><code>ggroc(list(&#39;测试集&#39;=roca,&#39;训练集&#39;=rocb),size=1)+
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color=&#39;black&#39;, linetype=&#39;dashed&#39;)+
  theme(legend.title=element_blank()) </code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-12"></span>
<img src="/post/cross_files/figure-html/unnamed-chunk-12-1.png" alt="ROC曲线:使用`ggroc`包绘制。模型在测试集的表现比训练集稍差一些。" width="90%" />
<p class="caption">
Figure 6: ROC曲线:使用<code>ggroc</code>包绘制。模型在测试集的表现比训练集稍差一些。
</p>
</div>
</div>
</div>
<div id="泰坦尼克生存预测" class="section level1">
<h1>泰坦尼克生存预测</h1>
<pre class="r"><code>library(readr)
titanic &lt;- read_csv(&quot;C:/Users/xsong/titanic_code/data/titanic.csv&quot;)

sampled &lt;- sample(1:nrow(titanic),nrow(titanic)*0.75,replace=F)
titanic_train &lt;- titanic[sampled,]
titanic_test &lt;- titanic[-sampled,]

titanic_train &lt;- titanic_train %&gt;% 
  drop_na(survived,sex,age,sibsp,parch,fare,embarked) %&gt;% 
  mutate(pclass=as.factor(pclass))

titanic_test &lt;- titanic_test %&gt;% 
  drop_na(survived,sex,age,sibsp,parch,fare,embarked) %&gt;% 
  mutate(pclass=as.factor(pclass))

control &lt;- trainControl(method = &#39;repeatedcv&#39;, number =10,repeats =10, savePredictions
=T)

set.seed(213)
logfit &lt;- train(survived ~ sex+age+sibsp+parch+fare+embarked,
                data=titanic_train,
                 method=&#39;glm&#39;,family=&#39;binomial&#39;,
 trControl=control,
 tuneLength = 2)
logfit
#&gt; Generalized Linear Model 
#&gt; 
#&gt; 779 samples
#&gt;   6 predictor
#&gt; 
#&gt; No pre-processing
#&gt; Resampling: Cross-Validated (10 fold, repeated 10 times) 
#&gt; Summary of sample sizes: 701, 701, 701, 701, 701, 701, ... 
#&gt; Resampling results:
#&gt; 
#&gt;   RMSE       Rsquared  MAE    
#&gt;   0.3996116  0.345518  0.31574

titanic_test &lt;- titanic_test %&gt;% 
  mutate(pred=predict(logfit,titanic_test),
         pred=ifelse(pred&gt;=0.5,&#39;1&#39;,&#39;0&#39;),
         pred=as.factor(pred),
         survived=as.factor(survived))

prob1 &lt;- predict(logfit,titanic_test)

titanic_train &lt;- titanic_train %&gt;%
  mutate(pred=predict(logfit,titanic_train),
         pred=ifelse(pred&gt;=0.5,&#39;1&#39;,&#39;0&#39;),
         pred=as.factor(pred),
         survived=as.factor(survived))

prob2 &lt;- predict(logfit,titanic_train)

conMatrix1 &lt;- confusionMatrix(titanic_test$pred,titanic_test$survived) 
matrix &lt;- conMatrix1[[&quot;table&quot;]]
names(dimnames(matrix))&lt;-c(&#39;预测值&#39;,&#39;真实值&#39;)
matrix
#&gt;       真实值
#&gt; 预测值   0   1
#&gt;      0 135  38
#&gt;      1  25  66

conMatrix2 &lt;- confusionMatrix(titanic_train$pred,titanic_train$survived)
matrix2 &lt;- conMatrix2[[&quot;table&quot;]]

names(dimnames(matrix2))&lt;-c(&#39;预测值&#39;,&#39;真实值&#39;)
matrix2
#&gt;       真实值
#&gt; 预测值   0   1
#&gt;      0 385  94
#&gt;      1  73 227

titanic_test$Survived &lt;- as.ordered(titanic_test$survived)
titanic_train$Survived &lt;- as.ordered(titanic_train$survived)

roca &lt;- roc(titanic_test$survived,prob1)
rocb &lt;- roc(titanic_train$survived,prob2)</code></pre>
<pre class="r"><code>list(&#39;test ROC&#39;=roca,
     &#39;train ROC&#39;=rocb) %&gt;% 
ggroc(.,size=1)+
  geom_segment(aes(x=1,y=0,
                   xend=0,yend=1),
               color=&#39;black&#39;,linetype=&#39;solid&#39;)+
  theme(legend.title=element_blank()) </code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-15"></span>
<img src="/post/cross_files/figure-html/unnamed-chunk-15-1.png" alt="ROC" width="90%" />
<p class="caption">
Figure 7: ROC
</p>
</div>
</div>
<div id="sklearn交叉验证api" class="section level1">
<h1><code>sklearn</code>交叉验证API</h1>
<p><code>sklearn</code>中有关交叉验证的API位于<code>sklearn.model_selection</code>中。</p>
<p>用于模型检验和评估的函数：</p>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate"><code>cross_validate</code></a><br />
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"><code>cross_val_score</code></a></p>
<p>用于数据集划分方式的函数为：</p>
<p>K折交叉验证：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold"><code>KFold</code></a><br />
分层K折交叉验证：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html"><code>StratifiedKFold</code></a><br />
随机播种法：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit"><code>ShuffleSplit</code></a><br />
分组K折交叉验证：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold"><code>GroupKFold</code></a><br />
重复K折交叉验证：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html#sklearn.model_selection.RepeatedKFold"><code>RepeatedKFold</code></a><br />
重复分层K折交叉验证：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html#sklearn.model_selection.RepeatedStratifiedKFold"><code>RepeatedStratifiedKFold</code></a></p>
<p>可以作为交叉验证方法代入：</p>
<pre class="python"><code>kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)
grid_search = GridSearchCV(model, param_grid, scoring=&quot;neg_log_loss&quot;, cv = kfold)</code></pre>
<p>用于参数调优的函数为：</p>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"><code>learn.model_selection.GridSearchCV</code></a><br />
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"><code>learn.model_selection.RandomizedSearchCV</code></a></p>
</div>
