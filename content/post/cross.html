---
title: '交叉验证：基于caret包'
author: ' '
date: '2019-05-25'
slug: cross
tags: ['机器学习']
---



<div id="section" class="section level2">
<h2><img src="https://img.shields.io/badge/License-CC%20BY--NC--ND%204.0-brightgreen.svg" /></h2>
<p><a href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation">Cross-validation: evaluating estimator performance</a></p>
<hr />
</div>
<div id="section-1" class="section level1">
<h1>基本概念</h1>
<p>交叉验证是机器学习中的常用模型选择方法。其中最常用的方法是K折交叉验证(k-fold cross validation)。K折交叉验证重复使用数据，把给定的数据切分为K个互斥子集，每次使用1个子集作为测试集，使用余下k-1个子集的并集作为训练集。其中，训练集用于模型的训练，测试集用于模型的评估和选择。在样本量不够充足的情况下，交叉验证法通过重复使用数据能够减少样本划分不同导致的差别，并且选择测试误差最小的模型，增强模型的泛化能力。</p>
<p>划分测试集和训练集同样有助于更加客观地评价模型的泛化能力。交叉验证也是模型调节超参数的评价过程。</p>
<p>交叉验证主要有以下种类：</p>
<ul>
<li><p>简单交叉验证 (hold-out cross validation)</p></li>
<li><p>k折交叉验证 (K-fold cross-validation)</p></li>
<li><p>留一交叉验证(leave-one-out cross validation)</p></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="https://scikit-learn.org/stable/_images/grid_search_cross_validation.png" alt="K折交叉验证原理：所有数据被划分为训练集和测试集。其中，训练集被划分为K份，循环使用其中的K-1份充当训练集，使用剩下的1分充当验证集。" width="100%" />
<p class="caption">
Figure 1: K折交叉验证原理：所有数据被划分为训练集和测试集。其中，训练集被划分为K份，循环使用其中的K-1份充当训练集，使用剩下的1分充当验证集。
</p>
</div>
<p>谢益辉的统计动画R包<a href="https://yihui.name/animation/example/cv-ani/">animation</a>直观地展示了交叉验证方法。</p>
<p>任务：通过一系列特征预测波士顿房价</p>
<p>输入<code>?mlbench::BostonHousing</code>查看数据集解释。下面主要使用<a href="http://topepo.github.io/caret/index.html">caret</a>包(classification and regression training)进行交叉验证。</p>
<pre class="r"><code>`%&gt;%` &lt;- magrittr::`%&gt;%`
library(caret)
library(readr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(pROC)
library(stargazer)
library(ROSE)</code></pre>
</div>
<div id="r" class="section level1">
<h1>R示例</h1>
<div id="k-fold-cross-validation" class="section level2">
<h2>10折交叉验证 (K-fold cross-validation)</h2>
<pre class="r"><code>rfControl &lt;-trainControl( #10折交叉验证
method =&quot;cv&quot;, 
number =10 
)

library(mlbench) #加载数据集
data(BostonHousing)
head(BostonHousing)</code></pre>
<pre><code>     crim zn indus chas   nox    rm  age    dis rad tax ptratio      b
1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90
2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90
3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83
4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63
5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90
6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12
  lstat medv
1  4.98 24.0
2  9.14 21.6
3  4.03 34.7
4  2.94 33.4
5  5.33 36.2
6  5.21 28.7</code></pre>
<pre class="r"><code>nrow(BostonHousing) #样本量</code></pre>
<pre><code>[1] 506</code></pre>
<pre class="r"><code>rpartFit &lt;- train(medv ~ .,
                  data = BostonHousing,
                  method = &quot;rpart&quot;,
                  trControl = rfControl)
rpartFit</code></pre>
<pre><code>CART 

506 samples
 13 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 456, 457, 455, 456, 454, 456, ... 
Resampling results across tuning parameters:

  cp          RMSE      Rsquared   MAE     
  0.07165784  5.798693  0.6079069  4.150778
  0.17117244  6.310756  0.5249764  4.646935
  0.45274420  8.005426  0.3648840  5.767876

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.07165784.</code></pre>
<p>将数据集D划分为K个子集同样存在多种划分方式。为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p 次k折交叉验证结果的均值， 例如常见的有“10次10 折交叉验证”。每一次重复都会有不同的训练-测试集划分。</p>
<pre class="r"><code>repeatedcv &lt;-trainControl(method =&quot;repeatedcv&quot;, number =10,repeats =10, savePredictions
=T)
rpartFit &lt;- train(medv ~ .,
                  data = BostonHousing,
                  method = &quot;rpart&quot;,
                  trControl = repeatedcv)
rpartFit</code></pre>
<pre><code>CART 

506 samples
 13 predictor

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 457, 456, 454, 456, 456, 455, ... 
Resampling results across tuning parameters:

  cp          RMSE      Rsquared   MAE     
  0.07165784  5.812733  0.6069060  4.175634
  0.17117244  6.662202  0.4786666  4.904228
  0.45274420  8.307043  0.3308596  6.034919

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.07165784.</code></pre>
<pre class="r"><code>importance &lt;- varImp(rpartFit, scale = F)
imp &lt;- as.data.frame(importance[[&quot;importance&quot;]])

imp &lt;- imp %&gt;%
  mutate(name=rownames(imp)) %&gt;%
  filter(Overall&gt;0) %&gt;%
  rename(&#39;impt&#39;=&#39;Overall&#39;) #%&gt;%
  #mutate(varname = fct_reorder(varname,impt))

ggplot(imp,aes(reorder(name,impt),impt)) +
  geom_segment( aes(x=reorder(name,impt), xend=reorder(name,impt), y=0, yend=impt),size=1) +
  geom_point(size=3, color=&quot;red&quot;)+
  coord_flip()+
  labs(x=&quot;重要性&quot;,y=&quot;变量名&quot;) + 
  theme_bw()</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="/post/cross_files/figure-html/unnamed-chunk-4-1.png" alt="变量重要性" width="70%" />
<p class="caption">
Figure 2: 变量重要性
</p>
</div>
</div>
<div id="leave-one-out-cross-validation" class="section level2">
<h2>留一交叉验证(leave-one-out cross validation)</h2>
<pre class="r"><code>rfControl &lt;-trainControl(
method =&quot;LOOCV&quot;
) #留一交叉验证
rpartFit &lt;- train(medv ~ .,
                  data = BostonHousing,
                  method = &quot;rpart&quot;,
                  trControl = rfControl)
rpartFit</code></pre>
<pre><code>CART 

506 samples
 13 predictor

No pre-processing
Resampling: Leave-One-Out Cross-Validation 
Summary of sample sizes: 505, 505, 505, 505, 505, 505, ... 
Resampling results across tuning parameters:

  cp          RMSE      Rsquared      MAE     
  0.07165784  6.060240  0.5665051649  4.289854
  0.17117244  7.489499  0.3524415744  6.088882
  0.45274420  9.700709  0.0008358062  7.539715

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.07165784.</code></pre>
<pre class="r"><code>varImp(rpartFit, scale = F)</code></pre>
<pre><code>rpart variable importance

        Overall
lstat    0.8646
nox      0.5008
ptratio  0.4643
rm       0.4527
indus    0.2595
crim     0.2484
age      0.2089
tax      0.0000
zn       0.0000
dis      0.0000
rad      0.0000
b        0.0000
chas1    0.0000</code></pre>
<ul>
<li>我们发现个别变量对于预测完全没有帮助，故可以将这些变量在之后的分析中删除。</li>
</ul>
</div>
</div>
<div id="caretk" class="section level1">
<h1>使用<code>caret</code>包训练K近邻模型</h1>
<ul>
<li>数据：<a href="https://www.kaggle.com/abcsds/pokemon">口袋妖怪</a>数据集</li>
<li>任务：通过各种属性预测神奇宝贝是否为传说级别</li>
</ul>
<blockquote>
<ul>
<li>ID,每只神奇宝贝的ID<br />
</li>
<li>Name,每只神奇宝贝的名字<br />
</li>
<li>Type1:每只神奇宝贝的类型，比如说水系，比如说火系<br />
</li>
<li>Type2:由于有特殊的神奇宝贝拥有复数以上的属性。<br />
</li>
<li>Total:指每只神奇宝贝的强度，一般越高越强<br />
</li>
<li>HP：指每只神奇宝贝的生命值<br />
</li>
<li>Attack:指每只神奇宝贝的攻击力<br />
</li>
<li>Defense:指每只神奇宝贝的防御力<br />
</li>
<li>SP Attack:指每只神奇宝贝面对相克属性神奇宝贝时的攻击力，通常会比普通攻击力高<br />
</li>
<li>SP Defense:指每只神奇宝贝面对相克属性神奇宝贝时的防御力，通常会比普通防御力高。<br />
</li>
<li>Speed:指每只神奇宝贝的速度<br />
</li>
<li>Generation：指每一只神奇宝贝属于哪一部神奇宝贝的，目前分为五部。</li>
<li>Legendary:指每一只神奇宝贝是不是属于传说级别的神奇宝贝，比如麒麟，超梦，梦幻之类的。</li>
</ul>
</blockquote>
<p><a href="https://dataaspirant.com/2017/01/09/knn-implementation-r-using-caret-package/">代码参考</a></p>
<pre class="r"><code>pokemon &lt;- read_csv(&quot;E:/R_codes/others/Pokemon.csv&quot;)
colnames(pokemon)[1] &lt;- &#39;id&#39; #修改变量名
colnames(pokemon)[3] &lt;- &#39;Type1&#39;
colnames(pokemon)[4] &lt;- &#39;Type2&#39;

pokemon &lt;- pokemon %&gt;%
  drop_na(Type1,Type2,Total,HP,Defense,Attack,Speed,Generation) %&gt;%
  select(Legendary,Type1,Type2,Total,HP,Defense,Attack,Speed,Generation) %&gt;%
  mutate(Legendary=as.factor(Legendary),
    Type1=recode(Type1,&#39;Fairy&#39;=&#39;oth&#39;,&#39;Fighting&#39;=&#39;oth&#39;,&#39;Flying&#39;=&#39;oth&#39;)
)</code></pre>
<pre class="r"><code>ggplot(pokemon,aes(Total,Attack,shape=Legendary,color=Legendary)) +
  geom_point(size=3)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-8"></span>
<img src="/post/cross_files/figure-html/unnamed-chunk-8-1.png" alt="过采样前" width="100%" />
<p class="caption">
Figure 3: 过采样前
</p>
</div>
<pre class="r"><code>#过采样
balance &lt;- ovun.sample(Legendary ~ Type1 + Type2 + Total+ HP + Defense+ Attack + Speed + Generation, data = pokemon, method = &quot;both&quot;, p=0.5,N=748,seed = 1)$data

sampled &lt;- sample(1:nrow(balance),nrow(balance)*0.7,replace=F)
trainpok &lt;- balance[sampled,]
testpok &lt;- balance[-sampled,]

ggplot(balance,aes(Total,Attack,shape=Legendary,color=Legendary)) +
  geom_jitter(size=1.5,height = 10, width = 40)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-9"></span>
<img src="/post/cross_files/figure-html/unnamed-chunk-9-1.png" alt="过采样后" width="100%" />
<p class="caption">
Figure 4: 过采样后
</p>
</div>
<div id="kk-nearest-neighbor" class="section level2">
<h2>K近邻算法(k-nearest neighbor)</h2>
<ul>
<li>K近邻算法对于测试集的给定样本，在训练集中找到与之最邻近的K个样本，把这个给定样本判定为它们的K临近点的多数类。</li>
<li>在样本比较少的情况下，通常使用交叉验证确定K值。</li>
<li>K越大，模型越简单；K越小，模型越复杂，越容易发生过拟合。</li>
</ul>
<pre class="r"><code>trctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number =10,repeats =10, savePredictions
=T)

set.seed(213)
knn_fit &lt;- train(Legendary ~ Type1 + Type2 + Total+ HP + Defense+ Attack + Speed + Generation, data = trainpok, method = &quot;knn&quot;,
 trControl=trctrl,
 tuneLength = 10)
knn_fit</code></pre>
<pre><code>k-Nearest Neighbors 

523 samples
  8 predictor
  2 classes: &#39;FALSE&#39;, &#39;TRUE&#39; 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 470, 470, 471, 471, 472, 471, ... 
Resampling results across tuning parameters:

  k   Accuracy   Kappa    
   5  0.9430503  0.8863625
   7  0.9449590  0.8901647
   9  0.9501266  0.9004474
  11  0.9375277  0.8753408
  13  0.9388303  0.8779424
  15  0.9414972  0.8832631
  17  0.9416970  0.8836506
  19  0.9436204  0.8874898
  21  0.9447597  0.8897732
  23  0.9422666  0.8848008

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was k = 9.</code></pre>
<pre class="r"><code>kacc &lt;- as.data.frame(knn_fit[[&quot;results&quot;]]) 
kacc &lt;- kacc[,1:2]
ggplot(kacc,aes(k,Accuracy)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks=seq(5,26,1))</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-10"></span>
<img src="/post/cross_files/figure-html/unnamed-chunk-10-1.png" alt="k近邻算法的K值和准确率的关系" width="100%" />
<p class="caption">
Figure 5: k近邻算法的K值和准确率的关系
</p>
</div>
<pre class="r"><code>testpok &lt;- testpok %&gt;%
  mutate(pred=predict(knn_fit,testpok))

prob1 &lt;- predict(knn_fit,testpok,type=&#39;prob&#39;)
pr1 &lt;- prob1[,2] 

trainpok &lt;- trainpok %&gt;%
  mutate(pred=predict(knn_fit,trainpok))

prob2 &lt;- predict(knn_fit,trainpok,type=&#39;prob&#39;)
pr2 &lt;- prob2[,2] 

conMatrix1 &lt;- confusionMatrix(testpok$pred,testpok$Legendary) 
#matrix &lt;- conMatrix1[[&quot;table&quot;]]
conMatrix1</code></pre>
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction FALSE TRUE
     FALSE   106    0
     TRUE     11  108
                                          
               Accuracy : 0.9511          
                 95% CI : (0.9142, 0.9753)
    No Information Rate : 0.52            
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
                                          
                  Kappa : 0.9024          
                                          
 Mcnemar&#39;s Test P-Value : 0.002569        
                                          
            Sensitivity : 0.9060          
            Specificity : 1.0000          
         Pos Pred Value : 1.0000          
         Neg Pred Value : 0.9076          
             Prevalence : 0.5200          
         Detection Rate : 0.4711          
   Detection Prevalence : 0.4711          
      Balanced Accuracy : 0.9530          
                                          
       &#39;Positive&#39; Class : FALSE           
                                          </code></pre>
<pre class="r"><code>conMatrix2 &lt;- confusionMatrix(trainpok$pred,trainpok$Legendary)
#matrix &lt;- conMatrix[[&quot;table&quot;]]
conMatrix2</code></pre>
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction FALSE TRUE
     FALSE   245    0
     TRUE     22  256
                                         
               Accuracy : 0.9579         
                 95% CI : (0.937, 0.9735)
    No Information Rate : 0.5105         
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
                                         
                  Kappa : 0.916          
                                         
 Mcnemar&#39;s Test P-Value : 7.562e-06      
                                         
            Sensitivity : 0.9176         
            Specificity : 1.0000         
         Pos Pred Value : 1.0000         
         Neg Pred Value : 0.9209         
             Prevalence : 0.5105         
         Detection Rate : 0.4685         
   Detection Prevalence : 0.4685         
      Balanced Accuracy : 0.9588         
                                         
       &#39;Positive&#39; Class : FALSE          
                                         </code></pre>
<pre class="r"><code>testpok$Legendary &lt;- as.ordered(testpok$Legendary)
trainpok$Legendary &lt;- as.ordered(trainpok$Legendary)

roca &lt;- roc(testpok$Legendary,pr1)
rocb &lt;- roc(trainpok$Legendary,pr2)</code></pre>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve">学习曲线</a></p>
<pre class="r"><code>ggroc(list(&#39;测试集&#39;=roca,&#39;训练集&#39;=rocb),size=1)+
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color=&#39;black&#39;, linetype=&#39;dashed&#39;)+
  theme(legend.title=element_blank()) </code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-11"></span>
<img src="/post/cross_files/figure-html/unnamed-chunk-11-1.png" alt="ROC曲线:使用`ggroc`包绘制。模型在测试集的表现比训练集稍差一些。" width="100%" />
<p class="caption">
Figure 6: ROC曲线:使用<code>ggroc</code>包绘制。模型在测试集的表现比训练集稍差一些。
</p>
</div>
</div>
</div>
<div id="section-2" class="section level1">
<h1>泰坦尼克</h1>
<pre class="r"><code>library(readxl)
titanic &lt;- read_excel(&#39;E:/MaLearning/titanic.xls&#39;)
sampled &lt;- sample(1:nrow(titanic),nrow(titanic)*0.75,replace=F)
titanic_train &lt;- titanic[sampled,]
titanic_test &lt;- titanic[-sampled,]

titanic_train &lt;- titanic_train %&gt;% 
  drop_na(survived,sex,age,sibsp,parch,fare,embarked) %&gt;% 
  mutate(pclass=as.factor(pclass))

titanic_test &lt;- titanic_test %&gt;% 
  drop_na(survived,sex,age,sibsp,parch,fare,embarked) %&gt;% 
  mutate(pclass=as.factor(pclass))

control &lt;- trainControl(method = &#39;repeatedcv&#39;, number =10,repeats =10, savePredictions
=T)

set.seed(213)
logfit &lt;- train(survived ~ sex+age+sibsp+parch+fare+embarked,data=titanic_train,
                 method=&#39;glm&#39;,family=&#39;binomial&#39;,
 trControl=control,
 tuneLength = 2)
logfit</code></pre>
<pre><code>Generalized Linear Model 

789 samples
  6 predictor

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 710, 710, 711, 710, 710, 710, ... 
Resampling results:

  RMSE       Rsquared   MAE     
  0.4033302  0.3396324  0.321572</code></pre>
<pre class="r"><code>titanic_test &lt;- titanic_test %&gt;% 
  mutate(pred=predict(logfit,titanic_test),
         pred=ifelse(pred&gt;=0.5,&#39;1&#39;,&#39;0&#39;),
         pred=as.factor(pred),
         survived=as.factor(survived))

prob1 &lt;- predict(logfit,titanic_test)

titanic_train &lt;- titanic_train %&gt;%
  mutate(pred=predict(logfit,titanic_train),
         pred=ifelse(pred&gt;=0.5,&#39;1&#39;,&#39;0&#39;),
         pred=as.factor(pred),
         survived=as.factor(survived))

prob2 &lt;- predict(logfit,titanic_train)

conMatrix1 &lt;- confusionMatrix(titanic_test$pred,titanic_test$survived) 
matrix &lt;- conMatrix1[[&quot;table&quot;]]
names(dimnames(matrix))&lt;-c(&#39;预测值&#39;,&#39;真实值&#39;)
matrix</code></pre>
<pre><code>      真实值
预测值   0   1
     0 127  26
     1  34  67</code></pre>
<pre class="r"><code>conMatrix2 &lt;- confusionMatrix(titanic_train$pred,titanic_train$survived)
matrix2 &lt;- conMatrix2[[&quot;table&quot;]]

names(dimnames(matrix2))&lt;-c(&#39;预测值&#39;,&#39;真实值&#39;)
matrix2</code></pre>
<pre><code>      真实值
预测值   0   1
     0 388 105
     1  69 227</code></pre>
<pre class="r"><code>titanic_test$Survived &lt;- as.ordered(titanic_test$survived)
titanic_train$Survived &lt;- as.ordered(titanic_train$survived)

roca &lt;- roc(titanic_test$survived,prob1)
rocb &lt;- roc(titanic_train$survived,prob2)</code></pre>
<pre class="r"><code>ggroc(list(&#39;test ROC&#39;=roca,&#39;train ROC&#39;=rocb),size=1)+
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color=&#39;black&#39;, linetype=&#39;solid&#39;)+
  theme(legend.title=element_blank()) </code></pre>
<div class="figure" style="text-align: center">
<img src="/post/cross_files/figure-html/unnamed-chunk-14-1.svg" alt=" " width="100%" />
<p class="caption">
</p>
</div>
<p>Python交叉验证</p>
<p>Sklearn库中的<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold"><code>KFold</code></a>、<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate"><code>cross_validate</code></a>、<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"><code>cross_val_score</code></a>、<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"><code>GridSearchCV</code></a>和<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"><code>RandomizedSearchCV</code></a>等函数实现。</p>
<p>其中，<code>KFold</code>、<code>cross_validate</code>等是外部的交叉验证。不涉及模型拟合。</p>
<p>而<code>GridSearchCV</code>等是内部交叉验证。在划分数据集的同时拟合模型。</p>
<p>还有其他的交叉验证方法诸如随机播种法：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit"><code>ShuffleSplit</code></a>。</p>
</div>
