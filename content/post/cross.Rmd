---
title: "交叉验证：基于caret包"
author: ' '
date: '2019-05-25'
slug: cross
tags: ["机器学习"]
output:
  html_document2:
    toc: true
    theme: united
---

![](https://img.shields.io/badge/License-CC%20BY--NC--ND%204.0-brightgreen.svg)

# 基本概念

交叉验证是机器学习中的常用模型选择方法。其中最常用的方法是K折交叉验证(k-fold cross validation)。K折交叉验证重复使用数据，把给定的数据切分为K个互斥子集，每次使用1个子集作为测试集，使用余下k-1个子集的并集作为训练集。其中，训练集用于模型的训练，测试集用于模型的评估和选择。在样本量不够充足的情况下，交叉验证法通过重复使用数据能够减少样本划分不同导致的差别，并且选择测试误差最小的模型，增强模型的泛化能力。

划分测试集和训练集同样有助于更加客观地评价模型的泛化能力。同时，交叉验证也是模型调参的过程。


交叉验证主要有以下种类：

+ 简单交叉验证 (hold-out cross validation)

+ k折交叉验证 (K-fold cross-validation)

+ 留一交叉验证(leave-one-out cross validation)

谢益辉的统计动画R包[animation](https://yihui.name/animation/example/cv-ani/)直观地展示了交叉验证方法。

任务：通过一系列特征预测波士顿房价

输入`?mlbench::BostonHousing`查看数据集解释。下面主要使用[caret](http://topepo.github.io/caret/index.html)包(classification and regression training)进行交叉验证。

```{r,comment = NA,warning = F,message = F,fig.align='center'}
`%>%` <- magrittr::`%>%`
library(caret)
library(readr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(pROC)
library(stargazer)
library(ROSE)
```

# R示例

## 10折交叉验证 (K-fold cross-validation)

```{r,comment = NA,warning = F,message = F,fig.align='center'}
rfControl <-trainControl( #10折交叉验证
method ="cv", 
number =10 
)

library(mlbench) #加载数据集
data(BostonHousing)
head(BostonHousing)
nrow(BostonHousing) #样本量

rpartFit <- train(medv ~ .,
                  data = BostonHousing,
                  method = "rpart",
                  trControl = rfControl)
rpartFit
```

将数据集D划分为K个子集同样存在多种划分方式。为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p 次k折交叉验证结果的均值， 例如常见的有"10次10 折交叉验证"。


```{r,comment=NA,warning = F,message=F,fig.align='center',out.width='70%',fig.asp=1.2,fig.cap='变量重要性'} 
repeatedcv <-trainControl(method ="repeatedcv", number =10,repeats =10, savePredictions
=T)
rpartFit <- train(medv ~ .,
                  data = BostonHousing,
                  method = "rpart",
                  trControl = repeatedcv)
rpartFit

importance <- varImp(rpartFit, scale = F)
imp <- as.data.frame(importance[["importance"]])

imp <- imp %>%
  mutate(name=rownames(imp)) %>%
  filter(Overall>0) %>%
  rename('impt'='Overall') #%>%
  #mutate(varname = fct_reorder(varname,impt))

ggplot(imp,aes(reorder(name,impt),impt)) +
  geom_segment( aes(x=reorder(name,impt), xend=reorder(name,impt), y=0, yend=impt),size=1) +
  geom_point(size=3, color="red")+
  coord_flip()+
  labs(x="重要性",y="变量名") + 
  theme_bw()
```

```{r,eval=F,echo=F}
ggplot(imp,aes(reorder(name,impt),impt,color=impt,fill=impt)) +
  geom_bar(stat = 'identity',width = 0.2) +
  coord_flip() +
  labs(x="重要性",y="变量名") + 
  theme_bw()
```


## 留一交叉验证(leave-one-out cross validation)
```{r,comment = NA,warning = F,message = F,fig.align='center'}
rfControl <-trainControl(
method ="LOOCV"
) #留一交叉验证
rpartFit <- train(medv ~ .,
                  data = BostonHousing,
                  method = "rpart",
                  trControl = rfControl)
rpartFit

varImp(rpartFit, scale = F)
```

+ 我们发现个别变量对于预测完全没有帮助，故可以将这些变量在之后的分析中删除。

# 使用`caret`包训练K近邻模型

 + 数据：[口袋妖怪](https://www.kaggle.com/abcsds/pokemon)数据集
 + 任务：通过各种属性预测神奇宝贝是否为传说级别
 
> + ID,每只神奇宝贝的ID  
+ Name,每只神奇宝贝的名字  
+ Type1:每只神奇宝贝的类型，比如说水系，比如说火系  
+ Type2:由于有特殊的神奇宝贝拥有复数以上的属性。  
+ Total:指每只神奇宝贝的强度，一般越高越强  
+ HP：指每只神奇宝贝的生命值  
+ Attack:指每只神奇宝贝的攻击力  
+ Defense:指每只神奇宝贝的防御力  
+ SP Attack:指每只神奇宝贝面对相克属性神奇宝贝时的攻击力，通常会比普通攻击力高  
+ SP Defense:指每只神奇宝贝面对相克属性神奇宝贝时的防御力，通常会比普通防御力高。  
+ Speed:指每只神奇宝贝的速度  
+ Generation：指每一只神奇宝贝属于哪一部神奇宝贝的，目前分为五部。 
+ Legendary:指每一只神奇宝贝是不是属于传说级别的神奇宝贝，比如麒麟，超梦，梦幻之类的。  

[代码参考](https://dataaspirant.com/2017/01/09/knn-implementation-r-using-caret-package/)


```{r,comment = NA,warning = F,message = F}
pokemon <- read_csv("E:/R_codes/others/Pokemon.csv")
colnames(pokemon)[1] <- 'id' #修改变量名
colnames(pokemon)[3] <- 'Type1'
colnames(pokemon)[4] <- 'Type2'

pokemon <- pokemon %>%
  drop_na(Type1,Type2,Total,HP,Defense,Attack,Speed,Generation) %>%
  select(Legendary,Type1,Type2,Total,HP,Defense,Attack,Speed,Generation) %>%
  mutate(Legendary=as.factor(Legendary),
    Type1=recode(Type1,'Fairy'='oth','Fighting'='oth','Flying'='oth')
)

```

```{r,comment = NA,warning = F,message = F,fig.align='center',fig.cap='过采样前',out.width='100%'}
ggplot(pokemon,aes(Total,Attack,shape=Legendary,color=Legendary)) +
  geom_point(size=3)
```




```{r,comment = NA,warning = F,message = F,fig.align='center',fig.cap='过采样后',out.width='100%'}
#过采样
balance <- ovun.sample(Legendary ~ Type1 + Type2 + Total+ HP + Defense+ Attack + Speed + Generation, data = pokemon, method = "both", p=0.5,N=748,seed = 1)$data

sampled <- sample(1:nrow(balance),nrow(balance)*0.7,replace=F)
trainpok <- balance[sampled,]
testpok <- balance[-sampled,]

ggplot(balance,aes(Total,Attack,shape=Legendary,color=Legendary)) +
  geom_jitter(size=1.5,height = 10, width = 40)
```

## K近邻算法(k-nearest neighbor)

+ K近邻算法对于测试集的给定样本，在训练集中找到与之最邻近的K个样本，把这个给定样本判定为它们的K临近点的多数类。
+ 在样本比较少的情况下，通常使用交叉验证确定K值。
+ K越大，模型越简单；K越小，模型越复杂，越容易发生过拟合。

```{r,comment = NA,warning = F,message = F,fig.align='center',fig.cap='k近邻算法的K值和准确率的关系',out.width='100%'}
trctrl <- trainControl(method = "repeatedcv", number =10,repeats =10, savePredictions
=T)

set.seed(213)
knn_fit <- train(Legendary ~ Type1 + Type2 + Total+ HP + Defense+ Attack + Speed + Generation, data = trainpok, method = "knn",
 trControl=trctrl,
 tuneLength = 10)
knn_fit

kacc <- as.data.frame(knn_fit[["results"]]) 
kacc <- kacc[,1:2]
ggplot(kacc,aes(k,Accuracy)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks=seq(5,26,1))

testpok <- testpok %>%
  mutate(pred=predict(knn_fit,testpok))

prob1 <- predict(knn_fit,testpok,type='prob')
pr1 <- prob1[,2] 

trainpok <- trainpok %>%
  mutate(pred=predict(knn_fit,trainpok))

prob2 <- predict(knn_fit,trainpok,type='prob')
pr2 <- prob2[,2] 

conMatrix1 <- confusionMatrix(testpok$pred,testpok$Legendary) 
#matrix <- conMatrix1[["table"]]
conMatrix1

conMatrix2 <- confusionMatrix(trainpok$pred,trainpok$Legendary)
#matrix <- conMatrix[["table"]]
conMatrix2

testpok$Legendary <- as.ordered(testpok$Legendary)
trainpok$Legendary <- as.ordered(trainpok$Legendary)

roca <- roc(testpok$Legendary,pr1)
rocb <- roc(trainpok$Legendary,pr2)
```



```{r,comment = NA,warning = F,message = F,fig.align='center',fig.cap='ROC曲线:使用`ggroc`包绘制。模型在测试集的表现比训练集稍差一些。',out.width='100%'}
ggroc(list('测试集'=roca,'训练集'=rocb),size=1)+
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color='black', linetype='dashed')+
  theme(legend.title=element_blank()) 
```



```{r,echo=F,eval=F}
balanced$test_pred <- as.ordered(balanced$test_pred)
balanced$Legendary <- as.ordered(balanced$Legendary)
roc(balanced$test_pred,balanced$Legendary,plot=T, print.auc=T, auc.polygon=T, max.auc.polygon=T,auc.polygon.col="skyblue", print.thres=T,xlab ='伪正类率',ylab='真正类率')
#ggroc()
```



# 5折交叉验证

使用`modelr`包的函数

```{r,message = F, comment = NA, warning = F,out.width ='100%',fig.asp=0.6,results='asis'}
library(readxl)
titanic <- read_excel('E:/MaLearning/titanic.xls')
sampled <- sample(1:nrow(titanic),nrow(titanic)*0.7,replace=F)
titanic_train <- titanic[sampled,]
titanic_test <- titanic[-sampled,]

titanic_train <- titanic_train %>% 
  drop_na(survived,sex,age,sibsp,parch,fare,embarked) %>% 
  mutate(pclass=as.factor(pclass))

titanic_test <- titanic_test %>% 
  drop_na(survived,sex,age,sibsp,parch,fare,embarked) %>% 
  mutate(pclass=as.factor(pclass))

control <- trainControl(method = 'repeatedcv', number =5,repeats =1, savePredictions
=T)

set.seed(213)
logfit <- train(survived ~ sex+age+sibsp+parch+fare+embarked,data=titanic_train,
                 method='glm',family='binomial',
 trControl=control,
 tuneLength = 2)
logfit

titanic_test <- titanic_test %>% 
  mutate(pred=predict(logfit,titanic_test),
         pred=ifelse(pred>=0.5,'1','0'),
         pred=as.factor(pred),
         survived=as.factor(survived))

prob1 <- predict(logfit,titanic_test)

titanic_train <- titanic_train %>%
  mutate(pred=predict(logfit,titanic_train),
         pred=ifelse(pred>=0.5,'1','0'),
         pred=as.factor(pred),
         survived=as.factor(survived))

prob2 <- predict(logfit,titanic_train)


conMatrix1 <- confusionMatrix(titanic_test$pred,titanic_test$survived) 
matrix <- conMatrix1[["table"]]
stargazer(matrix,type='html',title='测试集混淆矩阵')

conMatrix2 <- confusionMatrix(titanic_train$pred,titanic_train$survived)
matrix2 <- conMatrix2[["table"]]
stargazer(matrix2,type='html',title='训练集混淆矩阵')

#titanic_test$Survived <- as.ordered(titanic_test$Survived)
titanic_train$Survived <- as.ordered(titanic_train$survived)

#roca <- roc(titanic_test$Survived,pr1)
rocb <- roc(titanic_train$Survived,prob2)

#summary(models)

```

```{r,comment = NA,warning = F,message = F,fig.align='center',fig.cap=' ',out.width='100%'}
ggroc(list('test ROC'=roca,'train ROC'=rocb),size=1)+
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color='black', linetype='solid')+
  theme(legend.title=element_blank()) 
```

Python交叉验证

Sklearn库中的[`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)、[`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)和[`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)函数









