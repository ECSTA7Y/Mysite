---
title: '文本分析笔记'
author: 'Malcolm'
date: '`r Sys.Date()`'
output:
  bookdown::html_document2:
    toc: true
    theme: readable
---

```{r, include=F}
knitr::opts_chunk$set(comment=NA,error=T,message = F,warning = F,fig.align='center',out.width ='90%')
```

文献计量学三大定律：

1. Zipf's law

> given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word

```{r, echo=F,fig.cap='Zipf\'s law'}
knitr::include_graphics('https://upload.wikimedia.org/wikipedia/commons/7/70/Zipf_distribution_PMF.png')
```



2. Bradford's law

本质是Pareto distribution.


3. Lotka's Law


```{r, echo=F,fig.cap='Lotka\'s law'}
knitr::include_graphics('https://upload.wikimedia.org/wikipedia/commons/1/1e/Lotka_plot.png')
```

```{r}
library(dplyr)

df <- tibble(x = c(1, NA, 3))


w <- c(3,6,7,0,1,11,7)
w <- c(5,4,3,2)

rank(w)

```
```{r,eval=F}
#清除内存空间
rm(list=ls()) 
#导入tm包
library(tm)
library(SnowballC)
#查看tm包的文档
#vignette("tm")
 
##1.Data Import  导入自带的路透社的20篇xml文档
#找到/texts/crude的目录，作为DirSource的输入，读取20篇xml文档
reut21578 <- system.file("texts", "crude", package = "tm") 
reuters <- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XML))
 
##2.Data Export  将生成的语料库在磁盘上保存成多个纯文本文件
writeCorpus(reuters)
 
##3.Inspecting Corpora 查看语料库 
#can use inspect(),print(),summary()
#由于是从xml读取过来，所以现在的corpus还是非常杂乱
inspect(reuters)
print(reuters) 
summary(reuters)
##4.Transformations 
#对于xml格式的文档用tm_map命令对语料库文件进行预处理，将其转为纯文本并去除多余空格，
#转换小写，去除常用词汇、合并异形同意词汇，如此才能得到类似txt文件的效果
#可以用inspect(reuters)查看此时的效果，明显好很多
reuters <- tm_map(reuters,PlainTextDocument)#将reuters转化为纯文本文件，去除标签
reuters <- tm_map(reuters, stripWhitespace)#去掉空白

reuters <- tm_map(reuters, removeWords, stopwords("english"))#去停用词
reuters <- tm_map(reuters, content_transformer(tolower))#转换为小写
#采用Porter's stemming 算法 提取词干
#Stem words in a text document using Porter's stemming algorithm
#install.packages("SnowballC")
reuters <- tm_map(reuters, stemDocument)
#reuters <- tm_map(reuters, )
##5.Creating Term-Document Matrices
#将处理后的语料库进行断字处理，生成词频权重矩阵(稀疏矩阵)也叫词汇文档矩阵
#reuters <- as.VCorpus(reuters)
dtm <- TermDocumentMatrix(reuters)
#查看词汇文档矩阵
#inspect(dtm[[1]])
#Non-/sparse entries: 1990/22390     ---非0/是0 
#Sparsity           : 92%            ---稀疏性  稀疏元素占全部元素的比例
#Maximal term length: 17             ---切词结果的字符最长那个的长度
#Weighting          : term frequency (tf)
#如果需要考察多个文档中特有词汇的出现频率，可以手工生成字典，
#并将它作为生成矩阵的参数
d<-c("price","crude","oil","use")
#inspect(DocumentTermMatrix(reuters,control=list(dictionary=d)))
##6.Operations on Term-Document Matrices
#找出次数超过5的词
findFreqTerms(dtm, 5)
#找出与‘opec’单词相关系数在0.8以上的词
findAssocs(dtm,"opec",0.8)
#因为生成的矩阵是一个稀疏矩阵，再进行降维处理，之后转为标准数据框格式
#我们可以去掉某些出现频次太低的词。
dtm1<- removeSparseTerms(dtm, sparse=0.6)
dtm1 <- cleantext(dtm1)
inspect(dtm1)
data <- as.data.frame(inspect(dtm1))
#再之后就可以利用R语言中任何工具加以研究了，下面用层次聚类试试看
#先进行标准化处理，再生成距离矩阵，再用层次聚类
data.scale <- scale(data)
d <- dist(data.scale, method = "euclidean")
fit <- hclust(d, method="ward.D")
#绘制聚类图
#可以看到在20个文档中，489号和502号聚成一类，与其它文档区别较大。
plot(fit,main ="文件聚类分析")
#主成分分析
ozMat <- TermDocumentMatrix(makeChunks(reuters, 50),
                            list(weighting = weightBin))
k <- princomp(as.matrix(ozMat), features = 2)
screeplot(k,npcs=6,type='lines')
windows()
biplot(k)
```


文本分析

去除停止词

去除标点符号

去除数字

提取词干

文档矩阵
文本向量化的三种基本的编码方式

countvectorize

tf-idf

hashing
  
机器学习

概率图模型

# 支持向量机

```{r,eval=F}
library(e1071)
data(iris)
attach(iris)

## classification mode
# default with factor response:
model <- svm(Species ~ ., data = iris)

# alternatively the traditional interface:
x <- subset(iris, select = -Species)
y <- Species
model <- svm(x, y) 

print(model)
summary(model)
```


# 朴素贝叶斯

```{r,eval=F}
## Categorical data only:
data(HouseVotes84, package = "mlbench")
model <- naiveBayes(Class ~ ., data = HouseVotes84)
predict(model, HouseVotes84[1:10,])
predict(model, HouseVotes84[1:10,], type = "raw")

pred <- predict(model, HouseVotes84)
table(pred, HouseVotes84$Class)

## using laplace smoothing:
model <- naiveBayes(Class ~ ., data = HouseVotes84, laplace = 3)
pred <- predict(model, HouseVotes84[,-1])
table(pred, HouseVotes84$Class)


## Example of using a contingency table:
data(Titanic)
m <- naiveBayes(Survived ~ ., data = Titanic)
m
predict(m, as.data.frame(Titanic))

## Example with metric predictors:
data(iris)
m <- naiveBayes(Species ~ ., data = iris)
## alternatively:
m <- naiveBayes(iris[,-5], iris[,5])
m
table(predict(m, iris), iris[,5])
```

```{r}
library(tm)
library(tidytext)
data("crude")
crude

inspect(crude)
print(crude) 
summary(crude)
#crude <- VCorpus(crude)

library(dplyr)
library(tidytext)

#ap_td <- tidy(crude)
#ap_td

#crude <- Terms(crude)
#head(crude)

crude <- tm_map(crude,PlainTextDocument)#将reuters转化为纯文本文件，去除标签
inspect(crude[[1]])

crude <- tm_map(crude, stripWhitespace)#去掉空白
inspect(crude[[1]])

crude <- tm_map(crude,
                  removeWords,stopwords("english"))#去停用词
inspect(crude[[1]])

crude <- tm_map(crude, content_transformer(tolower))#转换为小写
inspect(crude[[1]])

crude <- tm_map(crude, stemDocument)
inspect(crude[[1]])

#inspect(crude)
```
  
  