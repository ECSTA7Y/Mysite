---
title: "R数据分析"
author: " "
date: " "
output:
  bookdown::html_document2:
    toc: true
    theme: united
slug: ranalysis
---

# 聚类分析

## k-means聚类

[k-means in R](https://uc-r.github.io/kmeans_clustering)

```{r,message = F, comment = NA,warning = F,fig.align='center',out.width ='100%',fig.asp=0.7}
#葡萄酒数据
data(wine, package="rattle")
knitr::kable(head(wine))

library(cluster)
set.seed(1234)
fit.pam <- pam(wine[-1], k=3, stand=T)#3个质心
knitr::kable(fit.pam$medoids) #查看质心

library(fMultivar)
set.seed(1234)
df <- rnorm2d(1000, rho=0.9) #2元正态分布
df <- as.data.frame(df)
plot(df, main="Bivariate Normal Distribution with rho=0.9")

library(ggplot2)
fit <- pam(df, k=2)
df$clustering <- factor(fit$clustering)#输出聚类结果变量
ggplot(data=df, aes(x=V1, y=V2, color=clustering, shape=clustering)) +
geom_point() + ggtitle("Clustering of Bivariate Normal Data")

require(graphics)
# a 2-dimensional example
x <- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),
           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))
colnames(x) <- c("x", "y")
(cl <- kmeans(x, 2))
plot(x, col = cl$cluster)
points(cl$centers, col = 1:2, pch = 8, cex = 2)

```

## 系统聚类

```{r,message = F, comment = NA,warning = F,fig.align='center',out.width ='95%',fig.asp=0.7}
Rclass=read.csv("E:/R_codes/others/Rdclass.csv",header=T)
Rclass1=Rclass[,-c(1,6,7,8)]
rownames(Rclass1)=Rclass$Name
gower_dist<-daisy(Rclass1,metric = "gower")#分类变量的聚类
#summary(gower_dist)
h <- hclust(gower_dist)
plot(h,main = '系统聚类图')
rect.hclust(h, k=7, border = "red")
(cn <- cutree(h, k = 7))
```

# 词云图

```{r ,message = F, comment = NA,warning = F,cache=T,fig.align='center',out.width ='100%',fig.asp=0.6}
library(wordcloud2)
library(jiebaR)
text <- readLines("E:/R_codes/others/Rkws.csv",encoding = "UTF-8")
text <- text[-1] 
freq <- freq(text)
index <- order(-freq[,2]) 
order2 <- freq[index, ]
wordcloud2(freq,size = 1, color = "random-light")
#wordcloud2(freq,size = 1, color = "random-light",shape='cardioid')
#wordcloud2(freq,size = 1, color = "random-light",shape='diamond')
```

# 主成分分析

```{r ,message = F, comment = NA,warning = F,cache=T,fig.align='center',out.width ='90%',fig.asp=0.8}
#数据:USJudgeRatings
knitr::kable(head(USJudgeRatings))
library(psych)
fa.parallel(USJudgeRatings[,-1], fa="pc", n.iter=100,
show.legend=F, main="碎石图")
pc <- principal(USJudgeRatings[,-1], nfactors=1, rotate="varimax", score=T) #方差极大旋转
pc

USJudgeRatings$score <- pc$scores
knitr::kable(head(USJudgeRatings$score))

```

# 市场人群细分

+ CHAID算法
  + 卡方自动交互检测器

+ Latent Class

+ Canonical Correlation (典型相关分析)
  + 可以做多组变量(连续、离散）的分析
  + 之后做聚类
  + 出来之后的维度类似于因子，思路和因子分析相差不大
  + 权重和loading
  + 保存对象得分，得出连续因子
  
+ 最后做系统聚类，最小聚类数，最大聚类数。最好使用`ward`方法

+ 1000样本量尽量分4-5群

+ 说服自己、说服客服

+ 最后做判别分析

# 机器学习笔记

+ 机器学习不能解决[不完全信息动态博弈](https://mp.weixin.qq.com/s/Yz7gBKZK9mclVMl1A6hP7w)。

+ 深度学习能够解决简单的模式识别问题。



# 计量经济学：因果推断模型

## Regression Discontinuity Design

```{r ,message = F, comment = NA,warning = F}
library(dplyr)
library(magrittr)
library(MASS)
library(rdd)

set.seed(1)
n <- 10 ^ 3
mu <- matrix(c(0, 0))
rho <- 0.5
sigma1 <- 0.5
sigma2 <- 2
Sigma <- matrix(c(sigma1 ^ 2,
rho * sigma1 * sigma2,
rho * sigma1 * sigma2,
sigma2 ^ 2),
c(2, 2))
simulatedXandC <- mvrnorm(n = n, mu = mu, Sigma = Sigma)
simulatedError <- rnorm(n = n)
simulatedZ <- round(runif(n))
group <- round(runif(n))
df <- data.frame(X = simulatedXandC[, 1],
C = simulatedXandC[, 2],
error = simulatedError)
df %<>%
mutate(Y = 10 + 1 * X + 5 * C + error)
fit <- lm(Y ~ X,df)
summary(fit)
fit <- lm(Y ~ X + C,df)
summary(fit)

df <- data.frame(X = simulatedXandC[, 1],
C = simulatedXandC[, 2],
T = (simulatedXandC[, 1] > 0),
error = simulatedError)
df %<>%
mutate(Y = 10 + X ^ 3 + 5 * T + 1 * C + error)
fit <- RDestimate(Y ~ X,df)
summary(fit)

ggplot(df,aes(X,Y,color = (X > 0))) +
geom_point() +
geom_smooth(method=lm,se = F)
```


## Difference-in-Differences

```{r ,message = F, comment = NA,warning = F}
df <- data.frame(time = 10 * round(runif(n), 1) - 5,
treatment = group,
error = simulatedError)

df %<>%
mutate(Y = 1 - time + 5 * treatment + 7 * (time >= 0) * treatment + error)
fit <- lm(Y ~ time + treatment + I((time >= 0) * treatment), data = df)
summary(fit)
dfMean <- df %>%
group_by(time, treatment) %>%
summarise(Y = mean(Y))

ggplot(df, aes(time,Y, color = factor(treatment + 2 * (time >= 0)))) +
geom_point() +
geom_smooth(method = lm,se = F) +
theme(legend.position = "none") +
scale_color_manual(values = c("#F05253", "#2A73CC", "#F05253", "#2A73CC"))
```

## Fixed Effects Regression

```{r,message = F, comment = NA,warning = F}
df <- data.frame(X = simulatedXandC[, 1],
C = round(pmin(2, pmax(-2, simulatedXandC[, 2]))),
error = simulatedError)
df %<>%
mutate(Y = 10 + 1 * X + 5 * C + error)
fit <- lm(Y ~ X,df)
summary(fit)
fit <- lm(Y ~ X + factor(C),df)
summary(fit)
```

## Instrumental Variables

```{r,message = F, comment = NA,warning = F}
df <- data.frame(Z = simulatedZ,
X = simulatedXandC[, 1] + simulatedZ,
C = simulatedXandC[, 2],
error = simulatedError)
df %<>%
mutate(Y = 10 + 1.0 * X - 2 * C + error)

#Naive regression produces biased estimates
fit <- lm(Y ~ X,df)
summary(fit, vcov = sandwich)

#IV recovers the underlying model
fit <- ivreg(Y ~ X | Z, data = df)
summary(fit, vcov = sandwich, df = Inf, diagnostics = T)

```

# LASSO回归

+ OLS:
$$\hat \beta  = argmin_{{\beta  \in {R^d}}}||Y - X\beta |{|^2} = {({X^T}X)^{ - 1}}{X^T}Y$$
+ 惩罚似然函数的一般形式：
$$\hat{\beta} = argmin_{\beta \in R^d}(||Y - X\beta||^2 + P_{\lambda}(|\beta|))$$

其中，惩罚项$P_{\lambda}(|\beta|) = \lambda \sum_{j = 1}^{d}|\beta_j|^m,m>=0$。当$m=1$时，得到$L_1$惩罚项(LASSO回归)，当$m=2$时，得到$L_2$惩罚项(岭回归)。


$$\hat{\beta_{Lasso}} = arg min_{\beta \in R^d}||Y - X\beta||^2 +\lambda \sum_{j = 1}^{d}|\beta_j|$$



```{r,error=T,comment=NA,warning=F,message=F}
library(haven)
library(tidyr)
library(plotmo)
library(glmnet)
library(readr)
`%>%` <- magrittr::`%>%`

ceps <- read_csv("C:/Users/xsong/Desktop/table/ceps.imputed.csv")

ceps <- na.omit(ceps)

x_train <- model.matrix(sdtotal ~ schids + sex + onechi +  maedu +  faedu + drunk + qurel + relation +  desk + net + dialect + chkhmwk + chkcouse + classtm + clpre + revitm + subject + know + drsmok + commhr + schtype + schcsrm +  comno +  bknum + buget + eduqua + fight + brkpb + smok +  drink +  teainc + money + clfee + qianzi + lifetm +  dial +  chidia +  futcfd +  huko + eduy +  dangy + houspro,ceps)[,-1]

y_train <- ceps$sdtotal %>% 
  unlist() %>%
  as.numeric()

lasso_mod <- glmnet(x_train,y_train,alpha = 1)


plot_glmnet(lasso_mod,xvar='lambda',label=10)

cvfit <- cv.glmnet(x_train, 
                   y_train, 
                   alpha = 1)
cvfit$lambda.min
cvfit$lambda.1se
plot(cvfit)
```

# XGBOOST

+ 在变量的层面并行计算

+ 可以进行正则化收缩

参数：

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-cly1">booster </th>
    <th class="tg-cly1">基学习器</th>
    <th class="tg-cly1">线性回归：gblinear<br>&nbsp;&nbsp;&nbsp;&nbsp;决策树：gbtree</th>
  </tr>
  <tr>
    <td class="tg-cly1">eta</td>
    <td class="tg-cly1">学习率</td>
    <td class="tg-cly1"></td>
  </tr>
  <tr>
    <td class="tg-cly1">gamma</td>
    <td class="tg-cly1">最小伽马损失减少量</td>
    <td class="tg-cly1"></td>
  </tr>
  <tr>
    <td class="tg-0lax">max_depth</td>
    <td class="tg-0lax">每棵树的最大深度。默认为6</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">objective </td>
    <td class="tg-0lax">任务参数。</td>
    <td class="tg-0lax">reg:squarederror 回归<br>&nbsp;&nbsp;&nbsp;&nbsp;logistic回归<br>&nbsp;&nbsp;&nbsp;&nbsp;binary:logistic 输出logistic概率<br>&nbsp;&nbsp;&nbsp;&nbsp;rank:pairwise 排序<br>&nbsp;&nbsp;&nbsp;&nbsp;num_class 多分类<br>&nbsp;&nbsp;&nbsp;&nbsp;</td>
  </tr>
  <tr>
    <td class="tg-0lax">nrounds</td>
    <td class="tg-0lax">最大boost迭代数</td>
    <td class="tg-0lax">自变量和因变量关系越复杂nrounds需要越大</td>
  </tr>
</table>

```{r,error=T,comment=NA,warning=F,message=F,dev='svg',fig.showtext=T,out.width='80%',fig.show=T}
library(xgboost)
library(rpart.plot)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test
bst <- xgboost(data = train$data, label = train$label, max_depth = 2, eta = 1,nrounds = 2,verbose = 3, objective = "binary:logistic")
importance_matrix <- xgb.importance(model = bst)
xgb.plot.importance(importance_matrix = importance_matrix)

xgb.dump(bst, with_stats = T)
#xgb.plot.tree(model = bst,trees=0,show_node_id=T)

pred <- predict(bst, test$data)
```


```{r,error=T,comment=NA,warning=F,message=F}
logregobj <- function(preds, dtrain) {
labels <- getinfo(dtrain, "label")
preds <- 1/(1 + exp(-preds))
grad <- preds - labels
hess <- preds * (1 - preds)
return(list(grad = grad, hess = hess))
}
evalerror <- function(preds, dtrain) {
labels <- getinfo(dtrain, "label")
err <- sqrt(mean((preds-labels)^2))
return(list(metric = "MSE", value = err))
}

dtrain <- xgb.DMatrix(train$data, label = train$label)

dtest <- xgb.DMatrix(test$data, label = test$label)
watchlist <- list(eval = dtest, train = dtrain)
param <- list(max_depth = 2, eta = 1, silent = 1)
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist, logregobj, evalerror, maximize = FALSE)
```


