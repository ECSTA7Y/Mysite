---
title: 'R数据分析'
author: 
date: 
output:
  bookdown::html_document2:
    toc: true
    theme: united
slug: ranalysis
---

# 聚类分析

## k-means聚类

[k-means in R](https://uc-r.github.io/kmeans_clustering)

```{r,include=F}
knitr::opts_chunk$set(comment=NA,error=T,message = F,warning = F,fig.align='center',out.width ='100%')
```


```{r,fig.asp=0.7}
#葡萄酒数据
data(wine, package="rattle")
knitr::kable(head(wine))

library(cluster)
set.seed(1234)
fit.pam <- pam(wine[-1], k=3, stand=T)#3个质心
knitr::kable(fit.pam$medoids) #查看质心

library(fMultivar)
set.seed(1234)
df <- rnorm2d(1000, rho=0.9) #2元正态分布
df <- as.data.frame(df)
plot(df, main="Bivariate Normal Distribution with rho=0.9")

library(ggplot2)
fit <- pam(df, k=2)
df$clustering <- factor(fit$clustering)#输出聚类结果变量
ggplot(data=df, aes(x=V1, y=V2, color=clustering, shape=clustering)) +
geom_point() + ggtitle("Clustering of Bivariate Normal Data")

require(graphics)
# a 2-dimensional example
x <- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),
           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))
colnames(x) <- c("x", "y")
(cl <- kmeans(x, 2))
plot(x, col = cl$cluster)
points(cl$centers, col = 1:2, pch = 8, cex = 2)

```

## 系统聚类

```{r,fig.asp=0.7}
Rclass=read.csv("E:/R_codes/others/Rdclass.csv",header=T)
Rclass1=Rclass[,-c(1,6,7,8)]
rownames(Rclass1)=Rclass$Name
gower_dist<-daisy(Rclass1,metric = "gower")#分类变量的聚类
#summary(gower_dist)
h <- hclust(gower_dist)
plot(h,main = '系统聚类图')
rect.hclust(h, k=7, border = "red")
(cn <- cutree(h, k = 7))
```

# 词云图

```{r,fig.asp=0.6,cache=T}
library(wordcloud2)
library(jiebaR)
text <- readLines("E:/R_codes/others/Rkws.csv",encoding = "UTF-8")
text <- text[-1] 
freq <- freq(text)
index <- order(-freq[,2]) 
order2 <- freq[index, ]
wordcloud2(freq,size = 1, color = "random-light")
#wordcloud2(freq,size = 1, color = "random-light",shape='cardioid')
#wordcloud2(freq,size = 1, color = "random-light",shape='diamond')
```

# 主成分分析

```{r,fig.asp=0.8}
#数据:USJudgeRatings
knitr::kable(head(USJudgeRatings))
library(psych)
fa.parallel(USJudgeRatings[,-1], fa="pc", n.iter=100,
show.legend=F, main="碎石图")
pc <- principal(USJudgeRatings[,-1], nfactors=1, rotate="varimax", score=T) #方差极大旋转
pc

USJudgeRatings$score <- pc$scores
knitr::kable(head(USJudgeRatings$score))

```

# 市场人群细分

+ CHAID算法
  + 卡方自动交互检测器

+ Latent Class

+ Canonical Correlation (典型相关分析)
  + 可以做多组变量(连续、离散）的分析
  + 之后做聚类
  + 出来之后的维度类似于因子，思路和因子分析相差不大
  + 权重和loading
  + 保存对象得分，得出连续因子
  
+ 最后做系统聚类，最小聚类数，最大聚类数。最好使用`ward`方法

+ 1000样本量尽量分4-5群

+ 说服自己、说服客服

+ 最后做判别分析

# 机器学习笔记

+ 机器学习不能解决[不完全信息动态博弈](https://mp.weixin.qq.com/s/Yz7gBKZK9mclVMl1A6hP7w)。

+ 深度学习能够解决简单的模式识别问题。



# 计量经济学

教材：[Introduction to Econometrics with R](https://www.econometrics-with-r.org/index.html)

## 断点回归

+ Regression Discontinuity Design

```{r}
library(dplyr)
library(magrittr)
library(MASS)
library(rdd)
library(modelr)
library(cowplot)
set.seed(1)
n <- 10 ^ 3
mu <- matrix(c(0, 0))
rho <- 0.5
sigma1 <- 0.5
sigma2 <- 2
Sigma <- matrix(c(sigma1 ^ 2,
rho * sigma1 * sigma2,
rho * sigma1 * sigma2,
sigma2 ^ 2),
c(2, 2))
simulatedXandC <- mvrnorm(n = n, mu = mu, Sigma = Sigma)
simulatedError <- rnorm(n = n)
simulatedZ <- round(runif(n))
group <- round(runif(n))
df <- data.frame(X = simulatedXandC[, 1],
C = simulatedXandC[, 2],
error = simulatedError)
df %<>%
mutate(Y = 10 + 1 * X + 5 * C + error)
fit <- lm(Y ~ X,df)
summary(fit)
fit <- lm(Y ~ X + C,df)
summary(fit)

df <- data.frame(X = simulatedXandC[, 1],
C = simulatedXandC[, 2],
T = (simulatedXandC[, 1] > 0),
error = simulatedError)
df %<>%
mutate(Y = 10 + X ^ 3 + 5 * T + 1 * C + error)
fit <- RDestimate(Y ~ X,df)
summary(fit)

ggplot(df,aes(X,Y,color = (X > 0))) +
geom_point() +
geom_smooth(method=lm,se = F)
```

## 双重差分

+ Difference-in-Differences

```{r}
df <- data.frame(time = 10 * round(runif(n), 1) - 5,
treatment = group,
error = simulatedError)

df %<>%
mutate(Y = 1 - time + 5 * treatment + 7 * (time >= 0) * treatment + error)
fit <- lm(Y ~ time + treatment + I((time >= 0) * treatment), data = df)
summary(fit)
dfMean <- df %>%
group_by(time, treatment) %>%
summarise(Y = mean(Y))

ggplot(df, aes(time,Y, color = factor(treatment + 2 * (time >= 0)))) +
geom_jitter(width=0.5,height=1) +
geom_smooth(method = lm,se = F) +
theme(legend.position = "none") +
scale_color_manual(values = c("#F05253", "#2A73CC", "#F05253", "#2A73CC"))
```

## 固定效应模型

+ Fixed Effects Regression

```{r,fig.asp=0.4,fig.cap= '$x$为自变量，$y$为因变量。形状代表不同个体，颜色代表多个调查年份。左图：合并回归(Pooled OLS)，将所有点视为一个个体。右图：组内估计量(within-group)，控制个体，观测每个个体随调查时间的变化。'}
set.seed(222)
x <- seq(0,4,length = 100)
y <- -x + jitter(rep(1:5, each = 20), 2)
z <-  rep(1:5, each = 20) %>% as.factor()
dt <- data.frame(x,y,z) %>% 
  group_by(z) %>% 
  mutate(c=rep(1:20,length = 20),
         year=as.factor(c))
#View(dt)
plta <- ggplot(dt, aes(x, y)) +
  geom_point(aes(color=year,shape=z),size=2) +
  geom_smooth(method='lm',se=F,color='red') +
  theme_bw() +
  theme(legend.position = 'none')

moda <- lm(y ~ x + z,dt)
grid <- dt %>%
data_grid(x,z) %>%
gather_predictions(moda)
          
pltb <- ggplot(dt, aes(x, y)) +
  geom_point(aes(color=year,shape=z),size=2) +
  geom_line(data=grid,aes(pred,x,group=z),
            size=0.75,color='red') +
  ylim(0.2,1.9)+
  xlim(0,4)+
  theme_bw() +
  theme(legend.position = 'none')

plot_grid(plta,pltb,ncol = 2)
```

```{r,fig.asp=0.8,fig.cap='左上：线性回归。右上：分年份拟合。左下：固定效应。右下：时间固定效应。'}
data('Grunfeld', package = 'plm')

Grunfeld <- Grunfeld %>% 
  filter(year<1940,firm<6) %>% 
  mutate(year=as.factor(year),
         firm=as.factor(firm))

plta <- ggplot(Grunfeld, aes(value,capital)) +
  geom_point(aes(col=firm,shape=year),size=2) +
  geom_smooth(method='lm',se=F,color='red') +
  theme_bw() +
  theme(legend.position = 'none')+
  ylim(0,400)

pltb <- ggplot(Grunfeld, aes(value,capital,group=year)) +
  geom_point(aes(col=firm,shape=year),size=2) +
  geom_smooth(method='lm',se=F,color='red') +
  theme_bw() +
  theme(legend.position = 'none')+
  ylim(0,400)

moda <- lm(capital ~ value + firm,Grunfeld)
grid <- Grunfeld %>%
data_grid(value,firm) %>%
gather_predictions(moda)
          
pltc <- ggplot(Grunfeld, aes(value,capital)) +
  geom_point(aes(col=firm,shape=year),size=2) +
  geom_line(data=grid,aes(value,pred,
                          color=firm,
                          group=firm),size=0.75) +
  theme_bw() +
  theme(legend.position = 'none')+
  ylim(0,400)

modc <- lm(capital ~ value + year,Grunfeld)
grid <- Grunfeld %>%
data_grid(value,year) %>%
gather_predictions(modc)
          
pltd <- ggplot(Grunfeld, aes(value,capital)) +
  geom_point(aes(col=firm,shape=year),size=2) +
  geom_line(data=grid,aes(value,pred,
                          color=year,
                          group=year),size=0.75) +
  theme_bw() +
  theme(legend.position = 'none')+
  ylim(0,400)

plot_grid(plta,pltb,pltc,pltd,ncol = 2)
```

```{r,out.width ='100%',results='asis'}
library(plm)
ols <- lm(inv ~ value + capital,
          data = Grunfeld)

fe <- plm(inv ~ value + capital,
          data = Grunfeld, model = 'within',
          effect='individual')

tw <- plm(inv ~ value + capital,
          data = Grunfeld, model = 'within', effect = 'twoways')

stargazer::stargazer(ols,fe,tw,
          title='回归结果',
          dep.var.caption=' ',
          dep.var.labels=' ',
          align=T,
          type='html',
          column.labels=c('线性回归','组内估计','双重固定'),
          keep.stat=c('n','rsq','ll'))
```




## 工具变量

+ Instrumental Variables

```{r}
df <- data.frame(Z = simulatedZ,
X = simulatedXandC[, 1] + simulatedZ,
C = simulatedXandC[, 2],
error = simulatedError)
df %<>%
mutate(Y = 10 + 1.0 * X - 2 * C + error)

#Naive regression produces biased estimates
fit1 <- lm(Y ~ X,df)
summary(fit, vcov = sandwich)

#IV recovers the underlying model
fit2 <- ivreg(Y ~ X | Z, data = df)
summary(fit, vcov = sandwich, df = Inf, diagnostics = T)
```

```{r,echo=F,results='asis'}
stargazer::stargazer(fit1,fit2,
          title='回归结果',
          dep.var.caption=' ',
          dep.var.labels=' ',
          align=T,
          type='html',
          keep.stat=c('n','rsq'))
```

# LASSO回归

+ OLS:
$$\hat \beta  = argmin_{{\beta  \in {R^d}}}||Y - X\beta |{|^2} = {({X^T}X)^{ - 1}}{X^T}Y$$
+ 惩罚似然函数的一般形式：
$$\hat{\beta} = argmin_{\beta \in R^d}(||Y - X\beta||^2 + P_{\lambda}(|\beta|))$$

其中，惩罚项$P_{\lambda}(|\beta|) = \lambda \sum_{j = 1}^{d}|\beta_j|^m,m>=0$。当$m=1$时，得到$L_1$惩罚项(LASSO回归)，当$m=2$时，得到$L_2$惩罚项(岭回归)。


$$\hat{\beta_{Lasso}} = arg min_{\beta \in R^d}||Y - X\beta||^2 +\lambda \sum_{j = 1}^{d}|\beta_j|$$



```{r}
library(haven)
library(tidyr)
library(plotmo)
library(glmnet)
library(readr)
`%>%` <- magrittr::`%>%`

ceps <- read_csv('C:/Users/xsong/Desktop/table/ceps.imputed.csv')

ceps <- na.omit(ceps)

x_train <- model.matrix(sdtotal ~ schids + sex + onechi +  maedu +  faedu + drunk + qurel + relation +  desk + net + dialect + chkhmwk + chkcouse + classtm + clpre + revitm + subject + know + drsmok + commhr + schtype + schcsrm +  comno +  bknum + buget + eduqua + fight + brkpb + smok +  drink +  teainc + money + clfee + qianzi + lifetm +  dial +  chidia +  futcfd +  huko + eduy +  dangy + houspro,ceps)[,-1]

y_train <- ceps$sdtotal %>% 
  unlist() %>%
  as.numeric()

lasso_mod <- glmnet(x_train,y_train,alpha = 1)


plot_glmnet(lasso_mod,xvar='lambda',label=10)

cvfit <- cv.glmnet(x_train, 
                   y_train, 
                   alpha = 1)
cvfit$lambda.min
cvfit$lambda.1se
plot(cvfit)
```

# XGBOOST

+ 在变量的层面并行计算

+ 可以进行正则化收缩

参数：

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-cly1">booster </th>
    <th class="tg-cly1">基学习器</th>
    <th class="tg-cly1">线性回归：gblinear<br>&nbsp;&nbsp;&nbsp;&nbsp;决策树：gbtree</th>
  </tr>
  <tr>
    <td class="tg-cly1">eta</td>
    <td class="tg-cly1">学习率</td>
    <td class="tg-cly1"></td>
  </tr>
  <tr>
    <td class="tg-cly1">gamma</td>
    <td class="tg-cly1">最小伽马损失减少量</td>
    <td class="tg-cly1"></td>
  </tr>
  <tr>
    <td class="tg-0lax">max_depth</td>
    <td class="tg-0lax">每棵树的最大深度。默认为6</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">objective </td>
    <td class="tg-0lax">任务参数。</td>
    <td class="tg-0lax">reg:squarederror 回归<br>&nbsp;&nbsp;&nbsp;&nbsp;logistic回归<br>&nbsp;&nbsp;&nbsp;&nbsp;binary:logistic 输出logistic概率<br>&nbsp;&nbsp;&nbsp;&nbsp;rank:pairwise 排序<br>&nbsp;&nbsp;&nbsp;&nbsp;num_class 多分类<br>&nbsp;&nbsp;&nbsp;&nbsp;</td>
  </tr>
  <tr>
    <td class="tg-0lax">nrounds</td>
    <td class="tg-0lax">最大boost迭代数</td>
    <td class="tg-0lax">自变量和因变量关系越复杂nrounds需要越大</td>
  </tr>
</table>

```{r,dev='svg'}
library(xgboost)
library(rpart.plot)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test
bst <- xgboost(data = train$data, label = train$label, max_depth = 2, eta = 1,nrounds = 2,verbose = 3, objective = "binary:logistic")
importance_matrix <- xgb.importance(model = bst)
xgb.plot.importance(importance_matrix = importance_matrix)

xgb.dump(bst, with_stats = T)
#xgb.plot.tree(model = bst,trees=0,show_node_id=T)

pred <- predict(bst, test$data)
```


```{r}
logregobj <- function(preds, dtrain) {
labels <- getinfo(dtrain, "label")
preds <- 1/(1 + exp(-preds))
grad <- preds - labels
hess <- preds * (1 - preds)
return(list(grad = grad, hess = hess))
}
evalerror <- function(preds, dtrain) {
labels <- getinfo(dtrain, "label")
err <- sqrt(mean((preds-labels)^2))
return(list(metric = "MSE", value = err))
}

dtrain <- xgb.DMatrix(train$data, label = train$label)

dtest <- xgb.DMatrix(test$data, label = test$label)
watchlist <- list(eval = dtest, train = dtrain)
param <- list(max_depth = 2, eta = 1, silent = 1)
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist, logregobj, evalerror, maximize = FALSE)
```

# 对数线性模型

```{r,eval=F}

## http://ww2.coastal.edu/kingw/statistics/R-tutorials/loglin.html

data(Titanic)
View(Titanic)
dimnames(Titanic)
margin.table(Titanic, c(2,4))

### begin copying script here
likelihood.test = function(x) {
  nrows = dim(x)[1]                      # no. of rows in contingency table
  ncols = dim(x)[2]                      # no. of cols in contingency table
  chi.out = chisq.test(x,correct=F)      # do a Pearson chi square test
  table = chi.out[[6]]                   # get the OFs
  ratios = chi.out[[6]]/chi.out[[7]]     # calculate OF/EF ratios
  sum = 0                                # storage for the test statistic
  for (i in 1:nrows) {
    for (j in 1:ncols) {
      sum = sum + table[i,j]*log(ratios[i,j])
    }
  }
  sum = 2 * sum                          # the likelihood ratio chi square
  df = chi.out[[2]]                      # degrees of freedom
  p = 1 - pchisq(sum,df)                 # p-value
  out = c(sum, df, p, chi.out[[1]])      # the output vector
  names(out) = c("LR-chisq","df","p-value","Pears-chisq")
  round(out,4)                           # done!
}
### end copying script here and paste into R Console
sex.survived = margin.table(Titanic, c(2,4)) 
summary(Titanic)
library(MASS)             
k <- loglm( ~ Sex + Survived, data=sex.survived)
k
summary(k)

loglm(~ Class + Sex + Age + Survived, data=Titanic)

loglm(~ Class * Sex * Age * Survived, data=Titanic)

loglm(~ Class * Sex * Age * Survived - Class:Sex:Age:Survived, data=Titanic)

loglm(formula = ~Class + Sex + Age + Survived + 
        Class:Sex + Class:Age + 
        Sex:Age + Class:Survived + Age:Survived + Class:Sex:Age + 
        Class:Age:Survived, data=Titanic)

```

# 有限混合模型

`flexmix`

```{r}
library(flexmix)
data("NPreg", package = "flexmix")
## mixture of two linear regression models. Note that control parameters
## can be specified as named list and abbreviated if unique.
ex1 <- flexmix(yn ~ x + I(x^2), data=NPreg, k=2,
               control=list(verb=5, iter=100))
ex1
summary(ex1)
plot(ex1)
```

# 文本分析笔记

中山大学

+ $$文本作为数据$$  


文本比较 微博情绪研究 情感分析 

网络论坛：天涯、凯迪社区、

媒体数据库：人民日报、纽约时报

网上社区：百度贴吧

中国裁判文书网：法学、犯罪学

+ 特征

多种来源  
非结构化  
长尾分布  
词语之间复杂微妙的关系  
词语模糊性和情景敏感性  

+ $$文本数据与外部数据结合$$

微博情绪文本、股票市场  
重大事件与重大事件的关系  
政策文本与地方实际表现  

+ 机器学习

无监督学习（文本聚类、主题模型）  
**半监督学习**（随机森林、朴素贝叶斯）

+ 新进展  

词向量模型（`Word2vec`）  
循环神经网络（`RNN`）

+ 清理文本  

删除停词  
大小写  
删除  
提取词干  
去除标点符号  
去除非字符符号  
提取标题、作者、日历、电子邮箱、地址等  
分词

包  
`text2vec`
`e1701` 
`tm` 
`Snomba`  

+ 词汇语义变迁研究  
特定类型词汇库的完善  
实体关系或网络构建  
文本相似性计算  
情感分析  

# 常见统计检验


```{r}
t.test(extra ~ group,sleep)
wilcox.test(extra ~ group,sleep)
```

```{r}
M <- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))
dimnames(M) <- list(gender = c("F", "M"),
                    party = c("Democrat","Independent", "Republican"))
(Xsq <- chisq.test(M))  # Prints test summary
Xsq$observed   # observed counts (same as M)
Xsq$expected   # expected counts under the null
Xsq$residuals  # Pearson residuals
Xsq$stdres     # standardized residuals
```


# 时间序列分析

[使用`tsibble`](https://mp.weixin.qq.com/s/EjCTFsvzYV7o5Q-V3GmBRA)

```{r}
library(tsibble)
USAccDeaths
USAccDeaths %>% as_tsibble() %>% head()

tourism %>%
  filter(Purpose == "Holiday") %>%
  group_by(State) %>%
  summarise(Trips = sum(Trips)) %>%
  head()

tourism %>%
  mutate(Year = lubridate::year(Quarter)) %>%
  index_by(Year) %>%
  group_by(Region, State, Purpose) %>%
  summarise(Trips = sum(Trips)) %>%
  ungroup()

pedestrian %>%
  mutate(
    Day = lubridate::wday(Date, label = TRUE),
    Weekend = (Day %in% c("周六", "周日"))
  ) %>%
  ggplot() +
  geom_line(aes(Time,Count,col = Weekend,group = Date)) +
  facet_grid(Sensor ~ .)
```


