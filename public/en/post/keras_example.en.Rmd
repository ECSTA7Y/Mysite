---
title: Guide to Keras Deep Learning 
slug: keras_example
author: ' '
date: '2020-05-16'
---

Xiao Song

https://xsong.ltd/

As a beginner, it's necessary to write some simple guides using simple data. 

```{r setup, include=F}
knitr::opts_chunk$set(comment='#>',error=T,message = F,warning = F,fig.align='center',collapse = T,out.width ='90%')
```

```{python}
import numpy as np 
import pandas as pd

X_train = pd.read_csv('E:/some_code/py_basic/house_price/data/train1.csv') # prepare data
X_test = pd.read_csv('E:/some_code/py_basic/house_price/data/test1.csv')

Y_train = np.array(X_train['SalePrice'])
Y_train = np.log1p(Y_train)

X_train.drop(['SalePrice'],axis = 1, inplace = True)
```

```{python}
Y_train
```

```{python}
X_train.shape
```

```{python}
X_test.shape
```

# Feature Standardization 

Standardize features by removing the mean and scaling to unit variance:
$$z = {{x - u} \over s}$$

$z$ is standardize feature of $x$, u is the mean of $x$, and $s$ is the standard deviation of $x$.

```{python}
#from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import StandardScaler

def standardize(df):
    '''standardize features'''
    transformer = StandardScaler().fit(df) 
    newX = transformer.transform(df)
    X = pd.DataFrame(newX,columns = df.columns)
    return X

X_train = standardize(X_train) # X train
X_test = standardize(X_test) 
```

```{python}
X_train
```

```{python}
X_train.describe()
```

```{python}
X_train.shape[1]
```

# Neural Network Building

```{python}
from keras import models
from keras import layers

model = models.Sequential()
model.add(layers.Dense(64, activation='relu',input_shape=(X_train.shape[1],)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(1))

model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
model.summary()
```

```{python}
#?models.Sequential.fit
```

```{python}
model.fit(X_train,Y_train,
    epochs = 20, # Number of epochs to train the model
    batch_size = 512) # Number of samples per gradient update.
```

# Validation data

first I create validation set:

```{python}
x_val = X_train[:1000]
partial_x_train = X_train[1000:]
y_val = Y_train[:1000]
partial_y_train = Y_train[1000:]
```

```{python}
history = model.fit(partial_x_train,partial_y_train,
    epochs=20,
    batch_size=512,
    validation_data=(x_val, y_val))
```

Extract cross validation information:

```{python}
history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(loss_values) + 1)
cv_info = pd.DataFrame({'epochs':epochs,'loss_values':loss_values,'val_loss_values':val_loss_values})  
```

```{python}
cv_info
```

```{python}
cv_info = pd.melt(cv_info, id_vars=['epochs'], value_vars=['loss_values', 'val_loss_values']) 
```

```{python}
from plotnine import *

(
ggplot(cv_info,aes('epochs','value',color = 'variable')) +
geom_line() +
geom_point()
)
```

# Prediction

```{python}
Y_pred = model.predict(X_test)
Y_pred
```

```{python}
Y_pred.shape
```

```{python}
Y_pred = np.concatenate(Y_pred).ravel() # to flatten 2 dimition array
Y_pred = np.expm1(Y_pred)
Y_pred
```

```{python}
test = pd.read_csv('E:/some_code/py_basic/house_price/data/test.csv')
submission = pd.DataFrame({'id': test['Id'], 'SalePrice': Y_pred})
submission.head(10)
```

```{python}
#submission.to_csv('./output/keras.csv',index = False) # save result
```

